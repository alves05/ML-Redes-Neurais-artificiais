{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX2YyoKU9t0VCJ3usRfCAc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alves05/ML-Redes-Neurais-artificiais/blob/main/ML_Redes_Neurais_artificiais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes neurais artificiais"
      ],
      "metadata": {
        "id": "1qit0O4fQT_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redes neurais artificiais são um conceito da computação que visa trabalhar no processamento de dados de maneira semelhante ao cérebro humano. O cérebro é tido como um processador altamente complexo e que realiza processamentos de maneira paralela. Para isso, ele organiza sua estrutura, ou seja, os neurônios, de forma que eles realizem o processamento necessário. Isso é feito numa velocidade extremamente alta e não existe qualquer computador no mundo capaz de realizar o que o cérebro humano faz.\n",
        "\n",
        "Nas redes neurais artificiais, a idéia é realizar o processamento de informações tendo como princípio a organização de neurônios do cérebro. Como o cérebro humano é capaz de aprender e tomar decisões baseadas na aprendizagem, as redes neurais artificiais devem fazer o mesmo. Assim, uma rede neural pode ser interpretada como um esquema de processamento capaz de armazenar conhecimento baseado em aprendizagem (experiência) e disponibilizar este conhecimento para a aplicação em questão."
      ],
      "metadata": {
        "id": "iHwzZPUcneJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação de bibliotecas e carregamento da base de dados"
      ],
      "metadata": {
        "id": "jQ2wJdaunwdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas\n",
        " \n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix"
      ],
      "metadata": {
        "id": "z15xIREORbWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvdlc7hgFSUB"
      },
      "outputs": [],
      "source": [
        "# Carregando base\n",
        "\n",
        "with open('credit.pkl', 'rb') as f:\n",
        "  x_credit_treino, y_credit_treino, x_credit_teste, y_credit_teste = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando base de treino\n",
        "x_credit_treino.shape, y_credit_treino.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4e3d6inSPZo",
        "outputId": "5b33935b-933c-4ebb-a432-bd0bed82f5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando base de teste\n",
        "x_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQCGm_CMSZcv",
        "outputId": "32bdc9b9-4815-4e3b-ee5a-c87c67946c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicando algoritmo"
      ],
      "metadata": {
        "id": "Rx6ZBfIen96Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a rede neural\n",
        "\n",
        "rede_neural_credit = MLPClassifier()\n",
        "rede_neural_credit.fit(x_credit_treino, y_credit_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKji5pvCSgBu",
        "outputId": "548ac13c-fcaf-4580-dd67-155a461eed4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a rede neural mudando o parametro max_iter e o varbose para True\n",
        "\n",
        "rede_neural_credit = MLPClassifier(max_iter=1000, verbose=True)\n",
        "rede_neural_credit.fit(x_credit_treino, y_credit_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MtDkE5XTBx9",
        "outputId": "b3be1adf-3461-4797-99eb-7f3d598db79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.74058032\n",
            "Iteration 2, loss = 0.67338691\n",
            "Iteration 3, loss = 0.61467298\n",
            "Iteration 4, loss = 0.56228387\n",
            "Iteration 5, loss = 0.51579547\n",
            "Iteration 6, loss = 0.47517786\n",
            "Iteration 7, loss = 0.43925719\n",
            "Iteration 8, loss = 0.40698863\n",
            "Iteration 9, loss = 0.37850811\n",
            "Iteration 10, loss = 0.35278339\n",
            "Iteration 11, loss = 0.32967389\n",
            "Iteration 12, loss = 0.30908756\n",
            "Iteration 13, loss = 0.29024054\n",
            "Iteration 14, loss = 0.27358086\n",
            "Iteration 15, loss = 0.25846076\n",
            "Iteration 16, loss = 0.24480634\n",
            "Iteration 17, loss = 0.23268414\n",
            "Iteration 18, loss = 0.22165371\n",
            "Iteration 19, loss = 0.21195987\n",
            "Iteration 20, loss = 0.20291637\n",
            "Iteration 21, loss = 0.19480152\n",
            "Iteration 22, loss = 0.18749572\n",
            "Iteration 23, loss = 0.18079755\n",
            "Iteration 24, loss = 0.17468843\n",
            "Iteration 25, loss = 0.16903447\n",
            "Iteration 26, loss = 0.16393714\n",
            "Iteration 27, loss = 0.15918846\n",
            "Iteration 28, loss = 0.15472586\n",
            "Iteration 29, loss = 0.15069927\n",
            "Iteration 30, loss = 0.14688389\n",
            "Iteration 31, loss = 0.14342106\n",
            "Iteration 32, loss = 0.14003795\n",
            "Iteration 33, loss = 0.13698850\n",
            "Iteration 34, loss = 0.13407931\n",
            "Iteration 35, loss = 0.13144238\n",
            "Iteration 36, loss = 0.12883056\n",
            "Iteration 37, loss = 0.12633553\n",
            "Iteration 38, loss = 0.12402438\n",
            "Iteration 39, loss = 0.12185026\n",
            "Iteration 40, loss = 0.11980990\n",
            "Iteration 41, loss = 0.11781653\n",
            "Iteration 42, loss = 0.11593212\n",
            "Iteration 43, loss = 0.11415304\n",
            "Iteration 44, loss = 0.11247367\n",
            "Iteration 45, loss = 0.11088862\n",
            "Iteration 46, loss = 0.10935088\n",
            "Iteration 47, loss = 0.10791759\n",
            "Iteration 48, loss = 0.10653333\n",
            "Iteration 49, loss = 0.10513991\n",
            "Iteration 50, loss = 0.10389076\n",
            "Iteration 51, loss = 0.10263009\n",
            "Iteration 52, loss = 0.10141425\n",
            "Iteration 53, loss = 0.10014745\n",
            "Iteration 54, loss = 0.09908944\n",
            "Iteration 55, loss = 0.09794197\n",
            "Iteration 56, loss = 0.09692430\n",
            "Iteration 57, loss = 0.09584644\n",
            "Iteration 58, loss = 0.09483056\n",
            "Iteration 59, loss = 0.09381987\n",
            "Iteration 60, loss = 0.09288099\n",
            "Iteration 61, loss = 0.09195920\n",
            "Iteration 62, loss = 0.09102738\n",
            "Iteration 63, loss = 0.09017147\n",
            "Iteration 64, loss = 0.08924802\n",
            "Iteration 65, loss = 0.08842331\n",
            "Iteration 66, loss = 0.08762764\n",
            "Iteration 67, loss = 0.08669524\n",
            "Iteration 68, loss = 0.08590650\n",
            "Iteration 69, loss = 0.08513493\n",
            "Iteration 70, loss = 0.08435623\n",
            "Iteration 71, loss = 0.08360021\n",
            "Iteration 72, loss = 0.08288892\n",
            "Iteration 73, loss = 0.08218481\n",
            "Iteration 74, loss = 0.08145186\n",
            "Iteration 75, loss = 0.08076967\n",
            "Iteration 76, loss = 0.08019533\n",
            "Iteration 77, loss = 0.07929470\n",
            "Iteration 78, loss = 0.07867716\n",
            "Iteration 79, loss = 0.07791321\n",
            "Iteration 80, loss = 0.07724556\n",
            "Iteration 81, loss = 0.07659284\n",
            "Iteration 82, loss = 0.07592670\n",
            "Iteration 83, loss = 0.07521645\n",
            "Iteration 84, loss = 0.07456063\n",
            "Iteration 85, loss = 0.07390652\n",
            "Iteration 86, loss = 0.07331529\n",
            "Iteration 87, loss = 0.07266225\n",
            "Iteration 88, loss = 0.07196787\n",
            "Iteration 89, loss = 0.07135887\n",
            "Iteration 90, loss = 0.07071510\n",
            "Iteration 91, loss = 0.07019107\n",
            "Iteration 92, loss = 0.06948442\n",
            "Iteration 93, loss = 0.06898256\n",
            "Iteration 94, loss = 0.06831166\n",
            "Iteration 95, loss = 0.06773277\n",
            "Iteration 96, loss = 0.06714456\n",
            "Iteration 97, loss = 0.06664957\n",
            "Iteration 98, loss = 0.06607823\n",
            "Iteration 99, loss = 0.06539040\n",
            "Iteration 100, loss = 0.06479993\n",
            "Iteration 101, loss = 0.06430130\n",
            "Iteration 102, loss = 0.06378493\n",
            "Iteration 103, loss = 0.06326527\n",
            "Iteration 104, loss = 0.06265992\n",
            "Iteration 105, loss = 0.06225259\n",
            "Iteration 106, loss = 0.06165206\n",
            "Iteration 107, loss = 0.06116163\n",
            "Iteration 108, loss = 0.06062642\n",
            "Iteration 109, loss = 0.06047402\n",
            "Iteration 110, loss = 0.05973862\n",
            "Iteration 111, loss = 0.05925288\n",
            "Iteration 112, loss = 0.05880319\n",
            "Iteration 113, loss = 0.05837591\n",
            "Iteration 114, loss = 0.05792790\n",
            "Iteration 115, loss = 0.05742036\n",
            "Iteration 116, loss = 0.05699726\n",
            "Iteration 117, loss = 0.05660630\n",
            "Iteration 118, loss = 0.05621535\n",
            "Iteration 119, loss = 0.05572662\n",
            "Iteration 120, loss = 0.05532235\n",
            "Iteration 121, loss = 0.05488139\n",
            "Iteration 122, loss = 0.05448532\n",
            "Iteration 123, loss = 0.05411178\n",
            "Iteration 124, loss = 0.05365373\n",
            "Iteration 125, loss = 0.05330678\n",
            "Iteration 126, loss = 0.05282853\n",
            "Iteration 127, loss = 0.05248265\n",
            "Iteration 128, loss = 0.05213910\n",
            "Iteration 129, loss = 0.05172687\n",
            "Iteration 130, loss = 0.05137146\n",
            "Iteration 131, loss = 0.05097369\n",
            "Iteration 132, loss = 0.05056742\n",
            "Iteration 133, loss = 0.05024128\n",
            "Iteration 134, loss = 0.04995574\n",
            "Iteration 135, loss = 0.04963679\n",
            "Iteration 136, loss = 0.04929947\n",
            "Iteration 137, loss = 0.04893840\n",
            "Iteration 138, loss = 0.04853253\n",
            "Iteration 139, loss = 0.04822009\n",
            "Iteration 140, loss = 0.04788439\n",
            "Iteration 141, loss = 0.04753062\n",
            "Iteration 142, loss = 0.04728770\n",
            "Iteration 143, loss = 0.04701159\n",
            "Iteration 144, loss = 0.04662167\n",
            "Iteration 145, loss = 0.04628169\n",
            "Iteration 146, loss = 0.04604091\n",
            "Iteration 147, loss = 0.04574373\n",
            "Iteration 148, loss = 0.04540857\n",
            "Iteration 149, loss = 0.04513988\n",
            "Iteration 150, loss = 0.04496118\n",
            "Iteration 151, loss = 0.04461279\n",
            "Iteration 152, loss = 0.04432417\n",
            "Iteration 153, loss = 0.04400453\n",
            "Iteration 154, loss = 0.04374044\n",
            "Iteration 155, loss = 0.04343554\n",
            "Iteration 156, loss = 0.04319288\n",
            "Iteration 157, loss = 0.04289062\n",
            "Iteration 158, loss = 0.04266642\n",
            "Iteration 159, loss = 0.04238713\n",
            "Iteration 160, loss = 0.04208890\n",
            "Iteration 161, loss = 0.04186259\n",
            "Iteration 162, loss = 0.04159949\n",
            "Iteration 163, loss = 0.04132564\n",
            "Iteration 164, loss = 0.04109906\n",
            "Iteration 165, loss = 0.04078763\n",
            "Iteration 166, loss = 0.04071603\n",
            "Iteration 167, loss = 0.04036025\n",
            "Iteration 168, loss = 0.04012485\n",
            "Iteration 169, loss = 0.03984746\n",
            "Iteration 170, loss = 0.03958064\n",
            "Iteration 171, loss = 0.03941633\n",
            "Iteration 172, loss = 0.03915160\n",
            "Iteration 173, loss = 0.03897369\n",
            "Iteration 174, loss = 0.03864620\n",
            "Iteration 175, loss = 0.03845991\n",
            "Iteration 176, loss = 0.03825244\n",
            "Iteration 177, loss = 0.03817547\n",
            "Iteration 178, loss = 0.03795290\n",
            "Iteration 179, loss = 0.03762202\n",
            "Iteration 180, loss = 0.03742291\n",
            "Iteration 181, loss = 0.03711444\n",
            "Iteration 182, loss = 0.03714752\n",
            "Iteration 183, loss = 0.03684565\n",
            "Iteration 184, loss = 0.03660957\n",
            "Iteration 185, loss = 0.03641478\n",
            "Iteration 186, loss = 0.03617756\n",
            "Iteration 187, loss = 0.03595813\n",
            "Iteration 188, loss = 0.03582288\n",
            "Iteration 189, loss = 0.03558243\n",
            "Iteration 190, loss = 0.03537522\n",
            "Iteration 191, loss = 0.03516507\n",
            "Iteration 192, loss = 0.03501724\n",
            "Iteration 193, loss = 0.03481268\n",
            "Iteration 194, loss = 0.03468942\n",
            "Iteration 195, loss = 0.03455399\n",
            "Iteration 196, loss = 0.03428164\n",
            "Iteration 197, loss = 0.03419578\n",
            "Iteration 198, loss = 0.03393696\n",
            "Iteration 199, loss = 0.03377216\n",
            "Iteration 200, loss = 0.03364085\n",
            "Iteration 201, loss = 0.03344932\n",
            "Iteration 202, loss = 0.03328718\n",
            "Iteration 203, loss = 0.03314448\n",
            "Iteration 204, loss = 0.03295495\n",
            "Iteration 205, loss = 0.03281975\n",
            "Iteration 206, loss = 0.03263984\n",
            "Iteration 207, loss = 0.03242292\n",
            "Iteration 208, loss = 0.03236078\n",
            "Iteration 209, loss = 0.03218730\n",
            "Iteration 210, loss = 0.03204666\n",
            "Iteration 211, loss = 0.03187963\n",
            "Iteration 212, loss = 0.03169322\n",
            "Iteration 213, loss = 0.03153706\n",
            "Iteration 214, loss = 0.03140860\n",
            "Iteration 215, loss = 0.03122522\n",
            "Iteration 216, loss = 0.03111016\n",
            "Iteration 217, loss = 0.03094738\n",
            "Iteration 218, loss = 0.03081422\n",
            "Iteration 219, loss = 0.03073938\n",
            "Iteration 220, loss = 0.03058904\n",
            "Iteration 221, loss = 0.03036407\n",
            "Iteration 222, loss = 0.03027932\n",
            "Iteration 223, loss = 0.03011213\n",
            "Iteration 224, loss = 0.03000970\n",
            "Iteration 225, loss = 0.02988975\n",
            "Iteration 226, loss = 0.02969836\n",
            "Iteration 227, loss = 0.02962607\n",
            "Iteration 228, loss = 0.02949477\n",
            "Iteration 229, loss = 0.02935308\n",
            "Iteration 230, loss = 0.02920797\n",
            "Iteration 231, loss = 0.02909384\n",
            "Iteration 232, loss = 0.02894664\n",
            "Iteration 233, loss = 0.02880439\n",
            "Iteration 234, loss = 0.02867101\n",
            "Iteration 235, loss = 0.02857852\n",
            "Iteration 236, loss = 0.02844385\n",
            "Iteration 237, loss = 0.02840284\n",
            "Iteration 238, loss = 0.02825873\n",
            "Iteration 239, loss = 0.02809702\n",
            "Iteration 240, loss = 0.02798609\n",
            "Iteration 241, loss = 0.02783477\n",
            "Iteration 242, loss = 0.02777864\n",
            "Iteration 243, loss = 0.02759780\n",
            "Iteration 244, loss = 0.02752197\n",
            "Iteration 245, loss = 0.02739602\n",
            "Iteration 246, loss = 0.02726762\n",
            "Iteration 247, loss = 0.02714119\n",
            "Iteration 248, loss = 0.02701644\n",
            "Iteration 249, loss = 0.02694114\n",
            "Iteration 250, loss = 0.02686859\n",
            "Iteration 251, loss = 0.02672781\n",
            "Iteration 252, loss = 0.02658565\n",
            "Iteration 253, loss = 0.02649256\n",
            "Iteration 254, loss = 0.02639532\n",
            "Iteration 255, loss = 0.02631108\n",
            "Iteration 256, loss = 0.02620558\n",
            "Iteration 257, loss = 0.02610257\n",
            "Iteration 258, loss = 0.02595385\n",
            "Iteration 259, loss = 0.02590259\n",
            "Iteration 260, loss = 0.02576398\n",
            "Iteration 261, loss = 0.02566854\n",
            "Iteration 262, loss = 0.02556264\n",
            "Iteration 263, loss = 0.02545501\n",
            "Iteration 264, loss = 0.02539755\n",
            "Iteration 265, loss = 0.02529986\n",
            "Iteration 266, loss = 0.02518728\n",
            "Iteration 267, loss = 0.02506265\n",
            "Iteration 268, loss = 0.02497276\n",
            "Iteration 269, loss = 0.02489917\n",
            "Iteration 270, loss = 0.02478714\n",
            "Iteration 271, loss = 0.02470392\n",
            "Iteration 272, loss = 0.02467178\n",
            "Iteration 273, loss = 0.02452226\n",
            "Iteration 274, loss = 0.02445879\n",
            "Iteration 275, loss = 0.02431311\n",
            "Iteration 276, loss = 0.02419642\n",
            "Iteration 277, loss = 0.02416459\n",
            "Iteration 278, loss = 0.02407007\n",
            "Iteration 279, loss = 0.02401245\n",
            "Iteration 280, loss = 0.02391724\n",
            "Iteration 281, loss = 0.02378162\n",
            "Iteration 282, loss = 0.02372557\n",
            "Iteration 283, loss = 0.02373659\n",
            "Iteration 284, loss = 0.02356462\n",
            "Iteration 285, loss = 0.02346287\n",
            "Iteration 286, loss = 0.02339511\n",
            "Iteration 287, loss = 0.02326308\n",
            "Iteration 288, loss = 0.02321972\n",
            "Iteration 289, loss = 0.02319914\n",
            "Iteration 290, loss = 0.02308116\n",
            "Iteration 291, loss = 0.02293182\n",
            "Iteration 292, loss = 0.02293778\n",
            "Iteration 293, loss = 0.02281174\n",
            "Iteration 294, loss = 0.02278765\n",
            "Iteration 295, loss = 0.02274014\n",
            "Iteration 296, loss = 0.02257285\n",
            "Iteration 297, loss = 0.02246871\n",
            "Iteration 298, loss = 0.02242021\n",
            "Iteration 299, loss = 0.02237838\n",
            "Iteration 300, loss = 0.02223178\n",
            "Iteration 301, loss = 0.02215352\n",
            "Iteration 302, loss = 0.02219783\n",
            "Iteration 303, loss = 0.02228824\n",
            "Iteration 304, loss = 0.02194586\n",
            "Iteration 305, loss = 0.02207027\n",
            "Iteration 306, loss = 0.02184139\n",
            "Iteration 307, loss = 0.02169520\n",
            "Iteration 308, loss = 0.02177380\n",
            "Iteration 309, loss = 0.02155855\n",
            "Iteration 310, loss = 0.02151506\n",
            "Iteration 311, loss = 0.02142342\n",
            "Iteration 312, loss = 0.02137838\n",
            "Iteration 313, loss = 0.02125988\n",
            "Iteration 314, loss = 0.02119690\n",
            "Iteration 315, loss = 0.02113100\n",
            "Iteration 316, loss = 0.02109331\n",
            "Iteration 317, loss = 0.02104724\n",
            "Iteration 318, loss = 0.02092641\n",
            "Iteration 319, loss = 0.02087371\n",
            "Iteration 320, loss = 0.02086270\n",
            "Iteration 321, loss = 0.02084007\n",
            "Iteration 322, loss = 0.02074004\n",
            "Iteration 323, loss = 0.02056671\n",
            "Iteration 324, loss = 0.02054572\n",
            "Iteration 325, loss = 0.02044865\n",
            "Iteration 326, loss = 0.02037299\n",
            "Iteration 327, loss = 0.02034931\n",
            "Iteration 328, loss = 0.02022367\n",
            "Iteration 329, loss = 0.02018274\n",
            "Iteration 330, loss = 0.02019208\n",
            "Iteration 331, loss = 0.02006407\n",
            "Iteration 332, loss = 0.02003815\n",
            "Iteration 333, loss = 0.01999048\n",
            "Iteration 334, loss = 0.01988040\n",
            "Iteration 335, loss = 0.01987057\n",
            "Iteration 336, loss = 0.01972368\n",
            "Iteration 337, loss = 0.01969783\n",
            "Iteration 338, loss = 0.01967742\n",
            "Iteration 339, loss = 0.01960655\n",
            "Iteration 340, loss = 0.01957049\n",
            "Iteration 341, loss = 0.01957202\n",
            "Iteration 342, loss = 0.01948007\n",
            "Iteration 343, loss = 0.01934470\n",
            "Iteration 344, loss = 0.01927353\n",
            "Iteration 345, loss = 0.01923401\n",
            "Iteration 346, loss = 0.01924525\n",
            "Iteration 347, loss = 0.01910705\n",
            "Iteration 348, loss = 0.01914978\n",
            "Iteration 349, loss = 0.01894541\n",
            "Iteration 350, loss = 0.01901441\n",
            "Iteration 351, loss = 0.01895158\n",
            "Iteration 352, loss = 0.01885989\n",
            "Iteration 353, loss = 0.01890175\n",
            "Iteration 354, loss = 0.01867115\n",
            "Iteration 355, loss = 0.01872862\n",
            "Iteration 356, loss = 0.01861753\n",
            "Iteration 357, loss = 0.01855119\n",
            "Iteration 358, loss = 0.01853219\n",
            "Iteration 359, loss = 0.01851432\n",
            "Iteration 360, loss = 0.01835303\n",
            "Iteration 361, loss = 0.01832605\n",
            "Iteration 362, loss = 0.01831156\n",
            "Iteration 363, loss = 0.01823701\n",
            "Iteration 364, loss = 0.01818816\n",
            "Iteration 365, loss = 0.01813151\n",
            "Iteration 366, loss = 0.01815329\n",
            "Iteration 367, loss = 0.01800591\n",
            "Iteration 368, loss = 0.01795206\n",
            "Iteration 369, loss = 0.01792005\n",
            "Iteration 370, loss = 0.01784133\n",
            "Iteration 371, loss = 0.01775877\n",
            "Iteration 372, loss = 0.01776407\n",
            "Iteration 373, loss = 0.01769673\n",
            "Iteration 374, loss = 0.01763329\n",
            "Iteration 375, loss = 0.01757668\n",
            "Iteration 376, loss = 0.01755187\n",
            "Iteration 377, loss = 0.01755918\n",
            "Iteration 378, loss = 0.01745871\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a rede neural mudando o parametro max_iter, varbose para True e o tol\n",
        "\n",
        "rede_neural_credit = MLPClassifier(max_iter=1000, verbose=True, tol=0.0000100)\n",
        "rede_neural_credit.fit(x_credit_treino, y_credit_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykmPqMFfTjzJ",
        "outputId": "e066c537-aa45-4c0f-fb7a-6b90c05f2556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.81578711\n",
            "Iteration 2, loss = 0.73348065\n",
            "Iteration 3, loss = 0.66106508\n",
            "Iteration 4, loss = 0.59832993\n",
            "Iteration 5, loss = 0.54364491\n",
            "Iteration 6, loss = 0.49653351\n",
            "Iteration 7, loss = 0.45594580\n",
            "Iteration 8, loss = 0.41981340\n",
            "Iteration 9, loss = 0.38850381\n",
            "Iteration 10, loss = 0.36088967\n",
            "Iteration 11, loss = 0.33665032\n",
            "Iteration 12, loss = 0.31530609\n",
            "Iteration 13, loss = 0.29609566\n",
            "Iteration 14, loss = 0.27930972\n",
            "Iteration 15, loss = 0.26410564\n",
            "Iteration 16, loss = 0.25048361\n",
            "Iteration 17, loss = 0.23823516\n",
            "Iteration 18, loss = 0.22717450\n",
            "Iteration 19, loss = 0.21718048\n",
            "Iteration 20, loss = 0.20804224\n",
            "Iteration 21, loss = 0.19978875\n",
            "Iteration 22, loss = 0.19222452\n",
            "Iteration 23, loss = 0.18512573\n",
            "Iteration 24, loss = 0.17868073\n",
            "Iteration 25, loss = 0.17259265\n",
            "Iteration 26, loss = 0.16708947\n",
            "Iteration 27, loss = 0.16197096\n",
            "Iteration 28, loss = 0.15718732\n",
            "Iteration 29, loss = 0.15287589\n",
            "Iteration 30, loss = 0.14874677\n",
            "Iteration 31, loss = 0.14498870\n",
            "Iteration 32, loss = 0.14141705\n",
            "Iteration 33, loss = 0.13805407\n",
            "Iteration 34, loss = 0.13491728\n",
            "Iteration 35, loss = 0.13188498\n",
            "Iteration 36, loss = 0.12909161\n",
            "Iteration 37, loss = 0.12642309\n",
            "Iteration 38, loss = 0.12398079\n",
            "Iteration 39, loss = 0.12150675\n",
            "Iteration 40, loss = 0.11939873\n",
            "Iteration 41, loss = 0.11721371\n",
            "Iteration 42, loss = 0.11527841\n",
            "Iteration 43, loss = 0.11338863\n",
            "Iteration 44, loss = 0.11155605\n",
            "Iteration 45, loss = 0.10988178\n",
            "Iteration 46, loss = 0.10814457\n",
            "Iteration 47, loss = 0.10656690\n",
            "Iteration 48, loss = 0.10509951\n",
            "Iteration 49, loss = 0.10369729\n",
            "Iteration 50, loss = 0.10225020\n",
            "Iteration 51, loss = 0.10088082\n",
            "Iteration 52, loss = 0.09956686\n",
            "Iteration 53, loss = 0.09842445\n",
            "Iteration 54, loss = 0.09716000\n",
            "Iteration 55, loss = 0.09606904\n",
            "Iteration 56, loss = 0.09485476\n",
            "Iteration 57, loss = 0.09376405\n",
            "Iteration 58, loss = 0.09273923\n",
            "Iteration 59, loss = 0.09173784\n",
            "Iteration 60, loss = 0.09072051\n",
            "Iteration 61, loss = 0.08973469\n",
            "Iteration 62, loss = 0.08881432\n",
            "Iteration 63, loss = 0.08796953\n",
            "Iteration 64, loss = 0.08711719\n",
            "Iteration 65, loss = 0.08620112\n",
            "Iteration 66, loss = 0.08536070\n",
            "Iteration 67, loss = 0.08454727\n",
            "Iteration 68, loss = 0.08376611\n",
            "Iteration 69, loss = 0.08297536\n",
            "Iteration 70, loss = 0.08223831\n",
            "Iteration 71, loss = 0.08151242\n",
            "Iteration 72, loss = 0.08072365\n",
            "Iteration 73, loss = 0.08002212\n",
            "Iteration 74, loss = 0.07933319\n",
            "Iteration 75, loss = 0.07866516\n",
            "Iteration 76, loss = 0.07794034\n",
            "Iteration 77, loss = 0.07737664\n",
            "Iteration 78, loss = 0.07670327\n",
            "Iteration 79, loss = 0.07599207\n",
            "Iteration 80, loss = 0.07550775\n",
            "Iteration 81, loss = 0.07471946\n",
            "Iteration 82, loss = 0.07420195\n",
            "Iteration 83, loss = 0.07358029\n",
            "Iteration 84, loss = 0.07286827\n",
            "Iteration 85, loss = 0.07230067\n",
            "Iteration 86, loss = 0.07170101\n",
            "Iteration 87, loss = 0.07119310\n",
            "Iteration 88, loss = 0.07051905\n",
            "Iteration 89, loss = 0.07001683\n",
            "Iteration 90, loss = 0.06940225\n",
            "Iteration 91, loss = 0.06887725\n",
            "Iteration 92, loss = 0.06824969\n",
            "Iteration 93, loss = 0.06777559\n",
            "Iteration 94, loss = 0.06710303\n",
            "Iteration 95, loss = 0.06656064\n",
            "Iteration 96, loss = 0.06604109\n",
            "Iteration 97, loss = 0.06550587\n",
            "Iteration 98, loss = 0.06496234\n",
            "Iteration 99, loss = 0.06453085\n",
            "Iteration 100, loss = 0.06391016\n",
            "Iteration 101, loss = 0.06349147\n",
            "Iteration 102, loss = 0.06302738\n",
            "Iteration 103, loss = 0.06263981\n",
            "Iteration 104, loss = 0.06206794\n",
            "Iteration 105, loss = 0.06160010\n",
            "Iteration 106, loss = 0.06111993\n",
            "Iteration 107, loss = 0.06074983\n",
            "Iteration 108, loss = 0.06030133\n",
            "Iteration 109, loss = 0.05984801\n",
            "Iteration 110, loss = 0.05936163\n",
            "Iteration 111, loss = 0.05899674\n",
            "Iteration 112, loss = 0.05857341\n",
            "Iteration 113, loss = 0.05809597\n",
            "Iteration 114, loss = 0.05765112\n",
            "Iteration 115, loss = 0.05722505\n",
            "Iteration 116, loss = 0.05688055\n",
            "Iteration 117, loss = 0.05641677\n",
            "Iteration 118, loss = 0.05601540\n",
            "Iteration 119, loss = 0.05561507\n",
            "Iteration 120, loss = 0.05516652\n",
            "Iteration 121, loss = 0.05481607\n",
            "Iteration 122, loss = 0.05449360\n",
            "Iteration 123, loss = 0.05403615\n",
            "Iteration 124, loss = 0.05363978\n",
            "Iteration 125, loss = 0.05332932\n",
            "Iteration 126, loss = 0.05294235\n",
            "Iteration 127, loss = 0.05258669\n",
            "Iteration 128, loss = 0.05220819\n",
            "Iteration 129, loss = 0.05190910\n",
            "Iteration 130, loss = 0.05156623\n",
            "Iteration 131, loss = 0.05124796\n",
            "Iteration 132, loss = 0.05087944\n",
            "Iteration 133, loss = 0.05070377\n",
            "Iteration 134, loss = 0.05017754\n",
            "Iteration 135, loss = 0.04986415\n",
            "Iteration 136, loss = 0.04956780\n",
            "Iteration 137, loss = 0.04930184\n",
            "Iteration 138, loss = 0.04895039\n",
            "Iteration 139, loss = 0.04858219\n",
            "Iteration 140, loss = 0.04827786\n",
            "Iteration 141, loss = 0.04798692\n",
            "Iteration 142, loss = 0.04768758\n",
            "Iteration 143, loss = 0.04738068\n",
            "Iteration 144, loss = 0.04709698\n",
            "Iteration 145, loss = 0.04681174\n",
            "Iteration 146, loss = 0.04652791\n",
            "Iteration 147, loss = 0.04635811\n",
            "Iteration 148, loss = 0.04599602\n",
            "Iteration 149, loss = 0.04566283\n",
            "Iteration 150, loss = 0.04543529\n",
            "Iteration 151, loss = 0.04518980\n",
            "Iteration 152, loss = 0.04487858\n",
            "Iteration 153, loss = 0.04456150\n",
            "Iteration 154, loss = 0.04432935\n",
            "Iteration 155, loss = 0.04404048\n",
            "Iteration 156, loss = 0.04384636\n",
            "Iteration 157, loss = 0.04357308\n",
            "Iteration 158, loss = 0.04331790\n",
            "Iteration 159, loss = 0.04306015\n",
            "Iteration 160, loss = 0.04280802\n",
            "Iteration 161, loss = 0.04256046\n",
            "Iteration 162, loss = 0.04236244\n",
            "Iteration 163, loss = 0.04209162\n",
            "Iteration 164, loss = 0.04187104\n",
            "Iteration 165, loss = 0.04174884\n",
            "Iteration 166, loss = 0.04153790\n",
            "Iteration 167, loss = 0.04124758\n",
            "Iteration 168, loss = 0.04099907\n",
            "Iteration 169, loss = 0.04071139\n",
            "Iteration 170, loss = 0.04058145\n",
            "Iteration 171, loss = 0.04030822\n",
            "Iteration 172, loss = 0.04006553\n",
            "Iteration 173, loss = 0.03988198\n",
            "Iteration 174, loss = 0.03972038\n",
            "Iteration 175, loss = 0.03945170\n",
            "Iteration 176, loss = 0.03923774\n",
            "Iteration 177, loss = 0.03905142\n",
            "Iteration 178, loss = 0.03885670\n",
            "Iteration 179, loss = 0.03867504\n",
            "Iteration 180, loss = 0.03849094\n",
            "Iteration 181, loss = 0.03821125\n",
            "Iteration 182, loss = 0.03802323\n",
            "Iteration 183, loss = 0.03789035\n",
            "Iteration 184, loss = 0.03768355\n",
            "Iteration 185, loss = 0.03743818\n",
            "Iteration 186, loss = 0.03743146\n",
            "Iteration 187, loss = 0.03706115\n",
            "Iteration 188, loss = 0.03702099\n",
            "Iteration 189, loss = 0.03676307\n",
            "Iteration 190, loss = 0.03657250\n",
            "Iteration 191, loss = 0.03647327\n",
            "Iteration 192, loss = 0.03619954\n",
            "Iteration 193, loss = 0.03606210\n",
            "Iteration 194, loss = 0.03582616\n",
            "Iteration 195, loss = 0.03564353\n",
            "Iteration 196, loss = 0.03549138\n",
            "Iteration 197, loss = 0.03531158\n",
            "Iteration 198, loss = 0.03515515\n",
            "Iteration 199, loss = 0.03497105\n",
            "Iteration 200, loss = 0.03479503\n",
            "Iteration 201, loss = 0.03466141\n",
            "Iteration 202, loss = 0.03447598\n",
            "Iteration 203, loss = 0.03435771\n",
            "Iteration 204, loss = 0.03416037\n",
            "Iteration 205, loss = 0.03396868\n",
            "Iteration 206, loss = 0.03382243\n",
            "Iteration 207, loss = 0.03365144\n",
            "Iteration 208, loss = 0.03352822\n",
            "Iteration 209, loss = 0.03334434\n",
            "Iteration 210, loss = 0.03320598\n",
            "Iteration 211, loss = 0.03310169\n",
            "Iteration 212, loss = 0.03289229\n",
            "Iteration 213, loss = 0.03274588\n",
            "Iteration 214, loss = 0.03258361\n",
            "Iteration 215, loss = 0.03245769\n",
            "Iteration 216, loss = 0.03236031\n",
            "Iteration 217, loss = 0.03217853\n",
            "Iteration 218, loss = 0.03204303\n",
            "Iteration 219, loss = 0.03190949\n",
            "Iteration 220, loss = 0.03175586\n",
            "Iteration 221, loss = 0.03161925\n",
            "Iteration 222, loss = 0.03141562\n",
            "Iteration 223, loss = 0.03133278\n",
            "Iteration 224, loss = 0.03130896\n",
            "Iteration 225, loss = 0.03105640\n",
            "Iteration 226, loss = 0.03087250\n",
            "Iteration 227, loss = 0.03074797\n",
            "Iteration 228, loss = 0.03059804\n",
            "Iteration 229, loss = 0.03048915\n",
            "Iteration 230, loss = 0.03037149\n",
            "Iteration 231, loss = 0.03025814\n",
            "Iteration 232, loss = 0.03014464\n",
            "Iteration 233, loss = 0.02994083\n",
            "Iteration 234, loss = 0.02989123\n",
            "Iteration 235, loss = 0.02971243\n",
            "Iteration 236, loss = 0.02965816\n",
            "Iteration 237, loss = 0.02949696\n",
            "Iteration 238, loss = 0.02938168\n",
            "Iteration 239, loss = 0.02921033\n",
            "Iteration 240, loss = 0.02913334\n",
            "Iteration 241, loss = 0.02897145\n",
            "Iteration 242, loss = 0.02886035\n",
            "Iteration 243, loss = 0.02884859\n",
            "Iteration 244, loss = 0.02861142\n",
            "Iteration 245, loss = 0.02852120\n",
            "Iteration 246, loss = 0.02838097\n",
            "Iteration 247, loss = 0.02829095\n",
            "Iteration 248, loss = 0.02816462\n",
            "Iteration 249, loss = 0.02803695\n",
            "Iteration 250, loss = 0.02802942\n",
            "Iteration 251, loss = 0.02790623\n",
            "Iteration 252, loss = 0.02774526\n",
            "Iteration 253, loss = 0.02760726\n",
            "Iteration 254, loss = 0.02750453\n",
            "Iteration 255, loss = 0.02752046\n",
            "Iteration 256, loss = 0.02725962\n",
            "Iteration 257, loss = 0.02725432\n",
            "Iteration 258, loss = 0.02710011\n",
            "Iteration 259, loss = 0.02701525\n",
            "Iteration 260, loss = 0.02689698\n",
            "Iteration 261, loss = 0.02682088\n",
            "Iteration 262, loss = 0.02673219\n",
            "Iteration 263, loss = 0.02653399\n",
            "Iteration 264, loss = 0.02656314\n",
            "Iteration 265, loss = 0.02642098\n",
            "Iteration 266, loss = 0.02624809\n",
            "Iteration 267, loss = 0.02620279\n",
            "Iteration 268, loss = 0.02606168\n",
            "Iteration 269, loss = 0.02606683\n",
            "Iteration 270, loss = 0.02596963\n",
            "Iteration 271, loss = 0.02575600\n",
            "Iteration 272, loss = 0.02572823\n",
            "Iteration 273, loss = 0.02566992\n",
            "Iteration 274, loss = 0.02555004\n",
            "Iteration 275, loss = 0.02542303\n",
            "Iteration 276, loss = 0.02530484\n",
            "Iteration 277, loss = 0.02526170\n",
            "Iteration 278, loss = 0.02513648\n",
            "Iteration 279, loss = 0.02514129\n",
            "Iteration 280, loss = 0.02495569\n",
            "Iteration 281, loss = 0.02486943\n",
            "Iteration 282, loss = 0.02485489\n",
            "Iteration 283, loss = 0.02467850\n",
            "Iteration 284, loss = 0.02461290\n",
            "Iteration 285, loss = 0.02454638\n",
            "Iteration 286, loss = 0.02441495\n",
            "Iteration 287, loss = 0.02442673\n",
            "Iteration 288, loss = 0.02431969\n",
            "Iteration 289, loss = 0.02420106\n",
            "Iteration 290, loss = 0.02409161\n",
            "Iteration 291, loss = 0.02398634\n",
            "Iteration 292, loss = 0.02397329\n",
            "Iteration 293, loss = 0.02393377\n",
            "Iteration 294, loss = 0.02380065\n",
            "Iteration 295, loss = 0.02373663\n",
            "Iteration 296, loss = 0.02360585\n",
            "Iteration 297, loss = 0.02350177\n",
            "Iteration 298, loss = 0.02343915\n",
            "Iteration 299, loss = 0.02341142\n",
            "Iteration 300, loss = 0.02327571\n",
            "Iteration 301, loss = 0.02325780\n",
            "Iteration 302, loss = 0.02319002\n",
            "Iteration 303, loss = 0.02310356\n",
            "Iteration 304, loss = 0.02299094\n",
            "Iteration 305, loss = 0.02287426\n",
            "Iteration 306, loss = 0.02281082\n",
            "Iteration 307, loss = 0.02273733\n",
            "Iteration 308, loss = 0.02268357\n",
            "Iteration 309, loss = 0.02257695\n",
            "Iteration 310, loss = 0.02251581\n",
            "Iteration 311, loss = 0.02241109\n",
            "Iteration 312, loss = 0.02234975\n",
            "Iteration 313, loss = 0.02226432\n",
            "Iteration 314, loss = 0.02221279\n",
            "Iteration 315, loss = 0.02217318\n",
            "Iteration 316, loss = 0.02214291\n",
            "Iteration 317, loss = 0.02196745\n",
            "Iteration 318, loss = 0.02193350\n",
            "Iteration 319, loss = 0.02191543\n",
            "Iteration 320, loss = 0.02184852\n",
            "Iteration 321, loss = 0.02179778\n",
            "Iteration 322, loss = 0.02173493\n",
            "Iteration 323, loss = 0.02164605\n",
            "Iteration 324, loss = 0.02150313\n",
            "Iteration 325, loss = 0.02148026\n",
            "Iteration 326, loss = 0.02148086\n",
            "Iteration 327, loss = 0.02129977\n",
            "Iteration 328, loss = 0.02120077\n",
            "Iteration 329, loss = 0.02121860\n",
            "Iteration 330, loss = 0.02110069\n",
            "Iteration 331, loss = 0.02101440\n",
            "Iteration 332, loss = 0.02098546\n",
            "Iteration 333, loss = 0.02095115\n",
            "Iteration 334, loss = 0.02081036\n",
            "Iteration 335, loss = 0.02075865\n",
            "Iteration 336, loss = 0.02068278\n",
            "Iteration 337, loss = 0.02061151\n",
            "Iteration 338, loss = 0.02058466\n",
            "Iteration 339, loss = 0.02054397\n",
            "Iteration 340, loss = 0.02043230\n",
            "Iteration 341, loss = 0.02043120\n",
            "Iteration 342, loss = 0.02034177\n",
            "Iteration 343, loss = 0.02021775\n",
            "Iteration 344, loss = 0.02018187\n",
            "Iteration 345, loss = 0.02012746\n",
            "Iteration 346, loss = 0.02011038\n",
            "Iteration 347, loss = 0.02007457\n",
            "Iteration 348, loss = 0.01998999\n",
            "Iteration 349, loss = 0.01988878\n",
            "Iteration 350, loss = 0.01986549\n",
            "Iteration 351, loss = 0.01975689\n",
            "Iteration 352, loss = 0.01970273\n",
            "Iteration 353, loss = 0.01969687\n",
            "Iteration 354, loss = 0.01956647\n",
            "Iteration 355, loss = 0.01953430\n",
            "Iteration 356, loss = 0.01946951\n",
            "Iteration 357, loss = 0.01939087\n",
            "Iteration 358, loss = 0.01939492\n",
            "Iteration 359, loss = 0.01930841\n",
            "Iteration 360, loss = 0.01925048\n",
            "Iteration 361, loss = 0.01911865\n",
            "Iteration 362, loss = 0.01913287\n",
            "Iteration 363, loss = 0.01904109\n",
            "Iteration 364, loss = 0.01902261\n",
            "Iteration 365, loss = 0.01906487\n",
            "Iteration 366, loss = 0.01888505\n",
            "Iteration 367, loss = 0.01894521\n",
            "Iteration 368, loss = 0.01876417\n",
            "Iteration 369, loss = 0.01872027\n",
            "Iteration 370, loss = 0.01867273\n",
            "Iteration 371, loss = 0.01862468\n",
            "Iteration 372, loss = 0.01866147\n",
            "Iteration 373, loss = 0.01861151\n",
            "Iteration 374, loss = 0.01856703\n",
            "Iteration 375, loss = 0.01843529\n",
            "Iteration 376, loss = 0.01837308\n",
            "Iteration 377, loss = 0.01827692\n",
            "Iteration 378, loss = 0.01826944\n",
            "Iteration 379, loss = 0.01820128\n",
            "Iteration 380, loss = 0.01815770\n",
            "Iteration 381, loss = 0.01808175\n",
            "Iteration 382, loss = 0.01801502\n",
            "Iteration 383, loss = 0.01797662\n",
            "Iteration 384, loss = 0.01796136\n",
            "Iteration 385, loss = 0.01791134\n",
            "Iteration 386, loss = 0.01784293\n",
            "Iteration 387, loss = 0.01775703\n",
            "Iteration 388, loss = 0.01773915\n",
            "Iteration 389, loss = 0.01771828\n",
            "Iteration 390, loss = 0.01765218\n",
            "Iteration 391, loss = 0.01762722\n",
            "Iteration 392, loss = 0.01755580\n",
            "Iteration 393, loss = 0.01752852\n",
            "Iteration 394, loss = 0.01741925\n",
            "Iteration 395, loss = 0.01743243\n",
            "Iteration 396, loss = 0.01738269\n",
            "Iteration 397, loss = 0.01732378\n",
            "Iteration 398, loss = 0.01723747\n",
            "Iteration 399, loss = 0.01724744\n",
            "Iteration 400, loss = 0.01720533\n",
            "Iteration 401, loss = 0.01714653\n",
            "Iteration 402, loss = 0.01708371\n",
            "Iteration 403, loss = 0.01705274\n",
            "Iteration 404, loss = 0.01699992\n",
            "Iteration 405, loss = 0.01701134\n",
            "Iteration 406, loss = 0.01692287\n",
            "Iteration 407, loss = 0.01690493\n",
            "Iteration 408, loss = 0.01685447\n",
            "Iteration 409, loss = 0.01686108\n",
            "Iteration 410, loss = 0.01671508\n",
            "Iteration 411, loss = 0.01663423\n",
            "Iteration 412, loss = 0.01667057\n",
            "Iteration 413, loss = 0.01661916\n",
            "Iteration 414, loss = 0.01653468\n",
            "Iteration 415, loss = 0.01649514\n",
            "Iteration 416, loss = 0.01658336\n",
            "Iteration 417, loss = 0.01645686\n",
            "Iteration 418, loss = 0.01637209\n",
            "Iteration 419, loss = 0.01629383\n",
            "Iteration 420, loss = 0.01632445\n",
            "Iteration 421, loss = 0.01637562\n",
            "Iteration 422, loss = 0.01619214\n",
            "Iteration 423, loss = 0.01620435\n",
            "Iteration 424, loss = 0.01609987\n",
            "Iteration 425, loss = 0.01610559\n",
            "Iteration 426, loss = 0.01601568\n",
            "Iteration 427, loss = 0.01606718\n",
            "Iteration 428, loss = 0.01594162\n",
            "Iteration 429, loss = 0.01593433\n",
            "Iteration 430, loss = 0.01591391\n",
            "Iteration 431, loss = 0.01582856\n",
            "Iteration 432, loss = 0.01574292\n",
            "Iteration 433, loss = 0.01571206\n",
            "Iteration 434, loss = 0.01572587\n",
            "Iteration 435, loss = 0.01567935\n",
            "Iteration 436, loss = 0.01559877\n",
            "Iteration 437, loss = 0.01557321\n",
            "Iteration 438, loss = 0.01552064\n",
            "Iteration 439, loss = 0.01554546\n",
            "Iteration 440, loss = 0.01547585\n",
            "Iteration 441, loss = 0.01541312\n",
            "Iteration 442, loss = 0.01536428\n",
            "Iteration 443, loss = 0.01539894\n",
            "Iteration 444, loss = 0.01531517\n",
            "Iteration 445, loss = 0.01529351\n",
            "Iteration 446, loss = 0.01523347\n",
            "Iteration 447, loss = 0.01518306\n",
            "Iteration 448, loss = 0.01518537\n",
            "Iteration 449, loss = 0.01519122\n",
            "Iteration 450, loss = 0.01513913\n",
            "Iteration 451, loss = 0.01508672\n",
            "Iteration 452, loss = 0.01497233\n",
            "Iteration 453, loss = 0.01518051\n",
            "Iteration 454, loss = 0.01496058\n",
            "Iteration 455, loss = 0.01489587\n",
            "Iteration 456, loss = 0.01486123\n",
            "Iteration 457, loss = 0.01491651\n",
            "Iteration 458, loss = 0.01480185\n",
            "Iteration 459, loss = 0.01478432\n",
            "Iteration 460, loss = 0.01473976\n",
            "Iteration 461, loss = 0.01466851\n",
            "Iteration 462, loss = 0.01462677\n",
            "Iteration 463, loss = 0.01461970\n",
            "Iteration 464, loss = 0.01456615\n",
            "Iteration 465, loss = 0.01456843\n",
            "Iteration 466, loss = 0.01446383\n",
            "Iteration 467, loss = 0.01445603\n",
            "Iteration 468, loss = 0.01441360\n",
            "Iteration 469, loss = 0.01442264\n",
            "Iteration 470, loss = 0.01435756\n",
            "Iteration 471, loss = 0.01430429\n",
            "Iteration 472, loss = 0.01426830\n",
            "Iteration 473, loss = 0.01424989\n",
            "Iteration 474, loss = 0.01425308\n",
            "Iteration 475, loss = 0.01420948\n",
            "Iteration 476, loss = 0.01416200\n",
            "Iteration 477, loss = 0.01411896\n",
            "Iteration 478, loss = 0.01405633\n",
            "Iteration 479, loss = 0.01402348\n",
            "Iteration 480, loss = 0.01401200\n",
            "Iteration 481, loss = 0.01400536\n",
            "Iteration 482, loss = 0.01396038\n",
            "Iteration 483, loss = 0.01394254\n",
            "Iteration 484, loss = 0.01394398\n",
            "Iteration 485, loss = 0.01386082\n",
            "Iteration 486, loss = 0.01383110\n",
            "Iteration 487, loss = 0.01384078\n",
            "Iteration 488, loss = 0.01372900\n",
            "Iteration 489, loss = 0.01373757\n",
            "Iteration 490, loss = 0.01376644\n",
            "Iteration 491, loss = 0.01369937\n",
            "Iteration 492, loss = 0.01363506\n",
            "Iteration 493, loss = 0.01372325\n",
            "Iteration 494, loss = 0.01359179\n",
            "Iteration 495, loss = 0.01356943\n",
            "Iteration 496, loss = 0.01353703\n",
            "Iteration 497, loss = 0.01348071\n",
            "Iteration 498, loss = 0.01346950\n",
            "Iteration 499, loss = 0.01340983\n",
            "Iteration 500, loss = 0.01346756\n",
            "Iteration 501, loss = 0.01335062\n",
            "Iteration 502, loss = 0.01333446\n",
            "Iteration 503, loss = 0.01331220\n",
            "Iteration 504, loss = 0.01330567\n",
            "Iteration 505, loss = 0.01325843\n",
            "Iteration 506, loss = 0.01324057\n",
            "Iteration 507, loss = 0.01327309\n",
            "Iteration 508, loss = 0.01314473\n",
            "Iteration 509, loss = 0.01313904\n",
            "Iteration 510, loss = 0.01309905\n",
            "Iteration 511, loss = 0.01307749\n",
            "Iteration 512, loss = 0.01303493\n",
            "Iteration 513, loss = 0.01296749\n",
            "Iteration 514, loss = 0.01294758\n",
            "Iteration 515, loss = 0.01292871\n",
            "Iteration 516, loss = 0.01300925\n",
            "Iteration 517, loss = 0.01287420\n",
            "Iteration 518, loss = 0.01284995\n",
            "Iteration 519, loss = 0.01276341\n",
            "Iteration 520, loss = 0.01276027\n",
            "Iteration 521, loss = 0.01275141\n",
            "Iteration 522, loss = 0.01270033\n",
            "Iteration 523, loss = 0.01270097\n",
            "Iteration 524, loss = 0.01267890\n",
            "Iteration 525, loss = 0.01273913\n",
            "Iteration 526, loss = 0.01268217\n",
            "Iteration 527, loss = 0.01262808\n",
            "Iteration 528, loss = 0.01255857\n",
            "Iteration 529, loss = 0.01254188\n",
            "Iteration 530, loss = 0.01249794\n",
            "Iteration 531, loss = 0.01250782\n",
            "Iteration 532, loss = 0.01246064\n",
            "Iteration 533, loss = 0.01246561\n",
            "Iteration 534, loss = 0.01236356\n",
            "Iteration 535, loss = 0.01246160\n",
            "Iteration 536, loss = 0.01230232\n",
            "Iteration 537, loss = 0.01247564\n",
            "Iteration 538, loss = 0.01239197\n",
            "Iteration 539, loss = 0.01236081\n",
            "Iteration 540, loss = 0.01220209\n",
            "Iteration 541, loss = 0.01223079\n",
            "Iteration 542, loss = 0.01222931\n",
            "Iteration 543, loss = 0.01215522\n",
            "Iteration 544, loss = 0.01212488\n",
            "Iteration 545, loss = 0.01211282\n",
            "Iteration 546, loss = 0.01205973\n",
            "Iteration 547, loss = 0.01203616\n",
            "Iteration 548, loss = 0.01201023\n",
            "Iteration 549, loss = 0.01193142\n",
            "Iteration 550, loss = 0.01200675\n",
            "Iteration 551, loss = 0.01199069\n",
            "Iteration 552, loss = 0.01194352\n",
            "Iteration 553, loss = 0.01190503\n",
            "Iteration 554, loss = 0.01189860\n",
            "Iteration 555, loss = 0.01185578\n",
            "Iteration 556, loss = 0.01178640\n",
            "Iteration 557, loss = 0.01182421\n",
            "Iteration 558, loss = 0.01178956\n",
            "Iteration 559, loss = 0.01183986\n",
            "Iteration 560, loss = 0.01177100\n",
            "Iteration 561, loss = 0.01173967\n",
            "Iteration 562, loss = 0.01168034\n",
            "Iteration 563, loss = 0.01160115\n",
            "Iteration 564, loss = 0.01159211\n",
            "Iteration 565, loss = 0.01159234\n",
            "Iteration 566, loss = 0.01161095\n",
            "Iteration 567, loss = 0.01153013\n",
            "Iteration 568, loss = 0.01154244\n",
            "Iteration 569, loss = 0.01150929\n",
            "Iteration 570, loss = 0.01149165\n",
            "Iteration 571, loss = 0.01148872\n",
            "Iteration 572, loss = 0.01147900\n",
            "Iteration 573, loss = 0.01137878\n",
            "Iteration 574, loss = 0.01146222\n",
            "Iteration 575, loss = 0.01133529\n",
            "Iteration 576, loss = 0.01135652\n",
            "Iteration 577, loss = 0.01127872\n",
            "Iteration 578, loss = 0.01122726\n",
            "Iteration 579, loss = 0.01123612\n",
            "Iteration 580, loss = 0.01123200\n",
            "Iteration 581, loss = 0.01124162\n",
            "Iteration 582, loss = 0.01114287\n",
            "Iteration 583, loss = 0.01117813\n",
            "Iteration 584, loss = 0.01116756\n",
            "Iteration 585, loss = 0.01110649\n",
            "Iteration 586, loss = 0.01109411\n",
            "Iteration 587, loss = 0.01106722\n",
            "Iteration 588, loss = 0.01110127\n",
            "Iteration 589, loss = 0.01102348\n",
            "Iteration 590, loss = 0.01096696\n",
            "Iteration 591, loss = 0.01097018\n",
            "Iteration 592, loss = 0.01089904\n",
            "Iteration 593, loss = 0.01091087\n",
            "Iteration 594, loss = 0.01085826\n",
            "Iteration 595, loss = 0.01089275\n",
            "Iteration 596, loss = 0.01092553\n",
            "Iteration 597, loss = 0.01081732\n",
            "Iteration 598, loss = 0.01081711\n",
            "Iteration 599, loss = 0.01082977\n",
            "Iteration 600, loss = 0.01080262\n",
            "Iteration 601, loss = 0.01072808\n",
            "Iteration 602, loss = 0.01069495\n",
            "Iteration 603, loss = 0.01067763\n",
            "Iteration 604, loss = 0.01067974\n",
            "Iteration 605, loss = 0.01064740\n",
            "Iteration 606, loss = 0.01062540\n",
            "Iteration 607, loss = 0.01060631\n",
            "Iteration 608, loss = 0.01065002\n",
            "Iteration 609, loss = 0.01055116\n",
            "Iteration 610, loss = 0.01051355\n",
            "Iteration 611, loss = 0.01053868\n",
            "Iteration 612, loss = 0.01048177\n",
            "Iteration 613, loss = 0.01048281\n",
            "Iteration 614, loss = 0.01046692\n",
            "Iteration 615, loss = 0.01043807\n",
            "Iteration 616, loss = 0.01040798\n",
            "Iteration 617, loss = 0.01036766\n",
            "Iteration 618, loss = 0.01039194\n",
            "Iteration 619, loss = 0.01038045\n",
            "Iteration 620, loss = 0.01035066\n",
            "Iteration 621, loss = 0.01032031\n",
            "Iteration 622, loss = 0.01033339\n",
            "Iteration 623, loss = 0.01025197\n",
            "Iteration 624, loss = 0.01027622\n",
            "Iteration 625, loss = 0.01026382\n",
            "Iteration 626, loss = 0.01028075\n",
            "Iteration 627, loss = 0.01025515\n",
            "Iteration 628, loss = 0.01018934\n",
            "Iteration 629, loss = 0.01012920\n",
            "Iteration 630, loss = 0.01010640\n",
            "Iteration 631, loss = 0.01014039\n",
            "Iteration 632, loss = 0.01012659\n",
            "Iteration 633, loss = 0.01012325\n",
            "Iteration 634, loss = 0.01008563\n",
            "Iteration 635, loss = 0.01007685\n",
            "Iteration 636, loss = 0.01004112\n",
            "Iteration 637, loss = 0.00994963\n",
            "Iteration 638, loss = 0.01009287\n",
            "Iteration 639, loss = 0.00994788\n",
            "Iteration 640, loss = 0.00989370\n",
            "Iteration 641, loss = 0.00997425\n",
            "Iteration 642, loss = 0.00993718\n",
            "Iteration 643, loss = 0.00985824\n",
            "Iteration 644, loss = 0.00984406\n",
            "Iteration 645, loss = 0.00983373\n",
            "Iteration 646, loss = 0.00977863\n",
            "Iteration 647, loss = 0.00982483\n",
            "Iteration 648, loss = 0.00974307\n",
            "Iteration 649, loss = 0.00973261\n",
            "Iteration 650, loss = 0.00970447\n",
            "Iteration 651, loss = 0.00977362\n",
            "Iteration 652, loss = 0.00972260\n",
            "Iteration 653, loss = 0.00964364\n",
            "Iteration 654, loss = 0.00961764\n",
            "Iteration 655, loss = 0.00963102\n",
            "Iteration 656, loss = 0.00959377\n",
            "Iteration 657, loss = 0.00957664\n",
            "Iteration 658, loss = 0.00960730\n",
            "Iteration 659, loss = 0.00952288\n",
            "Iteration 660, loss = 0.00954171\n",
            "Iteration 661, loss = 0.00952225\n",
            "Iteration 662, loss = 0.00952162\n",
            "Iteration 663, loss = 0.00947859\n",
            "Iteration 664, loss = 0.00945266\n",
            "Iteration 665, loss = 0.00943772\n",
            "Iteration 666, loss = 0.00944123\n",
            "Iteration 667, loss = 0.00940025\n",
            "Iteration 668, loss = 0.00937291\n",
            "Iteration 669, loss = 0.00946937\n",
            "Iteration 670, loss = 0.00939529\n",
            "Iteration 671, loss = 0.00933374\n",
            "Iteration 672, loss = 0.00936289\n",
            "Iteration 673, loss = 0.00936414\n",
            "Iteration 674, loss = 0.00926176\n",
            "Iteration 675, loss = 0.00926228\n",
            "Iteration 676, loss = 0.00925791\n",
            "Iteration 677, loss = 0.00925011\n",
            "Iteration 678, loss = 0.00923452\n",
            "Iteration 679, loss = 0.00917475\n",
            "Iteration 680, loss = 0.00919614\n",
            "Iteration 681, loss = 0.00919658\n",
            "Iteration 682, loss = 0.00921832\n",
            "Iteration 683, loss = 0.00915044\n",
            "Iteration 684, loss = 0.00926086\n",
            "Iteration 685, loss = 0.00911744\n",
            "Iteration 686, loss = 0.00903425\n",
            "Iteration 687, loss = 0.00909213\n",
            "Iteration 688, loss = 0.00907588\n",
            "Iteration 689, loss = 0.00909514\n",
            "Iteration 690, loss = 0.00900523\n",
            "Iteration 691, loss = 0.00904187\n",
            "Iteration 692, loss = 0.00892854\n",
            "Iteration 693, loss = 0.00906621\n",
            "Iteration 694, loss = 0.00900317\n",
            "Iteration 695, loss = 0.00904105\n",
            "Iteration 696, loss = 0.00890962\n",
            "Iteration 697, loss = 0.00892228\n",
            "Iteration 698, loss = 0.00888066\n",
            "Iteration 699, loss = 0.00884587\n",
            "Iteration 700, loss = 0.00890581\n",
            "Iteration 701, loss = 0.00879840\n",
            "Iteration 702, loss = 0.00880195\n",
            "Iteration 703, loss = 0.00875687\n",
            "Iteration 704, loss = 0.00882722\n",
            "Iteration 705, loss = 0.00878774\n",
            "Iteration 706, loss = 0.00876811\n",
            "Iteration 707, loss = 0.00879299\n",
            "Iteration 708, loss = 0.00875717\n",
            "Iteration 709, loss = 0.00870161\n",
            "Iteration 710, loss = 0.00872844\n",
            "Iteration 711, loss = 0.00872145\n",
            "Iteration 712, loss = 0.00866000\n",
            "Iteration 713, loss = 0.00861644\n",
            "Iteration 714, loss = 0.00864677\n",
            "Iteration 715, loss = 0.00858439\n",
            "Iteration 716, loss = 0.00857406\n",
            "Iteration 717, loss = 0.00854503\n",
            "Iteration 718, loss = 0.00854741\n",
            "Iteration 719, loss = 0.00858909\n",
            "Iteration 720, loss = 0.00852256\n",
            "Iteration 721, loss = 0.00849522\n",
            "Iteration 722, loss = 0.00844202\n",
            "Iteration 723, loss = 0.00863075\n",
            "Iteration 724, loss = 0.00839891\n",
            "Iteration 725, loss = 0.00842638\n",
            "Iteration 726, loss = 0.00842303\n",
            "Iteration 727, loss = 0.00838673\n",
            "Iteration 728, loss = 0.00843905\n",
            "Iteration 729, loss = 0.00842102\n",
            "Iteration 730, loss = 0.00837275\n",
            "Iteration 731, loss = 0.00841349\n",
            "Iteration 732, loss = 0.00831763\n",
            "Iteration 733, loss = 0.00843830\n",
            "Iteration 734, loss = 0.00825573\n",
            "Iteration 735, loss = 0.00829271\n",
            "Iteration 736, loss = 0.00828817\n",
            "Iteration 737, loss = 0.00834251\n",
            "Iteration 738, loss = 0.00836529\n",
            "Iteration 739, loss = 0.00827219\n",
            "Iteration 740, loss = 0.00826953\n",
            "Iteration 741, loss = 0.00827831\n",
            "Iteration 742, loss = 0.00816478\n",
            "Iteration 743, loss = 0.00818430\n",
            "Iteration 744, loss = 0.00812963\n",
            "Iteration 745, loss = 0.00811490\n",
            "Iteration 746, loss = 0.00812970\n",
            "Iteration 747, loss = 0.00810045\n",
            "Iteration 748, loss = 0.00812293\n",
            "Iteration 749, loss = 0.00809173\n",
            "Iteration 750, loss = 0.00808438\n",
            "Iteration 751, loss = 0.00810343\n",
            "Iteration 752, loss = 0.00804671\n",
            "Iteration 753, loss = 0.00797971\n",
            "Iteration 754, loss = 0.00806868\n",
            "Iteration 755, loss = 0.00797864\n",
            "Iteration 756, loss = 0.00793798\n",
            "Iteration 757, loss = 0.00797921\n",
            "Iteration 758, loss = 0.00797245\n",
            "Iteration 759, loss = 0.00795438\n",
            "Iteration 760, loss = 0.00798081\n",
            "Iteration 761, loss = 0.00794740\n",
            "Iteration 762, loss = 0.00792349\n",
            "Iteration 763, loss = 0.00788120\n",
            "Iteration 764, loss = 0.00800277\n",
            "Iteration 765, loss = 0.00789807\n",
            "Iteration 766, loss = 0.00783405\n",
            "Iteration 767, loss = 0.00779309\n",
            "Iteration 768, loss = 0.00782144\n",
            "Iteration 769, loss = 0.00782749\n",
            "Iteration 770, loss = 0.00778401\n",
            "Iteration 771, loss = 0.00775018\n",
            "Iteration 772, loss = 0.00781900\n",
            "Iteration 773, loss = 0.00774963\n",
            "Iteration 774, loss = 0.00771215\n",
            "Iteration 775, loss = 0.00766996\n",
            "Iteration 776, loss = 0.00769905\n",
            "Iteration 777, loss = 0.00769252\n",
            "Iteration 778, loss = 0.00768928\n",
            "Iteration 779, loss = 0.00769735\n",
            "Iteration 780, loss = 0.00771436\n",
            "Iteration 781, loss = 0.00763198\n",
            "Iteration 782, loss = 0.00763292\n",
            "Iteration 783, loss = 0.00760456\n",
            "Iteration 784, loss = 0.00757528\n",
            "Iteration 785, loss = 0.00755577\n",
            "Iteration 786, loss = 0.00752643\n",
            "Iteration 787, loss = 0.00753621\n",
            "Iteration 788, loss = 0.00755541\n",
            "Iteration 789, loss = 0.00751393\n",
            "Iteration 790, loss = 0.00756313\n",
            "Iteration 791, loss = 0.00744665\n",
            "Iteration 792, loss = 0.00762279\n",
            "Iteration 793, loss = 0.00744719\n",
            "Iteration 794, loss = 0.00748945\n",
            "Iteration 795, loss = 0.00751569\n",
            "Iteration 796, loss = 0.00741658\n",
            "Iteration 797, loss = 0.00744885\n",
            "Iteration 798, loss = 0.00742113\n",
            "Iteration 799, loss = 0.00739063\n",
            "Iteration 800, loss = 0.00737363\n",
            "Iteration 801, loss = 0.00766147\n",
            "Iteration 802, loss = 0.00738526\n",
            "Iteration 803, loss = 0.00734104\n",
            "Iteration 804, loss = 0.00747185\n",
            "Iteration 805, loss = 0.00728002\n",
            "Iteration 806, loss = 0.00759736\n",
            "Iteration 807, loss = 0.00748350\n",
            "Iteration 808, loss = 0.00729286\n",
            "Iteration 809, loss = 0.00735750\n",
            "Iteration 810, loss = 0.00724936\n",
            "Iteration 811, loss = 0.00727494\n",
            "Iteration 812, loss = 0.00730136\n",
            "Iteration 813, loss = 0.00719167\n",
            "Iteration 814, loss = 0.00731254\n",
            "Iteration 815, loss = 0.00717576\n",
            "Iteration 816, loss = 0.00718186\n",
            "Iteration 817, loss = 0.00723590\n",
            "Iteration 818, loss = 0.00715217\n",
            "Iteration 819, loss = 0.00726861\n",
            "Iteration 820, loss = 0.00717893\n",
            "Iteration 821, loss = 0.00713702\n",
            "Iteration 822, loss = 0.00710430\n",
            "Iteration 823, loss = 0.00708393\n",
            "Iteration 824, loss = 0.00719845\n",
            "Iteration 825, loss = 0.00705527\n",
            "Iteration 826, loss = 0.00724301\n",
            "Iteration 827, loss = 0.00707295\n",
            "Iteration 828, loss = 0.00707645\n",
            "Iteration 829, loss = 0.00704573\n",
            "Iteration 830, loss = 0.00702765\n",
            "Iteration 831, loss = 0.00707381\n",
            "Iteration 832, loss = 0.00706411\n",
            "Iteration 833, loss = 0.00699731\n",
            "Iteration 834, loss = 0.00696128\n",
            "Iteration 835, loss = 0.00699046\n",
            "Iteration 836, loss = 0.00695717\n",
            "Iteration 837, loss = 0.00696761\n",
            "Iteration 838, loss = 0.00691545\n",
            "Iteration 839, loss = 0.00690992\n",
            "Iteration 840, loss = 0.00690595\n",
            "Iteration 841, loss = 0.00686064\n",
            "Iteration 842, loss = 0.00687848\n",
            "Iteration 843, loss = 0.00684792\n",
            "Iteration 844, loss = 0.00689227\n",
            "Iteration 845, loss = 0.00686632\n",
            "Iteration 846, loss = 0.00695575\n",
            "Iteration 847, loss = 0.00696867\n",
            "Iteration 848, loss = 0.00685440\n",
            "Iteration 849, loss = 0.00678271\n",
            "Iteration 850, loss = 0.00678750\n",
            "Iteration 851, loss = 0.00683691\n",
            "Iteration 852, loss = 0.00683589\n",
            "Iteration 853, loss = 0.00677327\n",
            "Iteration 854, loss = 0.00673091\n",
            "Iteration 855, loss = 0.00675671\n",
            "Iteration 856, loss = 0.00674123\n",
            "Iteration 857, loss = 0.00668459\n",
            "Iteration 858, loss = 0.00675098\n",
            "Iteration 859, loss = 0.00671794\n",
            "Iteration 860, loss = 0.00675818\n",
            "Iteration 861, loss = 0.00673178\n",
            "Iteration 862, loss = 0.00668443\n",
            "Iteration 863, loss = 0.00670331\n",
            "Iteration 864, loss = 0.00669834\n",
            "Iteration 865, loss = 0.00659465\n",
            "Iteration 866, loss = 0.00664783\n",
            "Iteration 867, loss = 0.00660899\n",
            "Iteration 868, loss = 0.00662622\n",
            "Iteration 869, loss = 0.00663352\n",
            "Iteration 870, loss = 0.00654493\n",
            "Iteration 871, loss = 0.00662272\n",
            "Iteration 872, loss = 0.00653925\n",
            "Iteration 873, loss = 0.00652703\n",
            "Iteration 874, loss = 0.00656180\n",
            "Iteration 875, loss = 0.00656947\n",
            "Iteration 876, loss = 0.00648430\n",
            "Iteration 877, loss = 0.00659038\n",
            "Iteration 878, loss = 0.00650900\n",
            "Iteration 879, loss = 0.00647875\n",
            "Iteration 880, loss = 0.00646454\n",
            "Iteration 881, loss = 0.00646949\n",
            "Iteration 882, loss = 0.00644495\n",
            "Iteration 883, loss = 0.00642058\n",
            "Iteration 884, loss = 0.00645457\n",
            "Iteration 885, loss = 0.00659688\n",
            "Iteration 886, loss = 0.00653943\n",
            "Iteration 887, loss = 0.00638342\n",
            "Iteration 888, loss = 0.00639434\n",
            "Iteration 889, loss = 0.00635550\n",
            "Iteration 890, loss = 0.00634010\n",
            "Iteration 891, loss = 0.00633914\n",
            "Iteration 892, loss = 0.00631363\n",
            "Iteration 893, loss = 0.00632867\n",
            "Iteration 894, loss = 0.00633281\n",
            "Iteration 895, loss = 0.00631204\n",
            "Iteration 896, loss = 0.00628186\n",
            "Iteration 897, loss = 0.00626518\n",
            "Iteration 898, loss = 0.00629519\n",
            "Iteration 899, loss = 0.00627067\n",
            "Iteration 900, loss = 0.00621757\n",
            "Iteration 901, loss = 0.00640269\n",
            "Iteration 902, loss = 0.00624778\n",
            "Iteration 903, loss = 0.00624287\n",
            "Iteration 904, loss = 0.00625707\n",
            "Iteration 905, loss = 0.00621135\n",
            "Iteration 906, loss = 0.00620551\n",
            "Iteration 907, loss = 0.00631297\n",
            "Iteration 908, loss = 0.00622327\n",
            "Iteration 909, loss = 0.00622594\n",
            "Iteration 910, loss = 0.00614071\n",
            "Iteration 911, loss = 0.00618550\n",
            "Iteration 912, loss = 0.00621079\n",
            "Iteration 913, loss = 0.00609962\n",
            "Iteration 914, loss = 0.00610539\n",
            "Iteration 915, loss = 0.00611807\n",
            "Iteration 916, loss = 0.00610811\n",
            "Iteration 917, loss = 0.00606845\n",
            "Iteration 918, loss = 0.00608197\n",
            "Iteration 919, loss = 0.00610794\n",
            "Iteration 920, loss = 0.00605736\n",
            "Iteration 921, loss = 0.00610743\n",
            "Iteration 922, loss = 0.00607759\n",
            "Iteration 923, loss = 0.00612819\n",
            "Iteration 924, loss = 0.00600794\n",
            "Iteration 925, loss = 0.00607134\n",
            "Iteration 926, loss = 0.00603794\n",
            "Iteration 927, loss = 0.00601310\n",
            "Iteration 928, loss = 0.00598641\n",
            "Iteration 929, loss = 0.00597461\n",
            "Iteration 930, loss = 0.00598269\n",
            "Iteration 931, loss = 0.00597673\n",
            "Iteration 932, loss = 0.00599168\n",
            "Iteration 933, loss = 0.00598881\n",
            "Iteration 934, loss = 0.00596817\n",
            "Iteration 935, loss = 0.00597846\n",
            "Iteration 936, loss = 0.00596668\n",
            "Iteration 937, loss = 0.00592121\n",
            "Iteration 938, loss = 0.00594493\n",
            "Iteration 939, loss = 0.00587589\n",
            "Iteration 940, loss = 0.00590803\n",
            "Iteration 941, loss = 0.00586528\n",
            "Iteration 942, loss = 0.00587762\n",
            "Iteration 943, loss = 0.00586406\n",
            "Iteration 944, loss = 0.00587071\n",
            "Iteration 945, loss = 0.00580995\n",
            "Iteration 946, loss = 0.00580180\n",
            "Iteration 947, loss = 0.00584840\n",
            "Iteration 948, loss = 0.00581926\n",
            "Iteration 949, loss = 0.00581182\n",
            "Iteration 950, loss = 0.00588011\n",
            "Iteration 951, loss = 0.00580395\n",
            "Iteration 952, loss = 0.00578146\n",
            "Iteration 953, loss = 0.00576543\n",
            "Iteration 954, loss = 0.00574766\n",
            "Iteration 955, loss = 0.00574124\n",
            "Iteration 956, loss = 0.00574388\n",
            "Iteration 957, loss = 0.00576781\n",
            "Iteration 958, loss = 0.00578630\n",
            "Iteration 959, loss = 0.00577538\n",
            "Iteration 960, loss = 0.00576982\n",
            "Iteration 961, loss = 0.00568827\n",
            "Iteration 962, loss = 0.00570327\n",
            "Iteration 963, loss = 0.00569549\n",
            "Iteration 964, loss = 0.00568410\n",
            "Iteration 965, loss = 0.00570965\n",
            "Iteration 966, loss = 0.00564370\n",
            "Iteration 967, loss = 0.00568064\n",
            "Iteration 968, loss = 0.00560054\n",
            "Iteration 969, loss = 0.00583633\n",
            "Iteration 970, loss = 0.00566359\n",
            "Iteration 971, loss = 0.00569315\n",
            "Iteration 972, loss = 0.00564934\n",
            "Iteration 973, loss = 0.00564049\n",
            "Iteration 974, loss = 0.00561941\n",
            "Iteration 975, loss = 0.00562886\n",
            "Iteration 976, loss = 0.00561533\n",
            "Iteration 977, loss = 0.00555391\n",
            "Iteration 978, loss = 0.00560119\n",
            "Iteration 979, loss = 0.00553238\n",
            "Iteration 980, loss = 0.00554294\n",
            "Iteration 981, loss = 0.00566130\n",
            "Iteration 982, loss = 0.00571389\n",
            "Iteration 983, loss = 0.00558938\n",
            "Iteration 984, loss = 0.00556962\n",
            "Iteration 985, loss = 0.00560059\n",
            "Iteration 986, loss = 0.00559109\n",
            "Iteration 987, loss = 0.00544320\n",
            "Iteration 988, loss = 0.00545826\n",
            "Iteration 989, loss = 0.00550528\n",
            "Iteration 990, loss = 0.00544723\n",
            "Iteration 991, loss = 0.00546468\n",
            "Iteration 992, loss = 0.00540550\n",
            "Iteration 993, loss = 0.00544445\n",
            "Iteration 994, loss = 0.00544006\n",
            "Iteration 995, loss = 0.00542116\n",
            "Iteration 996, loss = 0.00546324\n",
            "Iteration 997, loss = 0.00543855\n",
            "Iteration 998, loss = 0.00538097\n",
            "Iteration 999, loss = 0.00546481\n",
            "Iteration 1000, loss = 0.00540442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=1000, tol=1e-05, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a rede neural testando por 1500 epocas e a tolerancia em 0.00000100\n",
        "\n",
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.00000100)\n",
        "rede_neural_credit.fit(x_credit_treino, y_credit_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt-hvAV7UWE9",
        "outputId": "00d27675-d0d0-46c5-8ae5-0a8c6bf06f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.57859318\n",
            "Iteration 2, loss = 0.52809883\n",
            "Iteration 3, loss = 0.48387380\n",
            "Iteration 4, loss = 0.44573027\n",
            "Iteration 5, loss = 0.41246952\n",
            "Iteration 6, loss = 0.38372406\n",
            "Iteration 7, loss = 0.35831773\n",
            "Iteration 8, loss = 0.33545784\n",
            "Iteration 9, loss = 0.31554420\n",
            "Iteration 10, loss = 0.29687626\n",
            "Iteration 11, loss = 0.28033247\n",
            "Iteration 12, loss = 0.26510633\n",
            "Iteration 13, loss = 0.25081414\n",
            "Iteration 14, loss = 0.23787085\n",
            "Iteration 15, loss = 0.22598982\n",
            "Iteration 16, loss = 0.21489327\n",
            "Iteration 17, loss = 0.20498102\n",
            "Iteration 18, loss = 0.19570299\n",
            "Iteration 19, loss = 0.18734625\n",
            "Iteration 20, loss = 0.17973045\n",
            "Iteration 21, loss = 0.17276729\n",
            "Iteration 22, loss = 0.16627297\n",
            "Iteration 23, loss = 0.16037235\n",
            "Iteration 24, loss = 0.15498067\n",
            "Iteration 25, loss = 0.14989538\n",
            "Iteration 26, loss = 0.14537243\n",
            "Iteration 27, loss = 0.14114851\n",
            "Iteration 28, loss = 0.13704946\n",
            "Iteration 29, loss = 0.13356270\n",
            "Iteration 30, loss = 0.13008514\n",
            "Iteration 31, loss = 0.12677780\n",
            "Iteration 32, loss = 0.12379477\n",
            "Iteration 33, loss = 0.12106072\n",
            "Iteration 34, loss = 0.11833466\n",
            "Iteration 35, loss = 0.11573820\n",
            "Iteration 36, loss = 0.11342577\n",
            "Iteration 37, loss = 0.11116183\n",
            "Iteration 38, loss = 0.10928482\n",
            "Iteration 39, loss = 0.10721812\n",
            "Iteration 40, loss = 0.10531983\n",
            "Iteration 41, loss = 0.10355409\n",
            "Iteration 42, loss = 0.10177954\n",
            "Iteration 43, loss = 0.10012979\n",
            "Iteration 44, loss = 0.09855931\n",
            "Iteration 45, loss = 0.09709408\n",
            "Iteration 46, loss = 0.09569603\n",
            "Iteration 47, loss = 0.09438700\n",
            "Iteration 48, loss = 0.09301802\n",
            "Iteration 49, loss = 0.09184789\n",
            "Iteration 50, loss = 0.09053279\n",
            "Iteration 51, loss = 0.08934084\n",
            "Iteration 52, loss = 0.08822522\n",
            "Iteration 53, loss = 0.08711106\n",
            "Iteration 54, loss = 0.08611934\n",
            "Iteration 55, loss = 0.08502624\n",
            "Iteration 56, loss = 0.08405259\n",
            "Iteration 57, loss = 0.08303689\n",
            "Iteration 58, loss = 0.08211632\n",
            "Iteration 59, loss = 0.08120502\n",
            "Iteration 60, loss = 0.08030982\n",
            "Iteration 61, loss = 0.07933884\n",
            "Iteration 62, loss = 0.07846694\n",
            "Iteration 63, loss = 0.07766418\n",
            "Iteration 64, loss = 0.07685265\n",
            "Iteration 65, loss = 0.07605133\n",
            "Iteration 66, loss = 0.07525201\n",
            "Iteration 67, loss = 0.07447956\n",
            "Iteration 68, loss = 0.07368230\n",
            "Iteration 69, loss = 0.07295523\n",
            "Iteration 70, loss = 0.07224089\n",
            "Iteration 71, loss = 0.07156498\n",
            "Iteration 72, loss = 0.07089825\n",
            "Iteration 73, loss = 0.07013384\n",
            "Iteration 74, loss = 0.06942808\n",
            "Iteration 75, loss = 0.06878898\n",
            "Iteration 76, loss = 0.06811857\n",
            "Iteration 77, loss = 0.06747631\n",
            "Iteration 78, loss = 0.06699813\n",
            "Iteration 79, loss = 0.06624505\n",
            "Iteration 80, loss = 0.06575183\n",
            "Iteration 81, loss = 0.06507227\n",
            "Iteration 82, loss = 0.06453110\n",
            "Iteration 83, loss = 0.06389624\n",
            "Iteration 84, loss = 0.06334870\n",
            "Iteration 85, loss = 0.06280021\n",
            "Iteration 86, loss = 0.06223715\n",
            "Iteration 87, loss = 0.06171962\n",
            "Iteration 88, loss = 0.06116657\n",
            "Iteration 89, loss = 0.06062030\n",
            "Iteration 90, loss = 0.06014638\n",
            "Iteration 91, loss = 0.05962164\n",
            "Iteration 92, loss = 0.05908545\n",
            "Iteration 93, loss = 0.05874838\n",
            "Iteration 94, loss = 0.05814826\n",
            "Iteration 95, loss = 0.05780843\n",
            "Iteration 96, loss = 0.05719481\n",
            "Iteration 97, loss = 0.05678003\n",
            "Iteration 98, loss = 0.05632244\n",
            "Iteration 99, loss = 0.05576524\n",
            "Iteration 100, loss = 0.05531614\n",
            "Iteration 101, loss = 0.05491750\n",
            "Iteration 102, loss = 0.05454111\n",
            "Iteration 103, loss = 0.05413265\n",
            "Iteration 104, loss = 0.05373968\n",
            "Iteration 105, loss = 0.05322554\n",
            "Iteration 106, loss = 0.05286119\n",
            "Iteration 107, loss = 0.05236006\n",
            "Iteration 108, loss = 0.05203370\n",
            "Iteration 109, loss = 0.05163103\n",
            "Iteration 110, loss = 0.05123861\n",
            "Iteration 111, loss = 0.05083333\n",
            "Iteration 112, loss = 0.05041646\n",
            "Iteration 113, loss = 0.05003894\n",
            "Iteration 114, loss = 0.04966605\n",
            "Iteration 115, loss = 0.04932179\n",
            "Iteration 116, loss = 0.04900761\n",
            "Iteration 117, loss = 0.04865202\n",
            "Iteration 118, loss = 0.04827617\n",
            "Iteration 119, loss = 0.04794325\n",
            "Iteration 120, loss = 0.04760487\n",
            "Iteration 121, loss = 0.04719546\n",
            "Iteration 122, loss = 0.04693080\n",
            "Iteration 123, loss = 0.04649974\n",
            "Iteration 124, loss = 0.04618335\n",
            "Iteration 125, loss = 0.04591859\n",
            "Iteration 126, loss = 0.04556562\n",
            "Iteration 127, loss = 0.04523796\n",
            "Iteration 128, loss = 0.04492603\n",
            "Iteration 129, loss = 0.04466676\n",
            "Iteration 130, loss = 0.04432837\n",
            "Iteration 131, loss = 0.04398983\n",
            "Iteration 132, loss = 0.04369918\n",
            "Iteration 133, loss = 0.04342241\n",
            "Iteration 134, loss = 0.04312298\n",
            "Iteration 135, loss = 0.04283133\n",
            "Iteration 136, loss = 0.04260671\n",
            "Iteration 137, loss = 0.04224759\n",
            "Iteration 138, loss = 0.04194907\n",
            "Iteration 139, loss = 0.04176447\n",
            "Iteration 140, loss = 0.04142786\n",
            "Iteration 141, loss = 0.04117975\n",
            "Iteration 142, loss = 0.04086237\n",
            "Iteration 143, loss = 0.04063308\n",
            "Iteration 144, loss = 0.04044406\n",
            "Iteration 145, loss = 0.04019100\n",
            "Iteration 146, loss = 0.03982225\n",
            "Iteration 147, loss = 0.03957518\n",
            "Iteration 148, loss = 0.03935377\n",
            "Iteration 149, loss = 0.03910195\n",
            "Iteration 150, loss = 0.03886084\n",
            "Iteration 151, loss = 0.03869050\n",
            "Iteration 152, loss = 0.03844803\n",
            "Iteration 153, loss = 0.03816478\n",
            "Iteration 154, loss = 0.03796903\n",
            "Iteration 155, loss = 0.03772710\n",
            "Iteration 156, loss = 0.03747758\n",
            "Iteration 157, loss = 0.03733157\n",
            "Iteration 158, loss = 0.03705896\n",
            "Iteration 159, loss = 0.03683221\n",
            "Iteration 160, loss = 0.03668424\n",
            "Iteration 161, loss = 0.03649688\n",
            "Iteration 162, loss = 0.03620078\n",
            "Iteration 163, loss = 0.03601045\n",
            "Iteration 164, loss = 0.03577034\n",
            "Iteration 165, loss = 0.03564593\n",
            "Iteration 166, loss = 0.03537905\n",
            "Iteration 167, loss = 0.03526306\n",
            "Iteration 168, loss = 0.03499222\n",
            "Iteration 169, loss = 0.03481785\n",
            "Iteration 170, loss = 0.03458756\n",
            "Iteration 171, loss = 0.03447715\n",
            "Iteration 172, loss = 0.03424905\n",
            "Iteration 173, loss = 0.03406045\n",
            "Iteration 174, loss = 0.03390454\n",
            "Iteration 175, loss = 0.03372540\n",
            "Iteration 176, loss = 0.03351684\n",
            "Iteration 177, loss = 0.03332857\n",
            "Iteration 178, loss = 0.03319279\n",
            "Iteration 179, loss = 0.03301122\n",
            "Iteration 180, loss = 0.03286151\n",
            "Iteration 181, loss = 0.03263492\n",
            "Iteration 182, loss = 0.03245230\n",
            "Iteration 183, loss = 0.03231601\n",
            "Iteration 184, loss = 0.03214083\n",
            "Iteration 185, loss = 0.03211245\n",
            "Iteration 186, loss = 0.03182516\n",
            "Iteration 187, loss = 0.03163413\n",
            "Iteration 188, loss = 0.03145930\n",
            "Iteration 189, loss = 0.03132013\n",
            "Iteration 190, loss = 0.03116532\n",
            "Iteration 191, loss = 0.03104128\n",
            "Iteration 192, loss = 0.03081926\n",
            "Iteration 193, loss = 0.03069416\n",
            "Iteration 194, loss = 0.03052117\n",
            "Iteration 195, loss = 0.03041582\n",
            "Iteration 196, loss = 0.03023518\n",
            "Iteration 197, loss = 0.03008381\n",
            "Iteration 198, loss = 0.02997625\n",
            "Iteration 199, loss = 0.02984746\n",
            "Iteration 200, loss = 0.02963315\n",
            "Iteration 201, loss = 0.02950358\n",
            "Iteration 202, loss = 0.02935929\n",
            "Iteration 203, loss = 0.02921317\n",
            "Iteration 204, loss = 0.02909477\n",
            "Iteration 205, loss = 0.02897820\n",
            "Iteration 206, loss = 0.02877146\n",
            "Iteration 207, loss = 0.02868579\n",
            "Iteration 208, loss = 0.02862968\n",
            "Iteration 209, loss = 0.02845980\n",
            "Iteration 210, loss = 0.02829619\n",
            "Iteration 211, loss = 0.02816754\n",
            "Iteration 212, loss = 0.02806488\n",
            "Iteration 213, loss = 0.02790321\n",
            "Iteration 214, loss = 0.02781235\n",
            "Iteration 215, loss = 0.02762859\n",
            "Iteration 216, loss = 0.02752508\n",
            "Iteration 217, loss = 0.02738067\n",
            "Iteration 218, loss = 0.02729181\n",
            "Iteration 219, loss = 0.02717884\n",
            "Iteration 220, loss = 0.02701016\n",
            "Iteration 221, loss = 0.02691640\n",
            "Iteration 222, loss = 0.02678944\n",
            "Iteration 223, loss = 0.02663985\n",
            "Iteration 224, loss = 0.02666050\n",
            "Iteration 225, loss = 0.02647056\n",
            "Iteration 226, loss = 0.02632374\n",
            "Iteration 227, loss = 0.02625987\n",
            "Iteration 228, loss = 0.02633327\n",
            "Iteration 229, loss = 0.02603096\n",
            "Iteration 230, loss = 0.02586634\n",
            "Iteration 231, loss = 0.02584254\n",
            "Iteration 232, loss = 0.02569470\n",
            "Iteration 233, loss = 0.02564468\n",
            "Iteration 234, loss = 0.02554442\n",
            "Iteration 235, loss = 0.02534001\n",
            "Iteration 236, loss = 0.02526482\n",
            "Iteration 237, loss = 0.02513104\n",
            "Iteration 238, loss = 0.02500358\n",
            "Iteration 239, loss = 0.02504034\n",
            "Iteration 240, loss = 0.02493313\n",
            "Iteration 241, loss = 0.02470034\n",
            "Iteration 242, loss = 0.02464565\n",
            "Iteration 243, loss = 0.02457214\n",
            "Iteration 244, loss = 0.02443079\n",
            "Iteration 245, loss = 0.02438425\n",
            "Iteration 246, loss = 0.02428218\n",
            "Iteration 247, loss = 0.02417280\n",
            "Iteration 248, loss = 0.02411244\n",
            "Iteration 249, loss = 0.02401557\n",
            "Iteration 250, loss = 0.02391820\n",
            "Iteration 251, loss = 0.02379329\n",
            "Iteration 252, loss = 0.02374068\n",
            "Iteration 253, loss = 0.02361542\n",
            "Iteration 254, loss = 0.02355487\n",
            "Iteration 255, loss = 0.02342840\n",
            "Iteration 256, loss = 0.02348950\n",
            "Iteration 257, loss = 0.02326792\n",
            "Iteration 258, loss = 0.02319587\n",
            "Iteration 259, loss = 0.02311239\n",
            "Iteration 260, loss = 0.02303636\n",
            "Iteration 261, loss = 0.02291066\n",
            "Iteration 262, loss = 0.02284673\n",
            "Iteration 263, loss = 0.02276770\n",
            "Iteration 264, loss = 0.02265060\n",
            "Iteration 265, loss = 0.02259978\n",
            "Iteration 266, loss = 0.02255954\n",
            "Iteration 267, loss = 0.02240767\n",
            "Iteration 268, loss = 0.02230136\n",
            "Iteration 269, loss = 0.02233383\n",
            "Iteration 270, loss = 0.02219009\n",
            "Iteration 271, loss = 0.02218087\n",
            "Iteration 272, loss = 0.02204323\n",
            "Iteration 273, loss = 0.02193847\n",
            "Iteration 274, loss = 0.02187443\n",
            "Iteration 275, loss = 0.02183272\n",
            "Iteration 276, loss = 0.02181043\n",
            "Iteration 277, loss = 0.02165534\n",
            "Iteration 278, loss = 0.02156174\n",
            "Iteration 279, loss = 0.02145167\n",
            "Iteration 280, loss = 0.02138897\n",
            "Iteration 281, loss = 0.02134035\n",
            "Iteration 282, loss = 0.02127060\n",
            "Iteration 283, loss = 0.02121201\n",
            "Iteration 284, loss = 0.02108220\n",
            "Iteration 285, loss = 0.02101178\n",
            "Iteration 286, loss = 0.02099178\n",
            "Iteration 287, loss = 0.02091570\n",
            "Iteration 288, loss = 0.02083938\n",
            "Iteration 289, loss = 0.02074443\n",
            "Iteration 290, loss = 0.02066764\n",
            "Iteration 291, loss = 0.02064222\n",
            "Iteration 292, loss = 0.02053897\n",
            "Iteration 293, loss = 0.02046863\n",
            "Iteration 294, loss = 0.02042397\n",
            "Iteration 295, loss = 0.02034897\n",
            "Iteration 296, loss = 0.02026424\n",
            "Iteration 297, loss = 0.02017798\n",
            "Iteration 298, loss = 0.02018976\n",
            "Iteration 299, loss = 0.02002182\n",
            "Iteration 300, loss = 0.02022299\n",
            "Iteration 301, loss = 0.01998105\n",
            "Iteration 302, loss = 0.01991430\n",
            "Iteration 303, loss = 0.01981665\n",
            "Iteration 304, loss = 0.01972552\n",
            "Iteration 305, loss = 0.01974761\n",
            "Iteration 306, loss = 0.01971904\n",
            "Iteration 307, loss = 0.01952184\n",
            "Iteration 308, loss = 0.01949140\n",
            "Iteration 309, loss = 0.01942671\n",
            "Iteration 310, loss = 0.01938382\n",
            "Iteration 311, loss = 0.01931397\n",
            "Iteration 312, loss = 0.01926681\n",
            "Iteration 313, loss = 0.01914652\n",
            "Iteration 314, loss = 0.01912511\n",
            "Iteration 315, loss = 0.01908103\n",
            "Iteration 316, loss = 0.01903780\n",
            "Iteration 317, loss = 0.01893838\n",
            "Iteration 318, loss = 0.01885925\n",
            "Iteration 319, loss = 0.01894082\n",
            "Iteration 320, loss = 0.01877611\n",
            "Iteration 321, loss = 0.01877362\n",
            "Iteration 322, loss = 0.01872261\n",
            "Iteration 323, loss = 0.01856246\n",
            "Iteration 324, loss = 0.01849200\n",
            "Iteration 325, loss = 0.01847030\n",
            "Iteration 326, loss = 0.01845407\n",
            "Iteration 327, loss = 0.01834021\n",
            "Iteration 328, loss = 0.01833747\n",
            "Iteration 329, loss = 0.01823846\n",
            "Iteration 330, loss = 0.01819080\n",
            "Iteration 331, loss = 0.01817953\n",
            "Iteration 332, loss = 0.01807377\n",
            "Iteration 333, loss = 0.01798761\n",
            "Iteration 334, loss = 0.01795760\n",
            "Iteration 335, loss = 0.01795896\n",
            "Iteration 336, loss = 0.01791579\n",
            "Iteration 337, loss = 0.01788402\n",
            "Iteration 338, loss = 0.01774292\n",
            "Iteration 339, loss = 0.01778118\n",
            "Iteration 340, loss = 0.01767170\n",
            "Iteration 341, loss = 0.01767681\n",
            "Iteration 342, loss = 0.01756591\n",
            "Iteration 343, loss = 0.01748355\n",
            "Iteration 344, loss = 0.01748557\n",
            "Iteration 345, loss = 0.01741035\n",
            "Iteration 346, loss = 0.01741396\n",
            "Iteration 347, loss = 0.01730879\n",
            "Iteration 348, loss = 0.01722831\n",
            "Iteration 349, loss = 0.01726089\n",
            "Iteration 350, loss = 0.01727169\n",
            "Iteration 351, loss = 0.01706941\n",
            "Iteration 352, loss = 0.01708914\n",
            "Iteration 353, loss = 0.01699387\n",
            "Iteration 354, loss = 0.01700522\n",
            "Iteration 355, loss = 0.01683749\n",
            "Iteration 356, loss = 0.01682695\n",
            "Iteration 357, loss = 0.01679863\n",
            "Iteration 358, loss = 0.01673282\n",
            "Iteration 359, loss = 0.01666493\n",
            "Iteration 360, loss = 0.01664062\n",
            "Iteration 361, loss = 0.01660999\n",
            "Iteration 362, loss = 0.01654686\n",
            "Iteration 363, loss = 0.01649433\n",
            "Iteration 364, loss = 0.01644632\n",
            "Iteration 365, loss = 0.01642655\n",
            "Iteration 366, loss = 0.01635887\n",
            "Iteration 367, loss = 0.01639369\n",
            "Iteration 368, loss = 0.01626707\n",
            "Iteration 369, loss = 0.01622028\n",
            "Iteration 370, loss = 0.01619101\n",
            "Iteration 371, loss = 0.01615565\n",
            "Iteration 372, loss = 0.01610456\n",
            "Iteration 373, loss = 0.01610557\n",
            "Iteration 374, loss = 0.01603686\n",
            "Iteration 375, loss = 0.01594561\n",
            "Iteration 376, loss = 0.01586728\n",
            "Iteration 377, loss = 0.01590187\n",
            "Iteration 378, loss = 0.01586773\n",
            "Iteration 379, loss = 0.01580161\n",
            "Iteration 380, loss = 0.01572000\n",
            "Iteration 381, loss = 0.01572044\n",
            "Iteration 382, loss = 0.01570014\n",
            "Iteration 383, loss = 0.01576143\n",
            "Iteration 384, loss = 0.01552293\n",
            "Iteration 385, loss = 0.01557645\n",
            "Iteration 386, loss = 0.01553185\n",
            "Iteration 387, loss = 0.01540922\n",
            "Iteration 388, loss = 0.01541073\n",
            "Iteration 389, loss = 0.01540742\n",
            "Iteration 390, loss = 0.01530113\n",
            "Iteration 391, loss = 0.01531756\n",
            "Iteration 392, loss = 0.01528592\n",
            "Iteration 393, loss = 0.01520171\n",
            "Iteration 394, loss = 0.01511132\n",
            "Iteration 395, loss = 0.01515802\n",
            "Iteration 396, loss = 0.01510582\n",
            "Iteration 397, loss = 0.01501584\n",
            "Iteration 398, loss = 0.01497295\n",
            "Iteration 399, loss = 0.01502365\n",
            "Iteration 400, loss = 0.01490495\n",
            "Iteration 401, loss = 0.01490330\n",
            "Iteration 402, loss = 0.01483014\n",
            "Iteration 403, loss = 0.01475692\n",
            "Iteration 404, loss = 0.01487971\n",
            "Iteration 405, loss = 0.01476223\n",
            "Iteration 406, loss = 0.01474973\n",
            "Iteration 407, loss = 0.01464091\n",
            "Iteration 408, loss = 0.01460036\n",
            "Iteration 409, loss = 0.01454939\n",
            "Iteration 410, loss = 0.01453544\n",
            "Iteration 411, loss = 0.01444174\n",
            "Iteration 412, loss = 0.01452201\n",
            "Iteration 413, loss = 0.01438770\n",
            "Iteration 414, loss = 0.01434961\n",
            "Iteration 415, loss = 0.01438777\n",
            "Iteration 416, loss = 0.01427177\n",
            "Iteration 417, loss = 0.01431950\n",
            "Iteration 418, loss = 0.01423512\n",
            "Iteration 419, loss = 0.01421768\n",
            "Iteration 420, loss = 0.01415446\n",
            "Iteration 421, loss = 0.01424983\n",
            "Iteration 422, loss = 0.01420265\n",
            "Iteration 423, loss = 0.01402415\n",
            "Iteration 424, loss = 0.01399781\n",
            "Iteration 425, loss = 0.01396292\n",
            "Iteration 426, loss = 0.01400932\n",
            "Iteration 427, loss = 0.01393965\n",
            "Iteration 428, loss = 0.01385783\n",
            "Iteration 429, loss = 0.01383933\n",
            "Iteration 430, loss = 0.01384090\n",
            "Iteration 431, loss = 0.01382279\n",
            "Iteration 432, loss = 0.01374615\n",
            "Iteration 433, loss = 0.01368504\n",
            "Iteration 434, loss = 0.01365974\n",
            "Iteration 435, loss = 0.01361727\n",
            "Iteration 436, loss = 0.01361460\n",
            "Iteration 437, loss = 0.01358978\n",
            "Iteration 438, loss = 0.01352524\n",
            "Iteration 439, loss = 0.01355770\n",
            "Iteration 440, loss = 0.01354130\n",
            "Iteration 441, loss = 0.01339717\n",
            "Iteration 442, loss = 0.01344593\n",
            "Iteration 443, loss = 0.01329183\n",
            "Iteration 444, loss = 0.01332170\n",
            "Iteration 445, loss = 0.01327601\n",
            "Iteration 446, loss = 0.01327064\n",
            "Iteration 447, loss = 0.01325702\n",
            "Iteration 448, loss = 0.01316212\n",
            "Iteration 449, loss = 0.01313356\n",
            "Iteration 450, loss = 0.01312579\n",
            "Iteration 451, loss = 0.01305209\n",
            "Iteration 452, loss = 0.01305202\n",
            "Iteration 453, loss = 0.01300627\n",
            "Iteration 454, loss = 0.01301117\n",
            "Iteration 455, loss = 0.01294067\n",
            "Iteration 456, loss = 0.01296632\n",
            "Iteration 457, loss = 0.01286481\n",
            "Iteration 458, loss = 0.01285393\n",
            "Iteration 459, loss = 0.01286442\n",
            "Iteration 460, loss = 0.01284496\n",
            "Iteration 461, loss = 0.01284280\n",
            "Iteration 462, loss = 0.01269984\n",
            "Iteration 463, loss = 0.01269327\n",
            "Iteration 464, loss = 0.01264549\n",
            "Iteration 465, loss = 0.01269112\n",
            "Iteration 466, loss = 0.01270854\n",
            "Iteration 467, loss = 0.01254498\n",
            "Iteration 468, loss = 0.01260977\n",
            "Iteration 469, loss = 0.01250912\n",
            "Iteration 470, loss = 0.01242563\n",
            "Iteration 471, loss = 0.01245319\n",
            "Iteration 472, loss = 0.01243383\n",
            "Iteration 473, loss = 0.01249639\n",
            "Iteration 474, loss = 0.01240169\n",
            "Iteration 475, loss = 0.01238000\n",
            "Iteration 476, loss = 0.01228585\n",
            "Iteration 477, loss = 0.01228567\n",
            "Iteration 478, loss = 0.01231429\n",
            "Iteration 479, loss = 0.01222746\n",
            "Iteration 480, loss = 0.01218974\n",
            "Iteration 481, loss = 0.01224477\n",
            "Iteration 482, loss = 0.01211503\n",
            "Iteration 483, loss = 0.01210762\n",
            "Iteration 484, loss = 0.01210211\n",
            "Iteration 485, loss = 0.01209525\n",
            "Iteration 486, loss = 0.01216107\n",
            "Iteration 487, loss = 0.01201152\n",
            "Iteration 488, loss = 0.01199602\n",
            "Iteration 489, loss = 0.01189963\n",
            "Iteration 490, loss = 0.01197511\n",
            "Iteration 491, loss = 0.01196646\n",
            "Iteration 492, loss = 0.01209574\n",
            "Iteration 493, loss = 0.01190324\n",
            "Iteration 494, loss = 0.01177805\n",
            "Iteration 495, loss = 0.01183511\n",
            "Iteration 496, loss = 0.01178471\n",
            "Iteration 497, loss = 0.01175151\n",
            "Iteration 498, loss = 0.01181609\n",
            "Iteration 499, loss = 0.01169577\n",
            "Iteration 500, loss = 0.01171533\n",
            "Iteration 501, loss = 0.01164395\n",
            "Iteration 502, loss = 0.01170225\n",
            "Iteration 503, loss = 0.01156448\n",
            "Iteration 504, loss = 0.01151919\n",
            "Iteration 505, loss = 0.01158757\n",
            "Iteration 506, loss = 0.01147950\n",
            "Iteration 507, loss = 0.01143550\n",
            "Iteration 508, loss = 0.01142071\n",
            "Iteration 509, loss = 0.01141683\n",
            "Iteration 510, loss = 0.01141253\n",
            "Iteration 511, loss = 0.01134209\n",
            "Iteration 512, loss = 0.01131063\n",
            "Iteration 513, loss = 0.01131666\n",
            "Iteration 514, loss = 0.01127772\n",
            "Iteration 515, loss = 0.01125593\n",
            "Iteration 516, loss = 0.01131969\n",
            "Iteration 517, loss = 0.01118485\n",
            "Iteration 518, loss = 0.01121577\n",
            "Iteration 519, loss = 0.01119766\n",
            "Iteration 520, loss = 0.01127471\n",
            "Iteration 521, loss = 0.01115126\n",
            "Iteration 522, loss = 0.01108503\n",
            "Iteration 523, loss = 0.01103413\n",
            "Iteration 524, loss = 0.01110761\n",
            "Iteration 525, loss = 0.01103483\n",
            "Iteration 526, loss = 0.01096924\n",
            "Iteration 527, loss = 0.01095088\n",
            "Iteration 528, loss = 0.01096494\n",
            "Iteration 529, loss = 0.01089656\n",
            "Iteration 530, loss = 0.01092224\n",
            "Iteration 531, loss = 0.01093041\n",
            "Iteration 532, loss = 0.01101160\n",
            "Iteration 533, loss = 0.01078694\n",
            "Iteration 534, loss = 0.01089131\n",
            "Iteration 535, loss = 0.01080113\n",
            "Iteration 536, loss = 0.01086370\n",
            "Iteration 537, loss = 0.01071864\n",
            "Iteration 538, loss = 0.01068231\n",
            "Iteration 539, loss = 0.01078817\n",
            "Iteration 540, loss = 0.01072589\n",
            "Iteration 541, loss = 0.01061952\n",
            "Iteration 542, loss = 0.01067483\n",
            "Iteration 543, loss = 0.01061611\n",
            "Iteration 544, loss = 0.01060085\n",
            "Iteration 545, loss = 0.01055146\n",
            "Iteration 546, loss = 0.01061992\n",
            "Iteration 547, loss = 0.01053855\n",
            "Iteration 548, loss = 0.01051581\n",
            "Iteration 549, loss = 0.01046182\n",
            "Iteration 550, loss = 0.01051110\n",
            "Iteration 551, loss = 0.01045349\n",
            "Iteration 552, loss = 0.01036391\n",
            "Iteration 553, loss = 0.01038000\n",
            "Iteration 554, loss = 0.01034189\n",
            "Iteration 555, loss = 0.01033281\n",
            "Iteration 556, loss = 0.01030284\n",
            "Iteration 557, loss = 0.01035961\n",
            "Iteration 558, loss = 0.01044209\n",
            "Iteration 559, loss = 0.01026674\n",
            "Iteration 560, loss = 0.01027571\n",
            "Iteration 561, loss = 0.01018005\n",
            "Iteration 562, loss = 0.01017056\n",
            "Iteration 563, loss = 0.01011770\n",
            "Iteration 564, loss = 0.01015115\n",
            "Iteration 565, loss = 0.01026497\n",
            "Iteration 566, loss = 0.01023838\n",
            "Iteration 567, loss = 0.01013041\n",
            "Iteration 568, loss = 0.01005987\n",
            "Iteration 569, loss = 0.01002898\n",
            "Iteration 570, loss = 0.00997639\n",
            "Iteration 571, loss = 0.01003523\n",
            "Iteration 572, loss = 0.00996822\n",
            "Iteration 573, loss = 0.00996702\n",
            "Iteration 574, loss = 0.01011077\n",
            "Iteration 575, loss = 0.00994357\n",
            "Iteration 576, loss = 0.00988978\n",
            "Iteration 577, loss = 0.00985961\n",
            "Iteration 578, loss = 0.00996725\n",
            "Iteration 579, loss = 0.00983759\n",
            "Iteration 580, loss = 0.00985178\n",
            "Iteration 581, loss = 0.00979972\n",
            "Iteration 582, loss = 0.00978566\n",
            "Iteration 583, loss = 0.00979793\n",
            "Iteration 584, loss = 0.00989984\n",
            "Iteration 585, loss = 0.00973099\n",
            "Iteration 586, loss = 0.00967028\n",
            "Iteration 587, loss = 0.00964200\n",
            "Iteration 588, loss = 0.00970730\n",
            "Iteration 589, loss = 0.00962997\n",
            "Iteration 590, loss = 0.00960801\n",
            "Iteration 591, loss = 0.00963526\n",
            "Iteration 592, loss = 0.00962403\n",
            "Iteration 593, loss = 0.00960237\n",
            "Iteration 594, loss = 0.00952091\n",
            "Iteration 595, loss = 0.00952997\n",
            "Iteration 596, loss = 0.00947602\n",
            "Iteration 597, loss = 0.00954539\n",
            "Iteration 598, loss = 0.00944992\n",
            "Iteration 599, loss = 0.00955783\n",
            "Iteration 600, loss = 0.00938210\n",
            "Iteration 601, loss = 0.00937662\n",
            "Iteration 602, loss = 0.00936612\n",
            "Iteration 603, loss = 0.00932695\n",
            "Iteration 604, loss = 0.00929488\n",
            "Iteration 605, loss = 0.00936748\n",
            "Iteration 606, loss = 0.00929270\n",
            "Iteration 607, loss = 0.00927387\n",
            "Iteration 608, loss = 0.00921646\n",
            "Iteration 609, loss = 0.00924678\n",
            "Iteration 610, loss = 0.00923494\n",
            "Iteration 611, loss = 0.00923481\n",
            "Iteration 612, loss = 0.00911611\n",
            "Iteration 613, loss = 0.00923813\n",
            "Iteration 614, loss = 0.00912910\n",
            "Iteration 615, loss = 0.00921289\n",
            "Iteration 616, loss = 0.00912469\n",
            "Iteration 617, loss = 0.00923487\n",
            "Iteration 618, loss = 0.00914562\n",
            "Iteration 619, loss = 0.00902902\n",
            "Iteration 620, loss = 0.00903921\n",
            "Iteration 621, loss = 0.00905407\n",
            "Iteration 622, loss = 0.00904211\n",
            "Iteration 623, loss = 0.00905210\n",
            "Iteration 624, loss = 0.00897212\n",
            "Iteration 625, loss = 0.00898417\n",
            "Iteration 626, loss = 0.00892511\n",
            "Iteration 627, loss = 0.00896998\n",
            "Iteration 628, loss = 0.00893606\n",
            "Iteration 629, loss = 0.00889785\n",
            "Iteration 630, loss = 0.00889334\n",
            "Iteration 631, loss = 0.00895289\n",
            "Iteration 632, loss = 0.00898797\n",
            "Iteration 633, loss = 0.00887495\n",
            "Iteration 634, loss = 0.00889467\n",
            "Iteration 635, loss = 0.00877340\n",
            "Iteration 636, loss = 0.00873529\n",
            "Iteration 637, loss = 0.00877919\n",
            "Iteration 638, loss = 0.00874738\n",
            "Iteration 639, loss = 0.00874653\n",
            "Iteration 640, loss = 0.00868214\n",
            "Iteration 641, loss = 0.00874384\n",
            "Iteration 642, loss = 0.00867991\n",
            "Iteration 643, loss = 0.00864790\n",
            "Iteration 644, loss = 0.00860933\n",
            "Iteration 645, loss = 0.00859896\n",
            "Iteration 646, loss = 0.00862839\n",
            "Iteration 647, loss = 0.00866199\n",
            "Iteration 648, loss = 0.00858571\n",
            "Iteration 649, loss = 0.00853979\n",
            "Iteration 650, loss = 0.00853003\n",
            "Iteration 651, loss = 0.00852036\n",
            "Iteration 652, loss = 0.00843727\n",
            "Iteration 653, loss = 0.00852336\n",
            "Iteration 654, loss = 0.00848844\n",
            "Iteration 655, loss = 0.00841956\n",
            "Iteration 656, loss = 0.00845107\n",
            "Iteration 657, loss = 0.00844226\n",
            "Iteration 658, loss = 0.00842749\n",
            "Iteration 659, loss = 0.00835636\n",
            "Iteration 660, loss = 0.00837314\n",
            "Iteration 661, loss = 0.00843667\n",
            "Iteration 662, loss = 0.00832279\n",
            "Iteration 663, loss = 0.00831426\n",
            "Iteration 664, loss = 0.00829859\n",
            "Iteration 665, loss = 0.00828565\n",
            "Iteration 666, loss = 0.00827563\n",
            "Iteration 667, loss = 0.00824850\n",
            "Iteration 668, loss = 0.00825242\n",
            "Iteration 669, loss = 0.00842176\n",
            "Iteration 670, loss = 0.00821656\n",
            "Iteration 671, loss = 0.00817647\n",
            "Iteration 672, loss = 0.00820405\n",
            "Iteration 673, loss = 0.00818297\n",
            "Iteration 674, loss = 0.00815929\n",
            "Iteration 675, loss = 0.00816696\n",
            "Iteration 676, loss = 0.00808685\n",
            "Iteration 677, loss = 0.00811978\n",
            "Iteration 678, loss = 0.00809570\n",
            "Iteration 679, loss = 0.00806624\n",
            "Iteration 680, loss = 0.00808165\n",
            "Iteration 681, loss = 0.00804499\n",
            "Iteration 682, loss = 0.00804409\n",
            "Iteration 683, loss = 0.00805777\n",
            "Iteration 684, loss = 0.00801558\n",
            "Iteration 685, loss = 0.00798198\n",
            "Iteration 686, loss = 0.00813150\n",
            "Iteration 687, loss = 0.00797892\n",
            "Iteration 688, loss = 0.00792040\n",
            "Iteration 689, loss = 0.00796269\n",
            "Iteration 690, loss = 0.00787946\n",
            "Iteration 691, loss = 0.00799426\n",
            "Iteration 692, loss = 0.00789386\n",
            "Iteration 693, loss = 0.00785539\n",
            "Iteration 694, loss = 0.00781970\n",
            "Iteration 695, loss = 0.00781460\n",
            "Iteration 696, loss = 0.00781203\n",
            "Iteration 697, loss = 0.00786763\n",
            "Iteration 698, loss = 0.00776453\n",
            "Iteration 699, loss = 0.00788811\n",
            "Iteration 700, loss = 0.00777843\n",
            "Iteration 701, loss = 0.00785123\n",
            "Iteration 702, loss = 0.00784144\n",
            "Iteration 703, loss = 0.00780034\n",
            "Iteration 704, loss = 0.00782508\n",
            "Iteration 705, loss = 0.00767372\n",
            "Iteration 706, loss = 0.00769678\n",
            "Iteration 707, loss = 0.00766619\n",
            "Iteration 708, loss = 0.00759632\n",
            "Iteration 709, loss = 0.00768005\n",
            "Iteration 710, loss = 0.00769464\n",
            "Iteration 711, loss = 0.00779855\n",
            "Iteration 712, loss = 0.00757495\n",
            "Iteration 713, loss = 0.00755538\n",
            "Iteration 714, loss = 0.00765781\n",
            "Iteration 715, loss = 0.00760033\n",
            "Iteration 716, loss = 0.00753137\n",
            "Iteration 717, loss = 0.00757577\n",
            "Iteration 718, loss = 0.00748575\n",
            "Iteration 719, loss = 0.00748899\n",
            "Iteration 720, loss = 0.00746370\n",
            "Iteration 721, loss = 0.00743618\n",
            "Iteration 722, loss = 0.00757718\n",
            "Iteration 723, loss = 0.00742236\n",
            "Iteration 724, loss = 0.00745522\n",
            "Iteration 725, loss = 0.00744155\n",
            "Iteration 726, loss = 0.00750157\n",
            "Iteration 727, loss = 0.00734577\n",
            "Iteration 728, loss = 0.00743027\n",
            "Iteration 729, loss = 0.00738224\n",
            "Iteration 730, loss = 0.00738389\n",
            "Iteration 731, loss = 0.00735269\n",
            "Iteration 732, loss = 0.00730790\n",
            "Iteration 733, loss = 0.00728420\n",
            "Iteration 734, loss = 0.00732504\n",
            "Iteration 735, loss = 0.00727819\n",
            "Iteration 736, loss = 0.00731342\n",
            "Iteration 737, loss = 0.00725947\n",
            "Iteration 738, loss = 0.00723245\n",
            "Iteration 739, loss = 0.00723717\n",
            "Iteration 740, loss = 0.00718271\n",
            "Iteration 741, loss = 0.00723652\n",
            "Iteration 742, loss = 0.00713813\n",
            "Iteration 743, loss = 0.00719057\n",
            "Iteration 744, loss = 0.00722042\n",
            "Iteration 745, loss = 0.00718684\n",
            "Iteration 746, loss = 0.00723662\n",
            "Iteration 747, loss = 0.00714253\n",
            "Iteration 748, loss = 0.00712259\n",
            "Iteration 749, loss = 0.00718851\n",
            "Iteration 750, loss = 0.00709549\n",
            "Iteration 751, loss = 0.00705448\n",
            "Iteration 752, loss = 0.00713503\n",
            "Iteration 753, loss = 0.00700454\n",
            "Iteration 754, loss = 0.00711723\n",
            "Iteration 755, loss = 0.00712253\n",
            "Iteration 756, loss = 0.00703159\n",
            "Iteration 757, loss = 0.00699537\n",
            "Iteration 758, loss = 0.00701247\n",
            "Iteration 759, loss = 0.00702793\n",
            "Iteration 760, loss = 0.00696259\n",
            "Iteration 761, loss = 0.00695004\n",
            "Iteration 762, loss = 0.00692553\n",
            "Iteration 763, loss = 0.00702078\n",
            "Iteration 764, loss = 0.00700624\n",
            "Iteration 765, loss = 0.00695154\n",
            "Iteration 766, loss = 0.00690325\n",
            "Iteration 767, loss = 0.00688925\n",
            "Iteration 768, loss = 0.00687564\n",
            "Iteration 769, loss = 0.00692201\n",
            "Iteration 770, loss = 0.00684639\n",
            "Iteration 771, loss = 0.00680301\n",
            "Iteration 772, loss = 0.00682313\n",
            "Iteration 773, loss = 0.00680972\n",
            "Iteration 774, loss = 0.00681435\n",
            "Iteration 775, loss = 0.00679354\n",
            "Iteration 776, loss = 0.00682042\n",
            "Iteration 777, loss = 0.00670712\n",
            "Iteration 778, loss = 0.00679925\n",
            "Iteration 779, loss = 0.00672268\n",
            "Iteration 780, loss = 0.00673402\n",
            "Iteration 781, loss = 0.00669996\n",
            "Iteration 782, loss = 0.00676455\n",
            "Iteration 783, loss = 0.00671781\n",
            "Iteration 784, loss = 0.00668049\n",
            "Iteration 785, loss = 0.00667001\n",
            "Iteration 786, loss = 0.00663181\n",
            "Iteration 787, loss = 0.00674993\n",
            "Iteration 788, loss = 0.00674652\n",
            "Iteration 789, loss = 0.00663013\n",
            "Iteration 790, loss = 0.00662984\n",
            "Iteration 791, loss = 0.00664643\n",
            "Iteration 792, loss = 0.00670434\n",
            "Iteration 793, loss = 0.00657807\n",
            "Iteration 794, loss = 0.00670542\n",
            "Iteration 795, loss = 0.00661733\n",
            "Iteration 796, loss = 0.00657860\n",
            "Iteration 797, loss = 0.00653591\n",
            "Iteration 798, loss = 0.00660527\n",
            "Iteration 799, loss = 0.00651711\n",
            "Iteration 800, loss = 0.00646395\n",
            "Iteration 801, loss = 0.00649981\n",
            "Iteration 802, loss = 0.00649742\n",
            "Iteration 803, loss = 0.00646615\n",
            "Iteration 804, loss = 0.00642214\n",
            "Iteration 805, loss = 0.00649463\n",
            "Iteration 806, loss = 0.00647603\n",
            "Iteration 807, loss = 0.00643466\n",
            "Iteration 808, loss = 0.00641447\n",
            "Iteration 809, loss = 0.00642314\n",
            "Iteration 810, loss = 0.00645452\n",
            "Iteration 811, loss = 0.00636157\n",
            "Iteration 812, loss = 0.00637271\n",
            "Iteration 813, loss = 0.00642967\n",
            "Iteration 814, loss = 0.00632740\n",
            "Iteration 815, loss = 0.00641273\n",
            "Iteration 816, loss = 0.00639621\n",
            "Iteration 817, loss = 0.00629313\n",
            "Iteration 818, loss = 0.00626153\n",
            "Iteration 819, loss = 0.00627403\n",
            "Iteration 820, loss = 0.00631957\n",
            "Iteration 821, loss = 0.00624649\n",
            "Iteration 822, loss = 0.00625563\n",
            "Iteration 823, loss = 0.00627470\n",
            "Iteration 824, loss = 0.00624969\n",
            "Iteration 825, loss = 0.00623862\n",
            "Iteration 826, loss = 0.00621399\n",
            "Iteration 827, loss = 0.00625745\n",
            "Iteration 828, loss = 0.00621141\n",
            "Iteration 829, loss = 0.00618554\n",
            "Iteration 830, loss = 0.00622002\n",
            "Iteration 831, loss = 0.00614562\n",
            "Iteration 832, loss = 0.00620581\n",
            "Iteration 833, loss = 0.00615432\n",
            "Iteration 834, loss = 0.00615448\n",
            "Iteration 835, loss = 0.00620003\n",
            "Iteration 836, loss = 0.00616388\n",
            "Iteration 837, loss = 0.00616623\n",
            "Iteration 838, loss = 0.00606373\n",
            "Iteration 839, loss = 0.00606960\n",
            "Iteration 840, loss = 0.00607646\n",
            "Iteration 841, loss = 0.00608137\n",
            "Iteration 842, loss = 0.00609896\n",
            "Iteration 843, loss = 0.00609038\n",
            "Iteration 844, loss = 0.00608939\n",
            "Iteration 845, loss = 0.00604779\n",
            "Iteration 846, loss = 0.00611691\n",
            "Iteration 847, loss = 0.00605471\n",
            "Iteration 848, loss = 0.00596187\n",
            "Iteration 849, loss = 0.00595694\n",
            "Iteration 850, loss = 0.00598988\n",
            "Iteration 851, loss = 0.00597978\n",
            "Iteration 852, loss = 0.00595606\n",
            "Iteration 853, loss = 0.00595978\n",
            "Iteration 854, loss = 0.00592555\n",
            "Iteration 855, loss = 0.00599196\n",
            "Iteration 856, loss = 0.00595105\n",
            "Iteration 857, loss = 0.00590844\n",
            "Iteration 858, loss = 0.00593212\n",
            "Iteration 859, loss = 0.00589106\n",
            "Iteration 860, loss = 0.00600926\n",
            "Iteration 861, loss = 0.00593897\n",
            "Iteration 862, loss = 0.00587139\n",
            "Iteration 863, loss = 0.00583847\n",
            "Iteration 864, loss = 0.00581838\n",
            "Iteration 865, loss = 0.00583225\n",
            "Iteration 866, loss = 0.00580025\n",
            "Iteration 867, loss = 0.00583167\n",
            "Iteration 868, loss = 0.00591981\n",
            "Iteration 869, loss = 0.00580205\n",
            "Iteration 870, loss = 0.00588899\n",
            "Iteration 871, loss = 0.00578746\n",
            "Iteration 872, loss = 0.00583012\n",
            "Iteration 873, loss = 0.00570700\n",
            "Iteration 874, loss = 0.00576695\n",
            "Iteration 875, loss = 0.00572869\n",
            "Iteration 876, loss = 0.00574405\n",
            "Iteration 877, loss = 0.00571944\n",
            "Iteration 878, loss = 0.00573161\n",
            "Iteration 879, loss = 0.00567974\n",
            "Iteration 880, loss = 0.00567563\n",
            "Iteration 881, loss = 0.00568347\n",
            "Iteration 882, loss = 0.00569983\n",
            "Iteration 883, loss = 0.00562654\n",
            "Iteration 884, loss = 0.00566237\n",
            "Iteration 885, loss = 0.00566773\n",
            "Iteration 886, loss = 0.00566576\n",
            "Iteration 887, loss = 0.00564937\n",
            "Iteration 888, loss = 0.00573934\n",
            "Iteration 889, loss = 0.00566216\n",
            "Iteration 890, loss = 0.00569360\n",
            "Iteration 891, loss = 0.00563030\n",
            "Iteration 892, loss = 0.00558453\n",
            "Iteration 893, loss = 0.00557463\n",
            "Iteration 894, loss = 0.00556583\n",
            "Iteration 895, loss = 0.00554942\n",
            "Iteration 896, loss = 0.00555370\n",
            "Iteration 897, loss = 0.00555272\n",
            "Iteration 898, loss = 0.00559085\n",
            "Iteration 899, loss = 0.00551576\n",
            "Iteration 900, loss = 0.00553118\n",
            "Iteration 901, loss = 0.00548660\n",
            "Iteration 902, loss = 0.00552596\n",
            "Iteration 903, loss = 0.00551506\n",
            "Iteration 904, loss = 0.00548471\n",
            "Iteration 905, loss = 0.00547019\n",
            "Iteration 906, loss = 0.00551775\n",
            "Iteration 907, loss = 0.00541660\n",
            "Iteration 908, loss = 0.00547887\n",
            "Iteration 909, loss = 0.00548743\n",
            "Iteration 910, loss = 0.00546034\n",
            "Iteration 911, loss = 0.00539649\n",
            "Iteration 912, loss = 0.00538754\n",
            "Iteration 913, loss = 0.00538227\n",
            "Iteration 914, loss = 0.00539352\n",
            "Iteration 915, loss = 0.00539860\n",
            "Iteration 916, loss = 0.00540273\n",
            "Iteration 917, loss = 0.00532975\n",
            "Iteration 918, loss = 0.00543005\n",
            "Iteration 919, loss = 0.00536341\n",
            "Iteration 920, loss = 0.00533269\n",
            "Iteration 921, loss = 0.00543986\n",
            "Iteration 922, loss = 0.00535218\n",
            "Iteration 923, loss = 0.00530304\n",
            "Iteration 924, loss = 0.00529052\n",
            "Iteration 925, loss = 0.00533149\n",
            "Iteration 926, loss = 0.00526451\n",
            "Iteration 927, loss = 0.00528453\n",
            "Iteration 928, loss = 0.00530341\n",
            "Iteration 929, loss = 0.00527170\n",
            "Iteration 930, loss = 0.00526866\n",
            "Iteration 931, loss = 0.00521366\n",
            "Iteration 932, loss = 0.00529702\n",
            "Iteration 933, loss = 0.00539607\n",
            "Iteration 934, loss = 0.00520566\n",
            "Iteration 935, loss = 0.00522212\n",
            "Iteration 936, loss = 0.00526843\n",
            "Iteration 937, loss = 0.00514626\n",
            "Iteration 938, loss = 0.00519458\n",
            "Iteration 939, loss = 0.00523957\n",
            "Iteration 940, loss = 0.00519754\n",
            "Iteration 941, loss = 0.00537769\n",
            "Iteration 942, loss = 0.00518429\n",
            "Iteration 943, loss = 0.00517960\n",
            "Iteration 944, loss = 0.00514484\n",
            "Iteration 945, loss = 0.00512979\n",
            "Iteration 946, loss = 0.00510863\n",
            "Iteration 947, loss = 0.00509308\n",
            "Iteration 948, loss = 0.00511790\n",
            "Iteration 949, loss = 0.00508475\n",
            "Iteration 950, loss = 0.00514988\n",
            "Iteration 951, loss = 0.00502208\n",
            "Iteration 952, loss = 0.00526287\n",
            "Iteration 953, loss = 0.00515807\n",
            "Iteration 954, loss = 0.00507882\n",
            "Iteration 955, loss = 0.00512991\n",
            "Iteration 956, loss = 0.00502858\n",
            "Iteration 957, loss = 0.00505374\n",
            "Iteration 958, loss = 0.00504261\n",
            "Iteration 959, loss = 0.00499670\n",
            "Iteration 960, loss = 0.00499893\n",
            "Iteration 961, loss = 0.00500597\n",
            "Iteration 962, loss = 0.00502867\n",
            "Iteration 963, loss = 0.00500288\n",
            "Iteration 964, loss = 0.00505693\n",
            "Iteration 965, loss = 0.00498409\n",
            "Iteration 966, loss = 0.00500284\n",
            "Iteration 967, loss = 0.00495138\n",
            "Iteration 968, loss = 0.00496224\n",
            "Iteration 969, loss = 0.00497007\n",
            "Iteration 970, loss = 0.00493814\n",
            "Iteration 971, loss = 0.00500274\n",
            "Iteration 972, loss = 0.00493402\n",
            "Iteration 973, loss = 0.00485489\n",
            "Iteration 974, loss = 0.00494170\n",
            "Iteration 975, loss = 0.00490964\n",
            "Iteration 976, loss = 0.00487293\n",
            "Iteration 977, loss = 0.00488078\n",
            "Iteration 978, loss = 0.00485559\n",
            "Iteration 979, loss = 0.00485044\n",
            "Iteration 980, loss = 0.00488912\n",
            "Iteration 981, loss = 0.00484695\n",
            "Iteration 982, loss = 0.00482053\n",
            "Iteration 983, loss = 0.00483681\n",
            "Iteration 984, loss = 0.00486317\n",
            "Iteration 985, loss = 0.00487075\n",
            "Iteration 986, loss = 0.00480775\n",
            "Iteration 987, loss = 0.00481120\n",
            "Iteration 988, loss = 0.00484660\n",
            "Iteration 989, loss = 0.00478160\n",
            "Iteration 990, loss = 0.00478611\n",
            "Iteration 991, loss = 0.00485884\n",
            "Iteration 992, loss = 0.00480251\n",
            "Iteration 993, loss = 0.00478168\n",
            "Iteration 994, loss = 0.00477304\n",
            "Iteration 995, loss = 0.00476990\n",
            "Iteration 996, loss = 0.00472905\n",
            "Iteration 997, loss = 0.00475227\n",
            "Iteration 998, loss = 0.00473886\n",
            "Iteration 999, loss = 0.00472114\n",
            "Iteration 1000, loss = 0.00475397\n",
            "Iteration 1001, loss = 0.00473564\n",
            "Iteration 1002, loss = 0.00476195\n",
            "Iteration 1003, loss = 0.00472148\n",
            "Iteration 1004, loss = 0.00473799\n",
            "Iteration 1005, loss = 0.00466118\n",
            "Iteration 1006, loss = 0.00474119\n",
            "Iteration 1007, loss = 0.00466332\n",
            "Iteration 1008, loss = 0.00466903\n",
            "Iteration 1009, loss = 0.00467323\n",
            "Iteration 1010, loss = 0.00466577\n",
            "Iteration 1011, loss = 0.00475590\n",
            "Iteration 1012, loss = 0.00467362\n",
            "Iteration 1013, loss = 0.00469701\n",
            "Iteration 1014, loss = 0.00470033\n",
            "Iteration 1015, loss = 0.00462874\n",
            "Iteration 1016, loss = 0.00460298\n",
            "Iteration 1017, loss = 0.00465779\n",
            "Iteration 1018, loss = 0.00461007\n",
            "Iteration 1019, loss = 0.00455014\n",
            "Iteration 1020, loss = 0.00462192\n",
            "Iteration 1021, loss = 0.00456228\n",
            "Iteration 1022, loss = 0.00457572\n",
            "Iteration 1023, loss = 0.00459397\n",
            "Iteration 1024, loss = 0.00458098\n",
            "Iteration 1025, loss = 0.00454373\n",
            "Iteration 1026, loss = 0.00451889\n",
            "Iteration 1027, loss = 0.00452050\n",
            "Iteration 1028, loss = 0.00451396\n",
            "Iteration 1029, loss = 0.00450738\n",
            "Iteration 1030, loss = 0.00450015\n",
            "Iteration 1031, loss = 0.00450523\n",
            "Iteration 1032, loss = 0.00451029\n",
            "Iteration 1033, loss = 0.00445638\n",
            "Iteration 1034, loss = 0.00450806\n",
            "Iteration 1035, loss = 0.00447129\n",
            "Iteration 1036, loss = 0.00442921\n",
            "Iteration 1037, loss = 0.00451315\n",
            "Iteration 1038, loss = 0.00447691\n",
            "Iteration 1039, loss = 0.00444467\n",
            "Iteration 1040, loss = 0.00443695\n",
            "Iteration 1041, loss = 0.00441963\n",
            "Iteration 1042, loss = 0.00444149\n",
            "Iteration 1043, loss = 0.00443531\n",
            "Iteration 1044, loss = 0.00440506\n",
            "Iteration 1045, loss = 0.00438958\n",
            "Iteration 1046, loss = 0.00444545\n",
            "Iteration 1047, loss = 0.00447314\n",
            "Iteration 1048, loss = 0.00445064\n",
            "Iteration 1049, loss = 0.00440769\n",
            "Iteration 1050, loss = 0.00444284\n",
            "Iteration 1051, loss = 0.00439470\n",
            "Iteration 1052, loss = 0.00435424\n",
            "Iteration 1053, loss = 0.00439177\n",
            "Iteration 1054, loss = 0.00434602\n",
            "Iteration 1055, loss = 0.00433645\n",
            "Iteration 1056, loss = 0.00433701\n",
            "Iteration 1057, loss = 0.00437234\n",
            "Iteration 1058, loss = 0.00436922\n",
            "Iteration 1059, loss = 0.00444817\n",
            "Iteration 1060, loss = 0.00429844\n",
            "Iteration 1061, loss = 0.00433388\n",
            "Iteration 1062, loss = 0.00432312\n",
            "Iteration 1063, loss = 0.00430524\n",
            "Iteration 1064, loss = 0.00431857\n",
            "Iteration 1065, loss = 0.00431530\n",
            "Iteration 1066, loss = 0.00428047\n",
            "Iteration 1067, loss = 0.00427096\n",
            "Iteration 1068, loss = 0.00429781\n",
            "Iteration 1069, loss = 0.00427641\n",
            "Iteration 1070, loss = 0.00425832\n",
            "Iteration 1071, loss = 0.00423844\n",
            "Iteration 1072, loss = 0.00423411\n",
            "Iteration 1073, loss = 0.00422434\n",
            "Iteration 1074, loss = 0.00427224\n",
            "Iteration 1075, loss = 0.00426216\n",
            "Iteration 1076, loss = 0.00420802\n",
            "Iteration 1077, loss = 0.00425709\n",
            "Iteration 1078, loss = 0.00422736\n",
            "Iteration 1079, loss = 0.00421289\n",
            "Iteration 1080, loss = 0.00419689\n",
            "Iteration 1081, loss = 0.00421291\n",
            "Iteration 1082, loss = 0.00419560\n",
            "Iteration 1083, loss = 0.00417658\n",
            "Iteration 1084, loss = 0.00418390\n",
            "Iteration 1085, loss = 0.00418368\n",
            "Iteration 1086, loss = 0.00418926\n",
            "Iteration 1087, loss = 0.00420121\n",
            "Iteration 1088, loss = 0.00418132\n",
            "Iteration 1089, loss = 0.00417366\n",
            "Iteration 1090, loss = 0.00414429\n",
            "Iteration 1091, loss = 0.00415610\n",
            "Iteration 1092, loss = 0.00413384\n",
            "Iteration 1093, loss = 0.00411292\n",
            "Iteration 1094, loss = 0.00417990\n",
            "Iteration 1095, loss = 0.00409336\n",
            "Iteration 1096, loss = 0.00408855\n",
            "Iteration 1097, loss = 0.00409082\n",
            "Iteration 1098, loss = 0.00409418\n",
            "Iteration 1099, loss = 0.00411281\n",
            "Iteration 1100, loss = 0.00414034\n",
            "Iteration 1101, loss = 0.00407151\n",
            "Iteration 1102, loss = 0.00416546\n",
            "Iteration 1103, loss = 0.00406270\n",
            "Iteration 1104, loss = 0.00407112\n",
            "Iteration 1105, loss = 0.00416273\n",
            "Iteration 1106, loss = 0.00403494\n",
            "Iteration 1107, loss = 0.00405271\n",
            "Iteration 1108, loss = 0.00403943\n",
            "Iteration 1109, loss = 0.00404446\n",
            "Iteration 1110, loss = 0.00400903\n",
            "Iteration 1111, loss = 0.00403264\n",
            "Iteration 1112, loss = 0.00401253\n",
            "Iteration 1113, loss = 0.00399013\n",
            "Iteration 1114, loss = 0.00398642\n",
            "Iteration 1115, loss = 0.00400853\n",
            "Iteration 1116, loss = 0.00399934\n",
            "Iteration 1117, loss = 0.00399058\n",
            "Iteration 1118, loss = 0.00401205\n",
            "Iteration 1119, loss = 0.00396097\n",
            "Iteration 1120, loss = 0.00396507\n",
            "Iteration 1121, loss = 0.00394105\n",
            "Iteration 1122, loss = 0.00399479\n",
            "Iteration 1123, loss = 0.00394536\n",
            "Iteration 1124, loss = 0.00399163\n",
            "Iteration 1125, loss = 0.00392570\n",
            "Iteration 1126, loss = 0.00392974\n",
            "Iteration 1127, loss = 0.00393902\n",
            "Iteration 1128, loss = 0.00394199\n",
            "Iteration 1129, loss = 0.00390774\n",
            "Iteration 1130, loss = 0.00391290\n",
            "Iteration 1131, loss = 0.00396762\n",
            "Iteration 1132, loss = 0.00388642\n",
            "Iteration 1133, loss = 0.00400913\n",
            "Iteration 1134, loss = 0.00389726\n",
            "Iteration 1135, loss = 0.00390930\n",
            "Iteration 1136, loss = 0.00386689\n",
            "Iteration 1137, loss = 0.00389099\n",
            "Iteration 1138, loss = 0.00395284\n",
            "Iteration 1139, loss = 0.00383509\n",
            "Iteration 1140, loss = 0.00389877\n",
            "Iteration 1141, loss = 0.00384367\n",
            "Iteration 1142, loss = 0.00387311\n",
            "Iteration 1143, loss = 0.00399670\n",
            "Iteration 1144, loss = 0.00397952\n",
            "Iteration 1145, loss = 0.00386972\n",
            "Iteration 1146, loss = 0.00388985\n",
            "Iteration 1147, loss = 0.00386522\n",
            "Iteration 1148, loss = 0.00380509\n",
            "Iteration 1149, loss = 0.00382677\n",
            "Iteration 1150, loss = 0.00379600\n",
            "Iteration 1151, loss = 0.00378116\n",
            "Iteration 1152, loss = 0.00388200\n",
            "Iteration 1153, loss = 0.00379291\n",
            "Iteration 1154, loss = 0.00384925\n",
            "Iteration 1155, loss = 0.00394058\n",
            "Iteration 1156, loss = 0.00378200\n",
            "Iteration 1157, loss = 0.00375994\n",
            "Iteration 1158, loss = 0.00380856\n",
            "Iteration 1159, loss = 0.00379094\n",
            "Iteration 1160, loss = 0.00378469\n",
            "Iteration 1161, loss = 0.00380237\n",
            "Iteration 1162, loss = 0.00372397\n",
            "Iteration 1163, loss = 0.00373160\n",
            "Iteration 1164, loss = 0.00374232\n",
            "Iteration 1165, loss = 0.00374995\n",
            "Iteration 1166, loss = 0.00368413\n",
            "Iteration 1167, loss = 0.00371910\n",
            "Iteration 1168, loss = 0.00369149\n",
            "Iteration 1169, loss = 0.00375295\n",
            "Iteration 1170, loss = 0.00370244\n",
            "Iteration 1171, loss = 0.00370136\n",
            "Iteration 1172, loss = 0.00370486\n",
            "Iteration 1173, loss = 0.00374323\n",
            "Iteration 1174, loss = 0.00374153\n",
            "Iteration 1175, loss = 0.00369961\n",
            "Iteration 1176, loss = 0.00370387\n",
            "Iteration 1177, loss = 0.00371212\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=1500, tol=1e-06, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a rede neural indicando todos os atributos do modelo\n",
        "# Será criada duas camadas ocultas\n",
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
        "                                   solver='adam', activation='relu',\n",
        "                                   hidden_layer_sizes=(2,2))\n",
        "rede_neural_credit.fit(x_credit_treino, y_credit_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmzdx9ryU2P7",
        "outputId": "b1772eac-e41b-4b53-f30a-480e6a553c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.55554765\n",
            "Iteration 2, loss = 1.53576160\n",
            "Iteration 3, loss = 1.51660436\n",
            "Iteration 4, loss = 1.49789511\n",
            "Iteration 5, loss = 1.47987204\n",
            "Iteration 6, loss = 1.46256325\n",
            "Iteration 7, loss = 1.44588105\n",
            "Iteration 8, loss = 1.42972348\n",
            "Iteration 9, loss = 1.41424343\n",
            "Iteration 10, loss = 1.39928046\n",
            "Iteration 11, loss = 1.38503928\n",
            "Iteration 12, loss = 1.37107784\n",
            "Iteration 13, loss = 1.35801219\n",
            "Iteration 14, loss = 1.34537926\n",
            "Iteration 15, loss = 1.33324099\n",
            "Iteration 16, loss = 1.32156871\n",
            "Iteration 17, loss = 1.31024883\n",
            "Iteration 18, loss = 1.29938914\n",
            "Iteration 19, loss = 1.28879558\n",
            "Iteration 20, loss = 1.27877702\n",
            "Iteration 21, loss = 1.26897667\n",
            "Iteration 22, loss = 1.25961360\n",
            "Iteration 23, loss = 1.25044170\n",
            "Iteration 24, loss = 1.24145822\n",
            "Iteration 25, loss = 1.23284391\n",
            "Iteration 26, loss = 1.22438439\n",
            "Iteration 27, loss = 1.21618559\n",
            "Iteration 28, loss = 1.20823394\n",
            "Iteration 29, loss = 1.20042585\n",
            "Iteration 30, loss = 1.19289677\n",
            "Iteration 31, loss = 1.18550554\n",
            "Iteration 32, loss = 1.17823843\n",
            "Iteration 33, loss = 1.17116815\n",
            "Iteration 34, loss = 1.16436024\n",
            "Iteration 35, loss = 1.15759177\n",
            "Iteration 36, loss = 1.15095180\n",
            "Iteration 37, loss = 1.14453826\n",
            "Iteration 38, loss = 1.13814568\n",
            "Iteration 39, loss = 1.13189915\n",
            "Iteration 40, loss = 1.12582090\n",
            "Iteration 41, loss = 1.11976653\n",
            "Iteration 42, loss = 1.11381887\n",
            "Iteration 43, loss = 1.10802889\n",
            "Iteration 44, loss = 1.10231923\n",
            "Iteration 45, loss = 1.09671901\n",
            "Iteration 46, loss = 1.09120708\n",
            "Iteration 47, loss = 1.08579940\n",
            "Iteration 48, loss = 1.08045697\n",
            "Iteration 49, loss = 1.07519891\n",
            "Iteration 50, loss = 1.07001373\n",
            "Iteration 51, loss = 1.06481813\n",
            "Iteration 52, loss = 1.05975105\n",
            "Iteration 53, loss = 1.05470854\n",
            "Iteration 54, loss = 1.04975985\n",
            "Iteration 55, loss = 1.04485713\n",
            "Iteration 56, loss = 1.03999331\n",
            "Iteration 57, loss = 1.03516422\n",
            "Iteration 58, loss = 1.03042018\n",
            "Iteration 59, loss = 1.02576569\n",
            "Iteration 60, loss = 1.02112336\n",
            "Iteration 61, loss = 1.01655020\n",
            "Iteration 62, loss = 1.01201780\n",
            "Iteration 63, loss = 1.00752028\n",
            "Iteration 64, loss = 1.00309251\n",
            "Iteration 65, loss = 0.99870527\n",
            "Iteration 66, loss = 0.99431207\n",
            "Iteration 67, loss = 0.99003089\n",
            "Iteration 68, loss = 0.98574817\n",
            "Iteration 69, loss = 0.98157002\n",
            "Iteration 70, loss = 0.97738181\n",
            "Iteration 71, loss = 0.97327409\n",
            "Iteration 72, loss = 0.96916923\n",
            "Iteration 73, loss = 0.96517293\n",
            "Iteration 74, loss = 0.96117300\n",
            "Iteration 75, loss = 0.95723397\n",
            "Iteration 76, loss = 0.95331680\n",
            "Iteration 77, loss = 0.94945589\n",
            "Iteration 78, loss = 0.94564105\n",
            "Iteration 79, loss = 0.94179449\n",
            "Iteration 80, loss = 0.93805058\n",
            "Iteration 81, loss = 0.93428040\n",
            "Iteration 82, loss = 0.93056878\n",
            "Iteration 83, loss = 0.92687598\n",
            "Iteration 84, loss = 0.92322670\n",
            "Iteration 85, loss = 0.91961393\n",
            "Iteration 86, loss = 0.91603173\n",
            "Iteration 87, loss = 0.91245686\n",
            "Iteration 88, loss = 0.90892435\n",
            "Iteration 89, loss = 0.90543272\n",
            "Iteration 90, loss = 0.90194140\n",
            "Iteration 91, loss = 0.89847250\n",
            "Iteration 92, loss = 0.89504647\n",
            "Iteration 93, loss = 0.89165464\n",
            "Iteration 94, loss = 0.88828256\n",
            "Iteration 95, loss = 0.88491107\n",
            "Iteration 96, loss = 0.88158969\n",
            "Iteration 97, loss = 0.87828946\n",
            "Iteration 98, loss = 0.87498326\n",
            "Iteration 99, loss = 0.87172079\n",
            "Iteration 100, loss = 0.86844430\n",
            "Iteration 101, loss = 0.86521773\n",
            "Iteration 102, loss = 0.86201795\n",
            "Iteration 103, loss = 0.85882539\n",
            "Iteration 104, loss = 0.85565470\n",
            "Iteration 105, loss = 0.85250558\n",
            "Iteration 106, loss = 0.84933342\n",
            "Iteration 107, loss = 0.84619246\n",
            "Iteration 108, loss = 0.84309403\n",
            "Iteration 109, loss = 0.83997564\n",
            "Iteration 110, loss = 0.83684857\n",
            "Iteration 111, loss = 0.83377774\n",
            "Iteration 112, loss = 0.83070083\n",
            "Iteration 113, loss = 0.82763590\n",
            "Iteration 114, loss = 0.82460352\n",
            "Iteration 115, loss = 0.82152679\n",
            "Iteration 116, loss = 0.81850207\n",
            "Iteration 117, loss = 0.81546038\n",
            "Iteration 118, loss = 0.81242139\n",
            "Iteration 119, loss = 0.80937107\n",
            "Iteration 120, loss = 0.80630701\n",
            "Iteration 121, loss = 0.80325452\n",
            "Iteration 122, loss = 0.80021017\n",
            "Iteration 123, loss = 0.79713910\n",
            "Iteration 124, loss = 0.79404351\n",
            "Iteration 125, loss = 0.79091243\n",
            "Iteration 126, loss = 0.78777747\n",
            "Iteration 127, loss = 0.78458454\n",
            "Iteration 128, loss = 0.78134995\n",
            "Iteration 129, loss = 0.77803234\n",
            "Iteration 130, loss = 0.77472250\n",
            "Iteration 131, loss = 0.77121986\n",
            "Iteration 132, loss = 0.76771217\n",
            "Iteration 133, loss = 0.76398403\n",
            "Iteration 134, loss = 0.76011575\n",
            "Iteration 135, loss = 0.75602542\n",
            "Iteration 136, loss = 0.75164747\n",
            "Iteration 137, loss = 0.74699558\n",
            "Iteration 138, loss = 0.74200404\n",
            "Iteration 139, loss = 0.73673705\n",
            "Iteration 140, loss = 0.73120320\n",
            "Iteration 141, loss = 0.72536403\n",
            "Iteration 142, loss = 0.71931973\n",
            "Iteration 143, loss = 0.71292056\n",
            "Iteration 144, loss = 0.70633934\n",
            "Iteration 145, loss = 0.69941151\n",
            "Iteration 146, loss = 0.69204110\n",
            "Iteration 147, loss = 0.68435869\n",
            "Iteration 148, loss = 0.67614522\n",
            "Iteration 149, loss = 0.66763398\n",
            "Iteration 150, loss = 0.65888424\n",
            "Iteration 151, loss = 0.64948618\n",
            "Iteration 152, loss = 0.63985317\n",
            "Iteration 153, loss = 0.62984884\n",
            "Iteration 154, loss = 0.61980587\n",
            "Iteration 155, loss = 0.60924870\n",
            "Iteration 156, loss = 0.59901262\n",
            "Iteration 157, loss = 0.58847844\n",
            "Iteration 158, loss = 0.57817540\n",
            "Iteration 159, loss = 0.56803219\n",
            "Iteration 160, loss = 0.55803234\n",
            "Iteration 161, loss = 0.54814575\n",
            "Iteration 162, loss = 0.53894331\n",
            "Iteration 163, loss = 0.52986890\n",
            "Iteration 164, loss = 0.52122239\n",
            "Iteration 165, loss = 0.51297264\n",
            "Iteration 166, loss = 0.50512920\n",
            "Iteration 167, loss = 0.49774728\n",
            "Iteration 168, loss = 0.49064727\n",
            "Iteration 169, loss = 0.48406306\n",
            "Iteration 170, loss = 0.47778805\n",
            "Iteration 171, loss = 0.47206435\n",
            "Iteration 172, loss = 0.46652675\n",
            "Iteration 173, loss = 0.46126253\n",
            "Iteration 174, loss = 0.45637191\n",
            "Iteration 175, loss = 0.45154338\n",
            "Iteration 176, loss = 0.44700890\n",
            "Iteration 177, loss = 0.44274115\n",
            "Iteration 178, loss = 0.43875247\n",
            "Iteration 179, loss = 0.43489947\n",
            "Iteration 180, loss = 0.43118306\n",
            "Iteration 181, loss = 0.42764710\n",
            "Iteration 182, loss = 0.42422884\n",
            "Iteration 183, loss = 0.42092720\n",
            "Iteration 184, loss = 0.41775643\n",
            "Iteration 185, loss = 0.41466857\n",
            "Iteration 186, loss = 0.41173062\n",
            "Iteration 187, loss = 0.40889983\n",
            "Iteration 188, loss = 0.40615701\n",
            "Iteration 189, loss = 0.40348396\n",
            "Iteration 190, loss = 0.40086804\n",
            "Iteration 191, loss = 0.39842758\n",
            "Iteration 192, loss = 0.39595525\n",
            "Iteration 193, loss = 0.39358335\n",
            "Iteration 194, loss = 0.39126108\n",
            "Iteration 195, loss = 0.38893914\n",
            "Iteration 196, loss = 0.38667643\n",
            "Iteration 197, loss = 0.38444476\n",
            "Iteration 198, loss = 0.38242241\n",
            "Iteration 199, loss = 0.38039793\n",
            "Iteration 200, loss = 0.37843813\n",
            "Iteration 201, loss = 0.37664220\n",
            "Iteration 202, loss = 0.37478177\n",
            "Iteration 203, loss = 0.37300043\n",
            "Iteration 204, loss = 0.37128471\n",
            "Iteration 205, loss = 0.36952780\n",
            "Iteration 206, loss = 0.36788397\n",
            "Iteration 207, loss = 0.36620294\n",
            "Iteration 208, loss = 0.36462673\n",
            "Iteration 209, loss = 0.36305389\n",
            "Iteration 210, loss = 0.36153165\n",
            "Iteration 211, loss = 0.35999389\n",
            "Iteration 212, loss = 0.35855841\n",
            "Iteration 213, loss = 0.35711460\n",
            "Iteration 214, loss = 0.35571318\n",
            "Iteration 215, loss = 0.35430980\n",
            "Iteration 216, loss = 0.35293436\n",
            "Iteration 217, loss = 0.35161274\n",
            "Iteration 218, loss = 0.35024224\n",
            "Iteration 219, loss = 0.34889735\n",
            "Iteration 220, loss = 0.34755652\n",
            "Iteration 221, loss = 0.34611224\n",
            "Iteration 222, loss = 0.34475074\n",
            "Iteration 223, loss = 0.34336004\n",
            "Iteration 224, loss = 0.34190164\n",
            "Iteration 225, loss = 0.34050492\n",
            "Iteration 226, loss = 0.33906070\n",
            "Iteration 227, loss = 0.33770091\n",
            "Iteration 228, loss = 0.33632366\n",
            "Iteration 229, loss = 0.33495483\n",
            "Iteration 230, loss = 0.33359573\n",
            "Iteration 231, loss = 0.33229008\n",
            "Iteration 232, loss = 0.33098293\n",
            "Iteration 233, loss = 0.32967352\n",
            "Iteration 234, loss = 0.32836906\n",
            "Iteration 235, loss = 0.32709245\n",
            "Iteration 236, loss = 0.32583567\n",
            "Iteration 237, loss = 0.32461333\n",
            "Iteration 238, loss = 0.32331852\n",
            "Iteration 239, loss = 0.32206989\n",
            "Iteration 240, loss = 0.32085800\n",
            "Iteration 241, loss = 0.31960569\n",
            "Iteration 242, loss = 0.31835241\n",
            "Iteration 243, loss = 0.31709824\n",
            "Iteration 244, loss = 0.31583547\n",
            "Iteration 245, loss = 0.31451623\n",
            "Iteration 246, loss = 0.31319432\n",
            "Iteration 247, loss = 0.31185154\n",
            "Iteration 248, loss = 0.31050463\n",
            "Iteration 249, loss = 0.30911149\n",
            "Iteration 250, loss = 0.30774689\n",
            "Iteration 251, loss = 0.30636747\n",
            "Iteration 252, loss = 0.30498653\n",
            "Iteration 253, loss = 0.30358540\n",
            "Iteration 254, loss = 0.30213542\n",
            "Iteration 255, loss = 0.30068278\n",
            "Iteration 256, loss = 0.29915405\n",
            "Iteration 257, loss = 0.29762228\n",
            "Iteration 258, loss = 0.29605403\n",
            "Iteration 259, loss = 0.29444983\n",
            "Iteration 260, loss = 0.29276278\n",
            "Iteration 261, loss = 0.29108169\n",
            "Iteration 262, loss = 0.28933530\n",
            "Iteration 263, loss = 0.28751334\n",
            "Iteration 264, loss = 0.28570425\n",
            "Iteration 265, loss = 0.28379242\n",
            "Iteration 266, loss = 0.28186002\n",
            "Iteration 267, loss = 0.27986570\n",
            "Iteration 268, loss = 0.27778783\n",
            "Iteration 269, loss = 0.27572423\n",
            "Iteration 270, loss = 0.27351229\n",
            "Iteration 271, loss = 0.27135044\n",
            "Iteration 272, loss = 0.26918773\n",
            "Iteration 273, loss = 0.26702241\n",
            "Iteration 274, loss = 0.26485296\n",
            "Iteration 275, loss = 0.26260243\n",
            "Iteration 276, loss = 0.26044102\n",
            "Iteration 277, loss = 0.25830785\n",
            "Iteration 278, loss = 0.25613584\n",
            "Iteration 279, loss = 0.25396455\n",
            "Iteration 280, loss = 0.25182979\n",
            "Iteration 281, loss = 0.24972090\n",
            "Iteration 282, loss = 0.24767001\n",
            "Iteration 283, loss = 0.24560540\n",
            "Iteration 284, loss = 0.24351613\n",
            "Iteration 285, loss = 0.24145684\n",
            "Iteration 286, loss = 0.23945385\n",
            "Iteration 287, loss = 0.23744077\n",
            "Iteration 288, loss = 0.23542803\n",
            "Iteration 289, loss = 0.23338690\n",
            "Iteration 290, loss = 0.23138535\n",
            "Iteration 291, loss = 0.22940658\n",
            "Iteration 292, loss = 0.22734997\n",
            "Iteration 293, loss = 0.22536183\n",
            "Iteration 294, loss = 0.22332814\n",
            "Iteration 295, loss = 0.22128305\n",
            "Iteration 296, loss = 0.21931926\n",
            "Iteration 297, loss = 0.21726620\n",
            "Iteration 298, loss = 0.21523510\n",
            "Iteration 299, loss = 0.21316683\n",
            "Iteration 300, loss = 0.21113122\n",
            "Iteration 301, loss = 0.20909332\n",
            "Iteration 302, loss = 0.20703422\n",
            "Iteration 303, loss = 0.20495667\n",
            "Iteration 304, loss = 0.20294200\n",
            "Iteration 305, loss = 0.20089776\n",
            "Iteration 306, loss = 0.19883128\n",
            "Iteration 307, loss = 0.19680915\n",
            "Iteration 308, loss = 0.19479830\n",
            "Iteration 309, loss = 0.19272134\n",
            "Iteration 310, loss = 0.19070176\n",
            "Iteration 311, loss = 0.18868326\n",
            "Iteration 312, loss = 0.18666285\n",
            "Iteration 313, loss = 0.18463617\n",
            "Iteration 314, loss = 0.18264606\n",
            "Iteration 315, loss = 0.18059536\n",
            "Iteration 316, loss = 0.17861027\n",
            "Iteration 317, loss = 0.17658075\n",
            "Iteration 318, loss = 0.17455651\n",
            "Iteration 319, loss = 0.17257142\n",
            "Iteration 320, loss = 0.17063141\n",
            "Iteration 321, loss = 0.16859950\n",
            "Iteration 322, loss = 0.16660319\n",
            "Iteration 323, loss = 0.16467878\n",
            "Iteration 324, loss = 0.16267241\n",
            "Iteration 325, loss = 0.16076586\n",
            "Iteration 326, loss = 0.15881332\n",
            "Iteration 327, loss = 0.15691197\n",
            "Iteration 328, loss = 0.15498253\n",
            "Iteration 329, loss = 0.15311451\n",
            "Iteration 330, loss = 0.15120969\n",
            "Iteration 331, loss = 0.14938227\n",
            "Iteration 332, loss = 0.14754283\n",
            "Iteration 333, loss = 0.14577674\n",
            "Iteration 334, loss = 0.14400210\n",
            "Iteration 335, loss = 0.14222501\n",
            "Iteration 336, loss = 0.14051428\n",
            "Iteration 337, loss = 0.13879800\n",
            "Iteration 338, loss = 0.13708495\n",
            "Iteration 339, loss = 0.13539992\n",
            "Iteration 340, loss = 0.13375970\n",
            "Iteration 341, loss = 0.13209000\n",
            "Iteration 342, loss = 0.13044491\n",
            "Iteration 343, loss = 0.12885408\n",
            "Iteration 344, loss = 0.12727897\n",
            "Iteration 345, loss = 0.12568018\n",
            "Iteration 346, loss = 0.12415857\n",
            "Iteration 347, loss = 0.12260000\n",
            "Iteration 348, loss = 0.12109202\n",
            "Iteration 349, loss = 0.11957984\n",
            "Iteration 350, loss = 0.11810278\n",
            "Iteration 351, loss = 0.11664026\n",
            "Iteration 352, loss = 0.11517620\n",
            "Iteration 353, loss = 0.11374522\n",
            "Iteration 354, loss = 0.11234989\n",
            "Iteration 355, loss = 0.11098607\n",
            "Iteration 356, loss = 0.10961546\n",
            "Iteration 357, loss = 0.10832086\n",
            "Iteration 358, loss = 0.10695595\n",
            "Iteration 359, loss = 0.10571348\n",
            "Iteration 360, loss = 0.10441356\n",
            "Iteration 361, loss = 0.10314453\n",
            "Iteration 362, loss = 0.10189414\n",
            "Iteration 363, loss = 0.10068920\n",
            "Iteration 364, loss = 0.09951721\n",
            "Iteration 365, loss = 0.09829680\n",
            "Iteration 366, loss = 0.09717118\n",
            "Iteration 367, loss = 0.09602077\n",
            "Iteration 368, loss = 0.09491675\n",
            "Iteration 369, loss = 0.09382898\n",
            "Iteration 370, loss = 0.09270750\n",
            "Iteration 371, loss = 0.09163873\n",
            "Iteration 372, loss = 0.09060000\n",
            "Iteration 373, loss = 0.08955347\n",
            "Iteration 374, loss = 0.08854558\n",
            "Iteration 375, loss = 0.08756778\n",
            "Iteration 376, loss = 0.08656693\n",
            "Iteration 377, loss = 0.08560841\n",
            "Iteration 378, loss = 0.08462279\n",
            "Iteration 379, loss = 0.08371733\n",
            "Iteration 380, loss = 0.08278891\n",
            "Iteration 381, loss = 0.08187143\n",
            "Iteration 382, loss = 0.08100199\n",
            "Iteration 383, loss = 0.08012931\n",
            "Iteration 384, loss = 0.07930896\n",
            "Iteration 385, loss = 0.07844799\n",
            "Iteration 386, loss = 0.07763287\n",
            "Iteration 387, loss = 0.07682852\n",
            "Iteration 388, loss = 0.07600278\n",
            "Iteration 389, loss = 0.07523123\n",
            "Iteration 390, loss = 0.07447551\n",
            "Iteration 391, loss = 0.07370930\n",
            "Iteration 392, loss = 0.07299573\n",
            "Iteration 393, loss = 0.07222729\n",
            "Iteration 394, loss = 0.07152202\n",
            "Iteration 395, loss = 0.07078381\n",
            "Iteration 396, loss = 0.07010161\n",
            "Iteration 397, loss = 0.06939950\n",
            "Iteration 398, loss = 0.06871387\n",
            "Iteration 399, loss = 0.06806333\n",
            "Iteration 400, loss = 0.06741481\n",
            "Iteration 401, loss = 0.06675752\n",
            "Iteration 402, loss = 0.06613406\n",
            "Iteration 403, loss = 0.06550649\n",
            "Iteration 404, loss = 0.06489010\n",
            "Iteration 405, loss = 0.06429779\n",
            "Iteration 406, loss = 0.06370594\n",
            "Iteration 407, loss = 0.06310619\n",
            "Iteration 408, loss = 0.06253908\n",
            "Iteration 409, loss = 0.06193014\n",
            "Iteration 410, loss = 0.06143249\n",
            "Iteration 411, loss = 0.06082763\n",
            "Iteration 412, loss = 0.06028282\n",
            "Iteration 413, loss = 0.05973818\n",
            "Iteration 414, loss = 0.05922445\n",
            "Iteration 415, loss = 0.05867075\n",
            "Iteration 416, loss = 0.05818800\n",
            "Iteration 417, loss = 0.05769595\n",
            "Iteration 418, loss = 0.05719705\n",
            "Iteration 419, loss = 0.05670379\n",
            "Iteration 420, loss = 0.05621709\n",
            "Iteration 421, loss = 0.05572328\n",
            "Iteration 422, loss = 0.05531163\n",
            "Iteration 423, loss = 0.05481229\n",
            "Iteration 424, loss = 0.05438209\n",
            "Iteration 425, loss = 0.05391099\n",
            "Iteration 426, loss = 0.05348355\n",
            "Iteration 427, loss = 0.05303886\n",
            "Iteration 428, loss = 0.05262311\n",
            "Iteration 429, loss = 0.05219158\n",
            "Iteration 430, loss = 0.05178302\n",
            "Iteration 431, loss = 0.05136339\n",
            "Iteration 432, loss = 0.05097018\n",
            "Iteration 433, loss = 0.05057083\n",
            "Iteration 434, loss = 0.05017554\n",
            "Iteration 435, loss = 0.04978829\n",
            "Iteration 436, loss = 0.04940616\n",
            "Iteration 437, loss = 0.04901832\n",
            "Iteration 438, loss = 0.04865147\n",
            "Iteration 439, loss = 0.04829283\n",
            "Iteration 440, loss = 0.04792389\n",
            "Iteration 441, loss = 0.04758647\n",
            "Iteration 442, loss = 0.04721693\n",
            "Iteration 443, loss = 0.04686658\n",
            "Iteration 444, loss = 0.04653640\n",
            "Iteration 445, loss = 0.04620215\n",
            "Iteration 446, loss = 0.04585969\n",
            "Iteration 447, loss = 0.04550668\n",
            "Iteration 448, loss = 0.04517549\n",
            "Iteration 449, loss = 0.04485541\n",
            "Iteration 450, loss = 0.04456369\n",
            "Iteration 451, loss = 0.04422242\n",
            "Iteration 452, loss = 0.04391429\n",
            "Iteration 453, loss = 0.04361432\n",
            "Iteration 454, loss = 0.04330124\n",
            "Iteration 455, loss = 0.04300419\n",
            "Iteration 456, loss = 0.04272193\n",
            "Iteration 457, loss = 0.04241112\n",
            "Iteration 458, loss = 0.04212437\n",
            "Iteration 459, loss = 0.04183932\n",
            "Iteration 460, loss = 0.04156560\n",
            "Iteration 461, loss = 0.04130711\n",
            "Iteration 462, loss = 0.04103903\n",
            "Iteration 463, loss = 0.04077274\n",
            "Iteration 464, loss = 0.04049205\n",
            "Iteration 465, loss = 0.04022812\n",
            "Iteration 466, loss = 0.03997833\n",
            "Iteration 467, loss = 0.03972513\n",
            "Iteration 468, loss = 0.03946663\n",
            "Iteration 469, loss = 0.03919168\n",
            "Iteration 470, loss = 0.03900147\n",
            "Iteration 471, loss = 0.03872499\n",
            "Iteration 472, loss = 0.03848304\n",
            "Iteration 473, loss = 0.03824173\n",
            "Iteration 474, loss = 0.03804088\n",
            "Iteration 475, loss = 0.03778341\n",
            "Iteration 476, loss = 0.03756790\n",
            "Iteration 477, loss = 0.03734113\n",
            "Iteration 478, loss = 0.03712342\n",
            "Iteration 479, loss = 0.03688877\n",
            "Iteration 480, loss = 0.03666304\n",
            "Iteration 481, loss = 0.03645030\n",
            "Iteration 482, loss = 0.03626131\n",
            "Iteration 483, loss = 0.03603248\n",
            "Iteration 484, loss = 0.03583922\n",
            "Iteration 485, loss = 0.03561074\n",
            "Iteration 486, loss = 0.03539696\n",
            "Iteration 487, loss = 0.03518850\n",
            "Iteration 488, loss = 0.03500749\n",
            "Iteration 489, loss = 0.03482200\n",
            "Iteration 490, loss = 0.03458844\n",
            "Iteration 491, loss = 0.03440609\n",
            "Iteration 492, loss = 0.03420625\n",
            "Iteration 493, loss = 0.03402123\n",
            "Iteration 494, loss = 0.03386420\n",
            "Iteration 495, loss = 0.03367304\n",
            "Iteration 496, loss = 0.03346886\n",
            "Iteration 497, loss = 0.03327489\n",
            "Iteration 498, loss = 0.03310848\n",
            "Iteration 499, loss = 0.03292090\n",
            "Iteration 500, loss = 0.03275207\n",
            "Iteration 501, loss = 0.03254525\n",
            "Iteration 502, loss = 0.03240144\n",
            "Iteration 503, loss = 0.03220642\n",
            "Iteration 504, loss = 0.03203575\n",
            "Iteration 505, loss = 0.03187426\n",
            "Iteration 506, loss = 0.03170022\n",
            "Iteration 507, loss = 0.03152501\n",
            "Iteration 508, loss = 0.03139752\n",
            "Iteration 509, loss = 0.03121120\n",
            "Iteration 510, loss = 0.03102460\n",
            "Iteration 511, loss = 0.03086168\n",
            "Iteration 512, loss = 0.03070579\n",
            "Iteration 513, loss = 0.03056865\n",
            "Iteration 514, loss = 0.03038905\n",
            "Iteration 515, loss = 0.03027695\n",
            "Iteration 516, loss = 0.03007689\n",
            "Iteration 517, loss = 0.02993617\n",
            "Iteration 518, loss = 0.02979950\n",
            "Iteration 519, loss = 0.02962752\n",
            "Iteration 520, loss = 0.02947265\n",
            "Iteration 521, loss = 0.02933538\n",
            "Iteration 522, loss = 0.02919532\n",
            "Iteration 523, loss = 0.02904114\n",
            "Iteration 524, loss = 0.02889426\n",
            "Iteration 525, loss = 0.02874097\n",
            "Iteration 526, loss = 0.02863014\n",
            "Iteration 527, loss = 0.02848170\n",
            "Iteration 528, loss = 0.02832420\n",
            "Iteration 529, loss = 0.02818733\n",
            "Iteration 530, loss = 0.02808237\n",
            "Iteration 531, loss = 0.02791440\n",
            "Iteration 532, loss = 0.02777799\n",
            "Iteration 533, loss = 0.02762301\n",
            "Iteration 534, loss = 0.02750755\n",
            "Iteration 535, loss = 0.02738463\n",
            "Iteration 536, loss = 0.02725390\n",
            "Iteration 537, loss = 0.02712683\n",
            "Iteration 538, loss = 0.02698701\n",
            "Iteration 539, loss = 0.02685177\n",
            "Iteration 540, loss = 0.02674732\n",
            "Iteration 541, loss = 0.02661828\n",
            "Iteration 542, loss = 0.02650682\n",
            "Iteration 543, loss = 0.02634560\n",
            "Iteration 544, loss = 0.02622424\n",
            "Iteration 545, loss = 0.02614631\n",
            "Iteration 546, loss = 0.02600723\n",
            "Iteration 547, loss = 0.02588949\n",
            "Iteration 548, loss = 0.02577290\n",
            "Iteration 549, loss = 0.02567974\n",
            "Iteration 550, loss = 0.02553951\n",
            "Iteration 551, loss = 0.02544215\n",
            "Iteration 552, loss = 0.02530868\n",
            "Iteration 553, loss = 0.02517065\n",
            "Iteration 554, loss = 0.02510680\n",
            "Iteration 555, loss = 0.02499419\n",
            "Iteration 556, loss = 0.02485902\n",
            "Iteration 557, loss = 0.02473622\n",
            "Iteration 558, loss = 0.02461797\n",
            "Iteration 559, loss = 0.02449800\n",
            "Iteration 560, loss = 0.02445758\n",
            "Iteration 561, loss = 0.02428337\n",
            "Iteration 562, loss = 0.02418456\n",
            "Iteration 563, loss = 0.02408227\n",
            "Iteration 564, loss = 0.02401432\n",
            "Iteration 565, loss = 0.02386844\n",
            "Iteration 566, loss = 0.02377298\n",
            "Iteration 567, loss = 0.02366210\n",
            "Iteration 568, loss = 0.02356266\n",
            "Iteration 569, loss = 0.02345930\n",
            "Iteration 570, loss = 0.02337307\n",
            "Iteration 571, loss = 0.02329140\n",
            "Iteration 572, loss = 0.02314589\n",
            "Iteration 573, loss = 0.02305864\n",
            "Iteration 574, loss = 0.02294907\n",
            "Iteration 575, loss = 0.02285064\n",
            "Iteration 576, loss = 0.02275296\n",
            "Iteration 577, loss = 0.02266781\n",
            "Iteration 578, loss = 0.02256712\n",
            "Iteration 579, loss = 0.02247547\n",
            "Iteration 580, loss = 0.02238531\n",
            "Iteration 581, loss = 0.02228191\n",
            "Iteration 582, loss = 0.02222371\n",
            "Iteration 583, loss = 0.02208971\n",
            "Iteration 584, loss = 0.02199663\n",
            "Iteration 585, loss = 0.02190643\n",
            "Iteration 586, loss = 0.02184220\n",
            "Iteration 587, loss = 0.02174514\n",
            "Iteration 588, loss = 0.02169418\n",
            "Iteration 589, loss = 0.02155254\n",
            "Iteration 590, loss = 0.02148212\n",
            "Iteration 591, loss = 0.02140362\n",
            "Iteration 592, loss = 0.02129993\n",
            "Iteration 593, loss = 0.02125364\n",
            "Iteration 594, loss = 0.02113578\n",
            "Iteration 595, loss = 0.02105269\n",
            "Iteration 596, loss = 0.02097641\n",
            "Iteration 597, loss = 0.02088545\n",
            "Iteration 598, loss = 0.02077699\n",
            "Iteration 599, loss = 0.02069104\n",
            "Iteration 600, loss = 0.02060783\n",
            "Iteration 601, loss = 0.02056750\n",
            "Iteration 602, loss = 0.02049148\n",
            "Iteration 603, loss = 0.02036924\n",
            "Iteration 604, loss = 0.02030861\n",
            "Iteration 605, loss = 0.02022423\n",
            "Iteration 606, loss = 0.02013830\n",
            "Iteration 607, loss = 0.02004531\n",
            "Iteration 608, loss = 0.02002014\n",
            "Iteration 609, loss = 0.01987719\n",
            "Iteration 610, loss = 0.01980553\n",
            "Iteration 611, loss = 0.01976702\n",
            "Iteration 612, loss = 0.01967938\n",
            "Iteration 613, loss = 0.01959629\n",
            "Iteration 614, loss = 0.01950566\n",
            "Iteration 615, loss = 0.01946018\n",
            "Iteration 616, loss = 0.01936433\n",
            "Iteration 617, loss = 0.01927856\n",
            "Iteration 618, loss = 0.01922625\n",
            "Iteration 619, loss = 0.01913273\n",
            "Iteration 620, loss = 0.01906176\n",
            "Iteration 621, loss = 0.01900726\n",
            "Iteration 622, loss = 0.01894097\n",
            "Iteration 623, loss = 0.01885244\n",
            "Iteration 624, loss = 0.01876219\n",
            "Iteration 625, loss = 0.01873389\n",
            "Iteration 626, loss = 0.01863416\n",
            "Iteration 627, loss = 0.01861490\n",
            "Iteration 628, loss = 0.01852560\n",
            "Iteration 629, loss = 0.01844739\n",
            "Iteration 630, loss = 0.01836548\n",
            "Iteration 631, loss = 0.01829989\n",
            "Iteration 632, loss = 0.01823484\n",
            "Iteration 633, loss = 0.01817617\n",
            "Iteration 634, loss = 0.01809954\n",
            "Iteration 635, loss = 0.01800896\n",
            "Iteration 636, loss = 0.01796649\n",
            "Iteration 637, loss = 0.01788070\n",
            "Iteration 638, loss = 0.01783244\n",
            "Iteration 639, loss = 0.01775047\n",
            "Iteration 640, loss = 0.01767458\n",
            "Iteration 641, loss = 0.01763028\n",
            "Iteration 642, loss = 0.01757070\n",
            "Iteration 643, loss = 0.01748581\n",
            "Iteration 644, loss = 0.01744436\n",
            "Iteration 645, loss = 0.01735368\n",
            "Iteration 646, loss = 0.01729172\n",
            "Iteration 647, loss = 0.01722995\n",
            "Iteration 648, loss = 0.01715829\n",
            "Iteration 649, loss = 0.01709794\n",
            "Iteration 650, loss = 0.01704671\n",
            "Iteration 651, loss = 0.01697025\n",
            "Iteration 652, loss = 0.01689573\n",
            "Iteration 653, loss = 0.01683140\n",
            "Iteration 654, loss = 0.01679767\n",
            "Iteration 655, loss = 0.01673175\n",
            "Iteration 656, loss = 0.01664914\n",
            "Iteration 657, loss = 0.01658644\n",
            "Iteration 658, loss = 0.01653273\n",
            "Iteration 659, loss = 0.01648656\n",
            "Iteration 660, loss = 0.01641712\n",
            "Iteration 661, loss = 0.01638946\n",
            "Iteration 662, loss = 0.01628650\n",
            "Iteration 663, loss = 0.01625158\n",
            "Iteration 664, loss = 0.01618313\n",
            "Iteration 665, loss = 0.01615346\n",
            "Iteration 666, loss = 0.01607071\n",
            "Iteration 667, loss = 0.01598411\n",
            "Iteration 668, loss = 0.01595232\n",
            "Iteration 669, loss = 0.01590862\n",
            "Iteration 670, loss = 0.01582536\n",
            "Iteration 671, loss = 0.01576183\n",
            "Iteration 672, loss = 0.01571328\n",
            "Iteration 673, loss = 0.01566456\n",
            "Iteration 674, loss = 0.01558612\n",
            "Iteration 675, loss = 0.01554015\n",
            "Iteration 676, loss = 0.01550588\n",
            "Iteration 677, loss = 0.01542900\n",
            "Iteration 678, loss = 0.01537713\n",
            "Iteration 679, loss = 0.01531200\n",
            "Iteration 680, loss = 0.01526022\n",
            "Iteration 681, loss = 0.01521688\n",
            "Iteration 682, loss = 0.01517119\n",
            "Iteration 683, loss = 0.01515651\n",
            "Iteration 684, loss = 0.01504585\n",
            "Iteration 685, loss = 0.01498801\n",
            "Iteration 686, loss = 0.01495725\n",
            "Iteration 687, loss = 0.01488964\n",
            "Iteration 688, loss = 0.01482712\n",
            "Iteration 689, loss = 0.01479396\n",
            "Iteration 690, loss = 0.01475022\n",
            "Iteration 691, loss = 0.01470254\n",
            "Iteration 692, loss = 0.01460012\n",
            "Iteration 693, loss = 0.01458869\n",
            "Iteration 694, loss = 0.01453025\n",
            "Iteration 695, loss = 0.01449682\n",
            "Iteration 696, loss = 0.01440932\n",
            "Iteration 697, loss = 0.01436629\n",
            "Iteration 698, loss = 0.01433364\n",
            "Iteration 699, loss = 0.01427073\n",
            "Iteration 700, loss = 0.01420921\n",
            "Iteration 701, loss = 0.01416709\n",
            "Iteration 702, loss = 0.01412584\n",
            "Iteration 703, loss = 0.01407323\n",
            "Iteration 704, loss = 0.01404852\n",
            "Iteration 705, loss = 0.01398382\n",
            "Iteration 706, loss = 0.01394422\n",
            "Iteration 707, loss = 0.01386577\n",
            "Iteration 708, loss = 0.01383063\n",
            "Iteration 709, loss = 0.01378597\n",
            "Iteration 710, loss = 0.01374982\n",
            "Iteration 711, loss = 0.01369801\n",
            "Iteration 712, loss = 0.01364979\n",
            "Iteration 713, loss = 0.01358893\n",
            "Iteration 714, loss = 0.01354639\n",
            "Iteration 715, loss = 0.01351742\n",
            "Iteration 716, loss = 0.01345788\n",
            "Iteration 717, loss = 0.01339537\n",
            "Iteration 718, loss = 0.01336973\n",
            "Iteration 719, loss = 0.01332050\n",
            "Iteration 720, loss = 0.01326255\n",
            "Iteration 721, loss = 0.01322987\n",
            "Iteration 722, loss = 0.01319635\n",
            "Iteration 723, loss = 0.01313045\n",
            "Iteration 724, loss = 0.01309083\n",
            "Iteration 725, loss = 0.01303862\n",
            "Iteration 726, loss = 0.01300442\n",
            "Iteration 727, loss = 0.01297955\n",
            "Iteration 728, loss = 0.01292600\n",
            "Iteration 729, loss = 0.01289305\n",
            "Iteration 730, loss = 0.01283807\n",
            "Iteration 731, loss = 0.01279890\n",
            "Iteration 732, loss = 0.01273507\n",
            "Iteration 733, loss = 0.01270178\n",
            "Iteration 734, loss = 0.01266464\n",
            "Iteration 735, loss = 0.01261315\n",
            "Iteration 736, loss = 0.01260254\n",
            "Iteration 737, loss = 0.01252307\n",
            "Iteration 738, loss = 0.01248775\n",
            "Iteration 739, loss = 0.01245795\n",
            "Iteration 740, loss = 0.01241562\n",
            "Iteration 741, loss = 0.01236014\n",
            "Iteration 742, loss = 0.01232761\n",
            "Iteration 743, loss = 0.01229456\n",
            "Iteration 744, loss = 0.01224031\n",
            "Iteration 745, loss = 0.01218559\n",
            "Iteration 746, loss = 0.01216501\n",
            "Iteration 747, loss = 0.01214559\n",
            "Iteration 748, loss = 0.01208527\n",
            "Iteration 749, loss = 0.01204145\n",
            "Iteration 750, loss = 0.01199930\n",
            "Iteration 751, loss = 0.01196797\n",
            "Iteration 752, loss = 0.01191866\n",
            "Iteration 753, loss = 0.01188910\n",
            "Iteration 754, loss = 0.01183735\n",
            "Iteration 755, loss = 0.01180994\n",
            "Iteration 756, loss = 0.01175842\n",
            "Iteration 757, loss = 0.01171624\n",
            "Iteration 758, loss = 0.01169396\n",
            "Iteration 759, loss = 0.01166373\n",
            "Iteration 760, loss = 0.01160215\n",
            "Iteration 761, loss = 0.01157746\n",
            "Iteration 762, loss = 0.01152920\n",
            "Iteration 763, loss = 0.01150653\n",
            "Iteration 764, loss = 0.01146143\n",
            "Iteration 765, loss = 0.01141979\n",
            "Iteration 766, loss = 0.01138198\n",
            "Iteration 767, loss = 0.01135042\n",
            "Iteration 768, loss = 0.01132713\n",
            "Iteration 769, loss = 0.01127647\n",
            "Iteration 770, loss = 0.01122365\n",
            "Iteration 771, loss = 0.01120233\n",
            "Iteration 772, loss = 0.01116836\n",
            "Iteration 773, loss = 0.01112446\n",
            "Iteration 774, loss = 0.01107024\n",
            "Iteration 775, loss = 0.01106799\n",
            "Iteration 776, loss = 0.01101125\n",
            "Iteration 777, loss = 0.01098232\n",
            "Iteration 778, loss = 0.01094425\n",
            "Iteration 779, loss = 0.01090136\n",
            "Iteration 780, loss = 0.01088027\n",
            "Iteration 781, loss = 0.01083038\n",
            "Iteration 782, loss = 0.01079925\n",
            "Iteration 783, loss = 0.01078754\n",
            "Iteration 784, loss = 0.01075650\n",
            "Iteration 785, loss = 0.01075568\n",
            "Iteration 786, loss = 0.01066471\n",
            "Iteration 787, loss = 0.01061465\n",
            "Iteration 788, loss = 0.01062425\n",
            "Iteration 789, loss = 0.01056185\n",
            "Iteration 790, loss = 0.01054556\n",
            "Iteration 791, loss = 0.01048505\n",
            "Iteration 792, loss = 0.01046077\n",
            "Iteration 793, loss = 0.01044315\n",
            "Iteration 794, loss = 0.01039108\n",
            "Iteration 795, loss = 0.01034448\n",
            "Iteration 796, loss = 0.01033226\n",
            "Iteration 797, loss = 0.01028312\n",
            "Iteration 798, loss = 0.01026445\n",
            "Iteration 799, loss = 0.01022609\n",
            "Iteration 800, loss = 0.01020830\n",
            "Iteration 801, loss = 0.01015805\n",
            "Iteration 802, loss = 0.01015643\n",
            "Iteration 803, loss = 0.01008934\n",
            "Iteration 804, loss = 0.01006475\n",
            "Iteration 805, loss = 0.01002651\n",
            "Iteration 806, loss = 0.00999438\n",
            "Iteration 807, loss = 0.00995445\n",
            "Iteration 808, loss = 0.00994187\n",
            "Iteration 809, loss = 0.00989871\n",
            "Iteration 810, loss = 0.00986792\n",
            "Iteration 811, loss = 0.00986485\n",
            "Iteration 812, loss = 0.00980134\n",
            "Iteration 813, loss = 0.00976983\n",
            "Iteration 814, loss = 0.00974339\n",
            "Iteration 815, loss = 0.00972891\n",
            "Iteration 816, loss = 0.00968918\n",
            "Iteration 817, loss = 0.00966751\n",
            "Iteration 818, loss = 0.00961253\n",
            "Iteration 819, loss = 0.00959277\n",
            "Iteration 820, loss = 0.00956061\n",
            "Iteration 821, loss = 0.00954466\n",
            "Iteration 822, loss = 0.00950875\n",
            "Iteration 823, loss = 0.00949374\n",
            "Iteration 824, loss = 0.00944499\n",
            "Iteration 825, loss = 0.00942804\n",
            "Iteration 826, loss = 0.00939807\n",
            "Iteration 827, loss = 0.00935532\n",
            "Iteration 828, loss = 0.00933341\n",
            "Iteration 829, loss = 0.00929387\n",
            "Iteration 830, loss = 0.00926340\n",
            "Iteration 831, loss = 0.00925909\n",
            "Iteration 832, loss = 0.00923889\n",
            "Iteration 833, loss = 0.00920959\n",
            "Iteration 834, loss = 0.00916514\n",
            "Iteration 835, loss = 0.00913134\n",
            "Iteration 836, loss = 0.00910475\n",
            "Iteration 837, loss = 0.00907851\n",
            "Iteration 838, loss = 0.00906342\n",
            "Iteration 839, loss = 0.00901571\n",
            "Iteration 840, loss = 0.00900000\n",
            "Iteration 841, loss = 0.00897297\n",
            "Iteration 842, loss = 0.00893927\n",
            "Iteration 843, loss = 0.00891755\n",
            "Iteration 844, loss = 0.00887475\n",
            "Iteration 845, loss = 0.00885761\n",
            "Iteration 846, loss = 0.00882085\n",
            "Iteration 847, loss = 0.00879793\n",
            "Iteration 848, loss = 0.00878035\n",
            "Iteration 849, loss = 0.00874681\n",
            "Iteration 850, loss = 0.00874908\n",
            "Iteration 851, loss = 0.00871945\n",
            "Iteration 852, loss = 0.00868968\n",
            "Iteration 853, loss = 0.00864110\n",
            "Iteration 854, loss = 0.00862531\n",
            "Iteration 855, loss = 0.00865307\n",
            "Iteration 856, loss = 0.00859973\n",
            "Iteration 857, loss = 0.00854698\n",
            "Iteration 858, loss = 0.00852619\n",
            "Iteration 859, loss = 0.00849861\n",
            "Iteration 860, loss = 0.00846479\n",
            "Iteration 861, loss = 0.00845117\n",
            "Iteration 862, loss = 0.00842426\n",
            "Iteration 863, loss = 0.00840726\n",
            "Iteration 864, loss = 0.00839937\n",
            "Iteration 865, loss = 0.00834830\n",
            "Iteration 866, loss = 0.00832725\n",
            "Iteration 867, loss = 0.00830244\n",
            "Iteration 868, loss = 0.00829534\n",
            "Iteration 869, loss = 0.00827241\n",
            "Iteration 870, loss = 0.00825066\n",
            "Iteration 871, loss = 0.00823183\n",
            "Iteration 872, loss = 0.00818130\n",
            "Iteration 873, loss = 0.00816640\n",
            "Iteration 874, loss = 0.00815694\n",
            "Iteration 875, loss = 0.00813894\n",
            "Iteration 876, loss = 0.00811521\n",
            "Iteration 877, loss = 0.00804883\n",
            "Iteration 878, loss = 0.00803704\n",
            "Iteration 879, loss = 0.00802959\n",
            "Iteration 880, loss = 0.00800993\n",
            "Iteration 881, loss = 0.00798139\n",
            "Iteration 882, loss = 0.00795609\n",
            "Iteration 883, loss = 0.00792628\n",
            "Iteration 884, loss = 0.00790691\n",
            "Iteration 885, loss = 0.00788776\n",
            "Iteration 886, loss = 0.00788370\n",
            "Iteration 887, loss = 0.00782384\n",
            "Iteration 888, loss = 0.00784390\n",
            "Iteration 889, loss = 0.00780532\n",
            "Iteration 890, loss = 0.00778625\n",
            "Iteration 891, loss = 0.00775593\n",
            "Iteration 892, loss = 0.00774440\n",
            "Iteration 893, loss = 0.00772696\n",
            "Iteration 894, loss = 0.00769857\n",
            "Iteration 895, loss = 0.00766752\n",
            "Iteration 896, loss = 0.00764112\n",
            "Iteration 897, loss = 0.00760952\n",
            "Iteration 898, loss = 0.00761043\n",
            "Iteration 899, loss = 0.00759887\n",
            "Iteration 900, loss = 0.00755522\n",
            "Iteration 901, loss = 0.00755303\n",
            "Iteration 902, loss = 0.00755340\n",
            "Iteration 903, loss = 0.00751272\n",
            "Iteration 904, loss = 0.00749366\n",
            "Iteration 905, loss = 0.00744877\n",
            "Iteration 906, loss = 0.00743188\n",
            "Iteration 907, loss = 0.00741311\n",
            "Iteration 908, loss = 0.00741177\n",
            "Iteration 909, loss = 0.00737096\n",
            "Iteration 910, loss = 0.00736171\n",
            "Iteration 911, loss = 0.00733647\n",
            "Iteration 912, loss = 0.00731884\n",
            "Iteration 913, loss = 0.00729948\n",
            "Iteration 914, loss = 0.00729129\n",
            "Iteration 915, loss = 0.00725439\n",
            "Iteration 916, loss = 0.00724448\n",
            "Iteration 917, loss = 0.00723113\n",
            "Iteration 918, loss = 0.00720509\n",
            "Iteration 919, loss = 0.00715717\n",
            "Iteration 920, loss = 0.00715050\n",
            "Iteration 921, loss = 0.00713095\n",
            "Iteration 922, loss = 0.00711647\n",
            "Iteration 923, loss = 0.00708608\n",
            "Iteration 924, loss = 0.00707398\n",
            "Iteration 925, loss = 0.00705955\n",
            "Iteration 926, loss = 0.00704521\n",
            "Iteration 927, loss = 0.00703117\n",
            "Iteration 928, loss = 0.00700739\n",
            "Iteration 929, loss = 0.00698580\n",
            "Iteration 930, loss = 0.00696876\n",
            "Iteration 931, loss = 0.00693700\n",
            "Iteration 932, loss = 0.00691985\n",
            "Iteration 933, loss = 0.00689310\n",
            "Iteration 934, loss = 0.00689605\n",
            "Iteration 935, loss = 0.00688485\n",
            "Iteration 936, loss = 0.00685482\n",
            "Iteration 937, loss = 0.00682083\n",
            "Iteration 938, loss = 0.00683916\n",
            "Iteration 939, loss = 0.00680963\n",
            "Iteration 940, loss = 0.00677740\n",
            "Iteration 941, loss = 0.00677528\n",
            "Iteration 942, loss = 0.00675623\n",
            "Iteration 943, loss = 0.00672858\n",
            "Iteration 944, loss = 0.00670258\n",
            "Iteration 945, loss = 0.00668911\n",
            "Iteration 946, loss = 0.00668498\n",
            "Iteration 947, loss = 0.00664915\n",
            "Iteration 948, loss = 0.00665642\n",
            "Iteration 949, loss = 0.00661798\n",
            "Iteration 950, loss = 0.00659787\n",
            "Iteration 951, loss = 0.00662911\n",
            "Iteration 952, loss = 0.00658326\n",
            "Iteration 953, loss = 0.00654983\n",
            "Iteration 954, loss = 0.00655266\n",
            "Iteration 955, loss = 0.00651795\n",
            "Iteration 956, loss = 0.00650075\n",
            "Iteration 957, loss = 0.00648841\n",
            "Iteration 958, loss = 0.00647688\n",
            "Iteration 959, loss = 0.00644372\n",
            "Iteration 960, loss = 0.00644069\n",
            "Iteration 961, loss = 0.00645680\n",
            "Iteration 962, loss = 0.00643558\n",
            "Iteration 963, loss = 0.00640923\n",
            "Iteration 964, loss = 0.00640806\n",
            "Iteration 965, loss = 0.00636798\n",
            "Iteration 966, loss = 0.00634492\n",
            "Iteration 967, loss = 0.00633533\n",
            "Iteration 968, loss = 0.00630556\n",
            "Iteration 969, loss = 0.00629813\n",
            "Iteration 970, loss = 0.00626399\n",
            "Iteration 971, loss = 0.00631487\n",
            "Iteration 972, loss = 0.00625622\n",
            "Iteration 973, loss = 0.00623639\n",
            "Iteration 974, loss = 0.00620648\n",
            "Iteration 975, loss = 0.00618437\n",
            "Iteration 976, loss = 0.00617902\n",
            "Iteration 977, loss = 0.00615854\n",
            "Iteration 978, loss = 0.00615256\n",
            "Iteration 979, loss = 0.00612434\n",
            "Iteration 980, loss = 0.00612400\n",
            "Iteration 981, loss = 0.00610211\n",
            "Iteration 982, loss = 0.00609972\n",
            "Iteration 983, loss = 0.00608784\n",
            "Iteration 984, loss = 0.00605821\n",
            "Iteration 985, loss = 0.00604676\n",
            "Iteration 986, loss = 0.00602590\n",
            "Iteration 987, loss = 0.00602078\n",
            "Iteration 988, loss = 0.00600038\n",
            "Iteration 989, loss = 0.00598610\n",
            "Iteration 990, loss = 0.00596047\n",
            "Iteration 991, loss = 0.00596310\n",
            "Iteration 992, loss = 0.00595947\n",
            "Iteration 993, loss = 0.00592081\n",
            "Iteration 994, loss = 0.00593074\n",
            "Iteration 995, loss = 0.00590350\n",
            "Iteration 996, loss = 0.00588276\n",
            "Iteration 997, loss = 0.00587505\n",
            "Iteration 998, loss = 0.00585065\n",
            "Iteration 999, loss = 0.00583515\n",
            "Iteration 1000, loss = 0.00583249\n",
            "Iteration 1001, loss = 0.00580715\n",
            "Iteration 1002, loss = 0.00578626\n",
            "Iteration 1003, loss = 0.00578049\n",
            "Iteration 1004, loss = 0.00576869\n",
            "Iteration 1005, loss = 0.00576284\n",
            "Iteration 1006, loss = 0.00574234\n",
            "Iteration 1007, loss = 0.00575694\n",
            "Iteration 1008, loss = 0.00572957\n",
            "Iteration 1009, loss = 0.00570403\n",
            "Iteration 1010, loss = 0.00569123\n",
            "Iteration 1011, loss = 0.00566865\n",
            "Iteration 1012, loss = 0.00567565\n",
            "Iteration 1013, loss = 0.00565427\n",
            "Iteration 1014, loss = 0.00565571\n",
            "Iteration 1015, loss = 0.00561803\n",
            "Iteration 1016, loss = 0.00564276\n",
            "Iteration 1017, loss = 0.00558984\n",
            "Iteration 1018, loss = 0.00557523\n",
            "Iteration 1019, loss = 0.00558863\n",
            "Iteration 1020, loss = 0.00558366\n",
            "Iteration 1021, loss = 0.00553680\n",
            "Iteration 1022, loss = 0.00552800\n",
            "Iteration 1023, loss = 0.00550961\n",
            "Iteration 1024, loss = 0.00549430\n",
            "Iteration 1025, loss = 0.00548354\n",
            "Iteration 1026, loss = 0.00547099\n",
            "Iteration 1027, loss = 0.00547242\n",
            "Iteration 1028, loss = 0.00544155\n",
            "Iteration 1029, loss = 0.00544116\n",
            "Iteration 1030, loss = 0.00543408\n",
            "Iteration 1031, loss = 0.00541196\n",
            "Iteration 1032, loss = 0.00542442\n",
            "Iteration 1033, loss = 0.00538854\n",
            "Iteration 1034, loss = 0.00539576\n",
            "Iteration 1035, loss = 0.00535847\n",
            "Iteration 1036, loss = 0.00535483\n",
            "Iteration 1037, loss = 0.00533305\n",
            "Iteration 1038, loss = 0.00533292\n",
            "Iteration 1039, loss = 0.00531636\n",
            "Iteration 1040, loss = 0.00529597\n",
            "Iteration 1041, loss = 0.00528709\n",
            "Iteration 1042, loss = 0.00528760\n",
            "Iteration 1043, loss = 0.00530424\n",
            "Iteration 1044, loss = 0.00526324\n",
            "Iteration 1045, loss = 0.00525651\n",
            "Iteration 1046, loss = 0.00522334\n",
            "Iteration 1047, loss = 0.00521440\n",
            "Iteration 1048, loss = 0.00520263\n",
            "Iteration 1049, loss = 0.00519663\n",
            "Iteration 1050, loss = 0.00516575\n",
            "Iteration 1051, loss = 0.00519537\n",
            "Iteration 1052, loss = 0.00516923\n",
            "Iteration 1053, loss = 0.00514682\n",
            "Iteration 1054, loss = 0.00513481\n",
            "Iteration 1055, loss = 0.00513553\n",
            "Iteration 1056, loss = 0.00510931\n",
            "Iteration 1057, loss = 0.00509216\n",
            "Iteration 1058, loss = 0.00508453\n",
            "Iteration 1059, loss = 0.00507404\n",
            "Iteration 1060, loss = 0.00509107\n",
            "Iteration 1061, loss = 0.00506291\n",
            "Iteration 1062, loss = 0.00504350\n",
            "Iteration 1063, loss = 0.00503348\n",
            "Iteration 1064, loss = 0.00502269\n",
            "Iteration 1065, loss = 0.00500508\n",
            "Iteration 1066, loss = 0.00503965\n",
            "Iteration 1067, loss = 0.00498479\n",
            "Iteration 1068, loss = 0.00499981\n",
            "Iteration 1069, loss = 0.00496919\n",
            "Iteration 1070, loss = 0.00498401\n",
            "Iteration 1071, loss = 0.00494490\n",
            "Iteration 1072, loss = 0.00494022\n",
            "Iteration 1073, loss = 0.00492470\n",
            "Iteration 1074, loss = 0.00492008\n",
            "Iteration 1075, loss = 0.00491061\n",
            "Iteration 1076, loss = 0.00489725\n",
            "Iteration 1077, loss = 0.00488396\n",
            "Iteration 1078, loss = 0.00486622\n",
            "Iteration 1079, loss = 0.00485204\n",
            "Iteration 1080, loss = 0.00484154\n",
            "Iteration 1081, loss = 0.00482770\n",
            "Iteration 1082, loss = 0.00481577\n",
            "Iteration 1083, loss = 0.00481093\n",
            "Iteration 1084, loss = 0.00479159\n",
            "Iteration 1085, loss = 0.00478830\n",
            "Iteration 1086, loss = 0.00480114\n",
            "Iteration 1087, loss = 0.00476525\n",
            "Iteration 1088, loss = 0.00479263\n",
            "Iteration 1089, loss = 0.00473001\n",
            "Iteration 1090, loss = 0.00475050\n",
            "Iteration 1091, loss = 0.00474868\n",
            "Iteration 1092, loss = 0.00473446\n",
            "Iteration 1093, loss = 0.00470858\n",
            "Iteration 1094, loss = 0.00470025\n",
            "Iteration 1095, loss = 0.00471126\n",
            "Iteration 1096, loss = 0.00470766\n",
            "Iteration 1097, loss = 0.00468272\n",
            "Iteration 1098, loss = 0.00464736\n",
            "Iteration 1099, loss = 0.00466435\n",
            "Iteration 1100, loss = 0.00463713\n",
            "Iteration 1101, loss = 0.00462767\n",
            "Iteration 1102, loss = 0.00460431\n",
            "Iteration 1103, loss = 0.00461103\n",
            "Iteration 1104, loss = 0.00460297\n",
            "Iteration 1105, loss = 0.00459220\n",
            "Iteration 1106, loss = 0.00458518\n",
            "Iteration 1107, loss = 0.00458287\n",
            "Iteration 1108, loss = 0.00457868\n",
            "Iteration 1109, loss = 0.00457809\n",
            "Iteration 1110, loss = 0.00454674\n",
            "Iteration 1111, loss = 0.00452490\n",
            "Iteration 1112, loss = 0.00451392\n",
            "Iteration 1113, loss = 0.00449937\n",
            "Iteration 1114, loss = 0.00451192\n",
            "Iteration 1115, loss = 0.00448914\n",
            "Iteration 1116, loss = 0.00449473\n",
            "Iteration 1117, loss = 0.00446574\n",
            "Iteration 1118, loss = 0.00449648\n",
            "Iteration 1119, loss = 0.00446840\n",
            "Iteration 1120, loss = 0.00445386\n",
            "Iteration 1121, loss = 0.00443088\n",
            "Iteration 1122, loss = 0.00441943\n",
            "Iteration 1123, loss = 0.00440196\n",
            "Iteration 1124, loss = 0.00439865\n",
            "Iteration 1125, loss = 0.00441547\n",
            "Iteration 1126, loss = 0.00439452\n",
            "Iteration 1127, loss = 0.00439001\n",
            "Iteration 1128, loss = 0.00438927\n",
            "Iteration 1129, loss = 0.00436056\n",
            "Iteration 1130, loss = 0.00434937\n",
            "Iteration 1131, loss = 0.00435148\n",
            "Iteration 1132, loss = 0.00435793\n",
            "Iteration 1133, loss = 0.00433383\n",
            "Iteration 1134, loss = 0.00432181\n",
            "Iteration 1135, loss = 0.00429914\n",
            "Iteration 1136, loss = 0.00429578\n",
            "Iteration 1137, loss = 0.00431879\n",
            "Iteration 1138, loss = 0.00429966\n",
            "Iteration 1139, loss = 0.00427059\n",
            "Iteration 1140, loss = 0.00426031\n",
            "Iteration 1141, loss = 0.00425343\n",
            "Iteration 1142, loss = 0.00423834\n",
            "Iteration 1143, loss = 0.00424001\n",
            "Iteration 1144, loss = 0.00423708\n",
            "Iteration 1145, loss = 0.00423415\n",
            "Iteration 1146, loss = 0.00420156\n",
            "Iteration 1147, loss = 0.00419391\n",
            "Iteration 1148, loss = 0.00418589\n",
            "Iteration 1149, loss = 0.00418764\n",
            "Iteration 1150, loss = 0.00416482\n",
            "Iteration 1151, loss = 0.00417550\n",
            "Iteration 1152, loss = 0.00416110\n",
            "Iteration 1153, loss = 0.00414202\n",
            "Iteration 1154, loss = 0.00414253\n",
            "Iteration 1155, loss = 0.00415244\n",
            "Iteration 1156, loss = 0.00412326\n",
            "Iteration 1157, loss = 0.00412161\n",
            "Iteration 1158, loss = 0.00411680\n",
            "Iteration 1159, loss = 0.00409985\n",
            "Iteration 1160, loss = 0.00408511\n",
            "Iteration 1161, loss = 0.00408655\n",
            "Iteration 1162, loss = 0.00410217\n",
            "Iteration 1163, loss = 0.00407046\n",
            "Iteration 1164, loss = 0.00406443\n",
            "Iteration 1165, loss = 0.00405170\n",
            "Iteration 1166, loss = 0.00405311\n",
            "Iteration 1167, loss = 0.00404460\n",
            "Iteration 1168, loss = 0.00402867\n",
            "Iteration 1169, loss = 0.00400527\n",
            "Iteration 1170, loss = 0.00401062\n",
            "Iteration 1171, loss = 0.00400033\n",
            "Iteration 1172, loss = 0.00400233\n",
            "Iteration 1173, loss = 0.00402974\n",
            "Iteration 1174, loss = 0.00398157\n",
            "Iteration 1175, loss = 0.00397572\n",
            "Iteration 1176, loss = 0.00398405\n",
            "Iteration 1177, loss = 0.00397553\n",
            "Iteration 1178, loss = 0.00395663\n",
            "Iteration 1179, loss = 0.00393589\n",
            "Iteration 1180, loss = 0.00393044\n",
            "Iteration 1181, loss = 0.00391740\n",
            "Iteration 1182, loss = 0.00390224\n",
            "Iteration 1183, loss = 0.00390086\n",
            "Iteration 1184, loss = 0.00389825\n",
            "Iteration 1185, loss = 0.00390401\n",
            "Iteration 1186, loss = 0.00387698\n",
            "Iteration 1187, loss = 0.00388768\n",
            "Iteration 1188, loss = 0.00390709\n",
            "Iteration 1189, loss = 0.00385782\n",
            "Iteration 1190, loss = 0.00388406\n",
            "Iteration 1191, loss = 0.00385129\n",
            "Iteration 1192, loss = 0.00385106\n",
            "Iteration 1193, loss = 0.00382374\n",
            "Iteration 1194, loss = 0.00383313\n",
            "Iteration 1195, loss = 0.00382941\n",
            "Iteration 1196, loss = 0.00385470\n",
            "Iteration 1197, loss = 0.00382322\n",
            "Iteration 1198, loss = 0.00381760\n",
            "Iteration 1199, loss = 0.00379563\n",
            "Iteration 1200, loss = 0.00383270\n",
            "Iteration 1201, loss = 0.00377541\n",
            "Iteration 1202, loss = 0.00377234\n",
            "Iteration 1203, loss = 0.00380314\n",
            "Iteration 1204, loss = 0.00376299\n",
            "Iteration 1205, loss = 0.00374788\n",
            "Iteration 1206, loss = 0.00372940\n",
            "Iteration 1207, loss = 0.00375398\n",
            "Iteration 1208, loss = 0.00372552\n",
            "Iteration 1209, loss = 0.00376975\n",
            "Iteration 1210, loss = 0.00374562\n",
            "Iteration 1211, loss = 0.00369730\n",
            "Iteration 1212, loss = 0.00370087\n",
            "Iteration 1213, loss = 0.00370469\n",
            "Iteration 1214, loss = 0.00367732\n",
            "Iteration 1215, loss = 0.00368020\n",
            "Iteration 1216, loss = 0.00365964\n",
            "Iteration 1217, loss = 0.00367638\n",
            "Iteration 1218, loss = 0.00364343\n",
            "Iteration 1219, loss = 0.00366206\n",
            "Iteration 1220, loss = 0.00366334\n",
            "Iteration 1221, loss = 0.00362856\n",
            "Iteration 1222, loss = 0.00362942\n",
            "Iteration 1223, loss = 0.00361047\n",
            "Iteration 1224, loss = 0.00361103\n",
            "Iteration 1225, loss = 0.00360819\n",
            "Iteration 1226, loss = 0.00360219\n",
            "Iteration 1227, loss = 0.00361314\n",
            "Iteration 1228, loss = 0.00359934\n",
            "Iteration 1229, loss = 0.00356336\n",
            "Iteration 1230, loss = 0.00356528\n",
            "Iteration 1231, loss = 0.00356732\n",
            "Iteration 1232, loss = 0.00356162\n",
            "Iteration 1233, loss = 0.00355868\n",
            "Iteration 1234, loss = 0.00354997\n",
            "Iteration 1235, loss = 0.00355725\n",
            "Iteration 1236, loss = 0.00352549\n",
            "Iteration 1237, loss = 0.00356305\n",
            "Iteration 1238, loss = 0.00352368\n",
            "Iteration 1239, loss = 0.00354278\n",
            "Iteration 1240, loss = 0.00354505\n",
            "Iteration 1241, loss = 0.00349656\n",
            "Iteration 1242, loss = 0.00347908\n",
            "Iteration 1243, loss = 0.00347810\n",
            "Iteration 1244, loss = 0.00348931\n",
            "Iteration 1245, loss = 0.00346082\n",
            "Iteration 1246, loss = 0.00349080\n",
            "Iteration 1247, loss = 0.00345827\n",
            "Iteration 1248, loss = 0.00344990\n",
            "Iteration 1249, loss = 0.00345010\n",
            "Iteration 1250, loss = 0.00344508\n",
            "Iteration 1251, loss = 0.00344017\n",
            "Iteration 1252, loss = 0.00341722\n",
            "Iteration 1253, loss = 0.00344971\n",
            "Iteration 1254, loss = 0.00342160\n",
            "Iteration 1255, loss = 0.00341283\n",
            "Iteration 1256, loss = 0.00342911\n",
            "Iteration 1257, loss = 0.00340826\n",
            "Iteration 1258, loss = 0.00340017\n",
            "Iteration 1259, loss = 0.00338584\n",
            "Iteration 1260, loss = 0.00337230\n",
            "Iteration 1261, loss = 0.00339963\n",
            "Iteration 1262, loss = 0.00338438\n",
            "Iteration 1263, loss = 0.00337045\n",
            "Iteration 1264, loss = 0.00337794\n",
            "Iteration 1265, loss = 0.00333406\n",
            "Iteration 1266, loss = 0.00334056\n",
            "Iteration 1267, loss = 0.00335162\n",
            "Iteration 1268, loss = 0.00332829\n",
            "Iteration 1269, loss = 0.00332557\n",
            "Iteration 1270, loss = 0.00333326\n",
            "Iteration 1271, loss = 0.00331832\n",
            "Iteration 1272, loss = 0.00331340\n",
            "Iteration 1273, loss = 0.00329278\n",
            "Iteration 1274, loss = 0.00329448\n",
            "Iteration 1275, loss = 0.00329035\n",
            "Iteration 1276, loss = 0.00328720\n",
            "Iteration 1277, loss = 0.00327435\n",
            "Iteration 1278, loss = 0.00326196\n",
            "Iteration 1279, loss = 0.00327455\n",
            "Iteration 1280, loss = 0.00327479\n",
            "Iteration 1281, loss = 0.00325643\n",
            "Iteration 1282, loss = 0.00324533\n",
            "Iteration 1283, loss = 0.00323841\n",
            "Iteration 1284, loss = 0.00322924\n",
            "Iteration 1285, loss = 0.00324089\n",
            "Iteration 1286, loss = 0.00321967\n",
            "Iteration 1287, loss = 0.00322816\n",
            "Iteration 1288, loss = 0.00321057\n",
            "Iteration 1289, loss = 0.00321330\n",
            "Iteration 1290, loss = 0.00320023\n",
            "Iteration 1291, loss = 0.00320555\n",
            "Iteration 1292, loss = 0.00319604\n",
            "Iteration 1293, loss = 0.00318850\n",
            "Iteration 1294, loss = 0.00319668\n",
            "Iteration 1295, loss = 0.00318844\n",
            "Iteration 1296, loss = 0.00317003\n",
            "Iteration 1297, loss = 0.00316366\n",
            "Iteration 1298, loss = 0.00317733\n",
            "Iteration 1299, loss = 0.00317222\n",
            "Iteration 1300, loss = 0.00318208\n",
            "Iteration 1301, loss = 0.00313867\n",
            "Iteration 1302, loss = 0.00313668\n",
            "Iteration 1303, loss = 0.00312497\n",
            "Iteration 1304, loss = 0.00312231\n",
            "Iteration 1305, loss = 0.00310900\n",
            "Iteration 1306, loss = 0.00313051\n",
            "Iteration 1307, loss = 0.00311039\n",
            "Iteration 1308, loss = 0.00311304\n",
            "Iteration 1309, loss = 0.00309505\n",
            "Iteration 1310, loss = 0.00309338\n",
            "Iteration 1311, loss = 0.00307829\n",
            "Iteration 1312, loss = 0.00307914\n",
            "Iteration 1313, loss = 0.00309151\n",
            "Iteration 1314, loss = 0.00307410\n",
            "Iteration 1315, loss = 0.00307437\n",
            "Iteration 1316, loss = 0.00306035\n",
            "Iteration 1317, loss = 0.00307128\n",
            "Iteration 1318, loss = 0.00305286\n",
            "Iteration 1319, loss = 0.00304360\n",
            "Iteration 1320, loss = 0.00304181\n",
            "Iteration 1321, loss = 0.00305661\n",
            "Iteration 1322, loss = 0.00306036\n",
            "Iteration 1323, loss = 0.00302576\n",
            "Iteration 1324, loss = 0.00302244\n",
            "Iteration 1325, loss = 0.00304004\n",
            "Iteration 1326, loss = 0.00301964\n",
            "Iteration 1327, loss = 0.00300231\n",
            "Iteration 1328, loss = 0.00300055\n",
            "Iteration 1329, loss = 0.00299693\n",
            "Iteration 1330, loss = 0.00298741\n",
            "Iteration 1331, loss = 0.00299843\n",
            "Iteration 1332, loss = 0.00297617\n",
            "Iteration 1333, loss = 0.00301144\n",
            "Iteration 1334, loss = 0.00297619\n",
            "Iteration 1335, loss = 0.00297975\n",
            "Iteration 1336, loss = 0.00297315\n",
            "Iteration 1337, loss = 0.00295578\n",
            "Iteration 1338, loss = 0.00294819\n",
            "Iteration 1339, loss = 0.00294656\n",
            "Iteration 1340, loss = 0.00294108\n",
            "Iteration 1341, loss = 0.00297381\n",
            "Iteration 1342, loss = 0.00294698\n",
            "Iteration 1343, loss = 0.00299432\n",
            "Iteration 1344, loss = 0.00293653\n",
            "Iteration 1345, loss = 0.00293777\n",
            "Iteration 1346, loss = 0.00291679\n",
            "Iteration 1347, loss = 0.00291007\n",
            "Iteration 1348, loss = 0.00292266\n",
            "Iteration 1349, loss = 0.00292287\n",
            "Iteration 1350, loss = 0.00291425\n",
            "Iteration 1351, loss = 0.00290540\n",
            "Iteration 1352, loss = 0.00292731\n",
            "Iteration 1353, loss = 0.00289503\n",
            "Iteration 1354, loss = 0.00288242\n",
            "Iteration 1355, loss = 0.00286727\n",
            "Iteration 1356, loss = 0.00289382\n",
            "Iteration 1357, loss = 0.00287543\n",
            "Iteration 1358, loss = 0.00286834\n",
            "Iteration 1359, loss = 0.00286565\n",
            "Iteration 1360, loss = 0.00285097\n",
            "Iteration 1361, loss = 0.00284220\n",
            "Iteration 1362, loss = 0.00284712\n",
            "Iteration 1363, loss = 0.00282358\n",
            "Iteration 1364, loss = 0.00287003\n",
            "Iteration 1365, loss = 0.00286730\n",
            "Iteration 1366, loss = 0.00283580\n",
            "Iteration 1367, loss = 0.00282736\n",
            "Iteration 1368, loss = 0.00281217\n",
            "Iteration 1369, loss = 0.00280946\n",
            "Iteration 1370, loss = 0.00281591\n",
            "Iteration 1371, loss = 0.00280810\n",
            "Iteration 1372, loss = 0.00280300\n",
            "Iteration 1373, loss = 0.00279001\n",
            "Iteration 1374, loss = 0.00280618\n",
            "Iteration 1375, loss = 0.00278806\n",
            "Iteration 1376, loss = 0.00279385\n",
            "Iteration 1377, loss = 0.00280364\n",
            "Iteration 1378, loss = 0.00278306\n",
            "Iteration 1379, loss = 0.00278570\n",
            "Iteration 1380, loss = 0.00278068\n",
            "Iteration 1381, loss = 0.00277915\n",
            "Iteration 1382, loss = 0.00276949\n",
            "Iteration 1383, loss = 0.00275999\n",
            "Iteration 1384, loss = 0.00275841\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo as previsões\n",
        "\n",
        "previsao = rede_neural_credit.predict(x_credit_teste)\n",
        "previsao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2DZmXzKXDNI",
        "outputId": "3c90ce7f-1231-455d-9616-e03380fa0c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparando\n",
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR4n6XJjXYyF",
        "outputId": "9bd1a24b-c0f6-4319-8706-cf074dd850ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Acuracia\n",
        "accuracy_score(y_credit_teste, previsao)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRpwoyaQXdS0",
        "outputId": "65dba594-9439-45a8-c5d3-1099e3620361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão\n",
        "\n",
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(x_credit_treino, y_credit_treino)\n",
        "cm.score(x_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "iA1lhUGrX3LM",
        "outputId": "342472ee-d0a8-45fd-bf45-d9c67918f77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando as estatisticas do modelo\n",
        "\n",
        "print(classification_report(y_credit_teste, previsao))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edhJ7A44Y211",
        "outputId": "6b965b21-3d9e-4dcf-f55e-46ded53fe1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      1.00      0.99        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       0.99      1.00      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}